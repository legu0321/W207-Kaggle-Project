{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza (RAOP) Notes\n",
    "\n",
    "**Source**: Althoff, T., Danescu-Niculescu-Mizil, C., & Jurafsky, D. (2014). *How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*. Association for the Advancement of Artificial\n",
    "Intelligence (www.aaai.org).\n",
    "\n",
    "- \"The community only publishes which users have given or received pizzas but not which requests were successful. \n",
    "In the case of successful users posting multiple times it is unclear which of the requests was actually successful. \n",
    "Therefore, we restrict our analysis to users with a single request for which we can be certain whether or not \n",
    "it was successful, leaving us with 5728 pizza requests. We split this dataset into development(70%) and test set (30%) \n",
    "such that both sets mirror the average success rate in our dataset of 24.6%. All features are developed on the \n",
    "development test only while the test set is used only once to evaluate the prediction accuracy of our proposed model on held-out data. For a small number of requests (379) we further observe the identity of the benefactor through a \n",
    "'thank you' post by the beneficiary after the successful request. This enables us to reason about the impact of \n",
    "user similarity on giving.\"\n",
    "\n",
    "\n",
    "- \"It is extremely difficult to disentangle the effects of all these factors in determining what makes people satisfy requests, and what makes them select some requests over others. . . In this paper, we develop a framework for controlling for each of these potential confounds while studying the role of two aspects that characterize compelling requests: **social factors** (who is asking and how the recipient is related to the donor and community) and **linguistic factors** (how they are asking and what linguistic devices accompany successful requests). With the notable exception of Mitra and Gilbert (2014), the effect of language on the success of requests has largely been ignored thus far.\"\n",
    "\n",
    "\n",
    "- \"[Their] goal is to understand what motivates people to give when they do not receive anything tangible in return. That is, [they] focus on the important special case of altruistic requests in which the giver receives no rewards.\" **DSC**: But how do you know people don't want something in return, especially if they are more likely to help requesters who have high status or are more similar to them?\n",
    "\n",
    "-----\n",
    "\n",
    "Temporal Factors\n",
    "- Specific months\n",
    "- Weekdays\n",
    "- **Days of the month (first half of the month)**\n",
    "- Hour of the day\n",
    "- **Community age of the request (earlier the better)**\n",
    "\n",
    "Textual Factors\n",
    "- Politeness (e.g., **gratitude**)\n",
    "- **Evidentiality** (2nd largest parameter estimate)\n",
    "- Reciprocity (respond to a positive action with another positive action, **pay it forward**)\n",
    "- Sentiment (e.g., **urgency**)\n",
    "- **Length**\n",
    "\n",
    "Social Factors\n",
    "- **Status**\n",
    "    - karma points (up-votes minus down-votes) that Reddit counts on link submissions and comments,\n",
    "    - user has posted on RAOP before and thus could be considered a member of the sub-community. \n",
    "    - **user account age based on the hypothesis that “younger” accounts might be less trusted**\n",
    "\n",
    "\n",
    "- Similarity: intersection size between the set of the giver and receiver, and the Jaccard similarity (intersection\n",
    "over union) of the two. NOT included in logistic regression model.\n",
    "\n",
    "Narratives (identified through topic modeling)\n",
    "- **Desire**\n",
    "- **Family**\n",
    "- **Job**\n",
    "- **Money**\n",
    "- Student\n",
    "\n",
    "-----\n",
    "\n",
    "Conclusion\n",
    "- Drawing from social psychology literature [they] extract high-level social features from text that operationalize the relation between recipient and donor and demonstrate that these extracted relations are predictive of success. \n",
    "- [They] show that [they] can detect key narratives automatically that have significant impact on the success of the request. \n",
    "- [They] further demonstrate that linguistic indications of gratitude, evidentiality, and reciprocity, as well as the high status of the asker, all increase the likelihood of success, while neither politeness nor positive sentiment seem to be associated with success in [the] setting.\n",
    "\n",
    "Limitations\n",
    "- A shortcoming of any case study is that findings might be specific to the scenario at hand. While [they] have shown that particular linguistic and social factors differentiate between successful and unsuccessful requests [they] cannot claim a causal relationship between the proposed factors and success that would guarantee success. \n",
    "- Furthermore, the set of success factors studied in this work is likely to be incomplete as well and excludes,\n",
    "for instance, group behavior dynamics. \n",
    "- Despite these limitations, [they] hope that this work and the data [they] make available will provide a basis for further research on success factors and helping behavior in other online communities.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maximus/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.19.\n",
      "  \"This module will be removed in 0.19.\", DeprecationWarning)\n",
      "/Users/Maximus/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.19.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(X)', <class 'pandas.core.frame.DataFrame'>)\n",
      "\n",
      "train_labels\n",
      "----------\n",
      "[u'requester_received_pizza']\n",
      "(3975, 1) /n\n",
      "train_data\n",
      "----------\n",
      "[u'giver_username_if_known', u'in_test_set', u'number_of_downvotes_of_request_at_retrieval', u'number_of_upvotes_of_request_at_retrieval', u'post_was_edited', u'request_id', u'request_number_of_comments_at_retrieval', u'request_text', u'request_text_edit_aware', u'request_title', u'requester_account_age_in_days_at_request', u'requester_account_age_in_days_at_retrieval', u'requester_days_since_first_post_on_raop_at_request', u'requester_days_since_first_post_on_raop_at_retrieval', u'requester_number_of_comments_at_request', u'requester_number_of_comments_at_retrieval', u'requester_number_of_comments_in_raop_at_request', u'requester_number_of_comments_in_raop_at_retrieval', u'requester_number_of_posts_at_request', u'requester_number_of_posts_at_retrieval', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_on_raop_at_retrieval', u'requester_number_of_subreddits_at_request', u'requester_subreddits_at_request', u'requester_upvotes_minus_downvotes_at_request', u'requester_upvotes_minus_downvotes_at_retrieval', u'requester_upvotes_plus_downvotes_at_request', u'requester_upvotes_plus_downvotes_at_retrieval', u'requester_user_flair', u'requester_username', u'unix_timestamp_of_request', u'unix_timestamp_of_request_utc']\n",
      "(3975, 32) /n\n",
      "dev_labels\n",
      "----------\n",
      "[u'requester_received_pizza']\n",
      "(1696, 1) /n\n",
      "dev_data\n",
      "----------\n",
      "[u'giver_username_if_known', u'in_test_set', u'number_of_downvotes_of_request_at_retrieval', u'number_of_upvotes_of_request_at_retrieval', u'post_was_edited', u'request_id', u'request_number_of_comments_at_retrieval', u'request_text', u'request_text_edit_aware', u'request_title', u'requester_account_age_in_days_at_request', u'requester_account_age_in_days_at_retrieval', u'requester_days_since_first_post_on_raop_at_request', u'requester_days_since_first_post_on_raop_at_retrieval', u'requester_number_of_comments_at_request', u'requester_number_of_comments_at_retrieval', u'requester_number_of_comments_in_raop_at_request', u'requester_number_of_comments_in_raop_at_retrieval', u'requester_number_of_posts_at_request', u'requester_number_of_posts_at_retrieval', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_on_raop_at_retrieval', u'requester_number_of_subreddits_at_request', u'requester_subreddits_at_request', u'requester_upvotes_minus_downvotes_at_request', u'requester_upvotes_minus_downvotes_at_retrieval', u'requester_upvotes_plus_downvotes_at_request', u'requester_upvotes_plus_downvotes_at_retrieval', u'requester_user_flair', u'requester_username', u'unix_timestamp_of_request', u'unix_timestamp_of_request_utc']\n",
      "(1696, 32) /n\n",
      "train labels\n",
      "----------\n",
      "requester_received_pizza    0.241006\n",
      "dtype: float64 /n\n",
      "dev labels\n",
      "----------\n",
      "requester_received_pizza    0.258844\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html\n",
    "# Convert a JSON string to pandas object\n",
    "\n",
    "X = pd.read_json('./pizza_request_dataset.json')\n",
    "print (\"type(X)\", type(X))\n",
    "print\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly assign 70% to train_data, and 30% to dev_data\n",
    "msk = np.random.rand(len(X)) <= 0.7\n",
    "train_data = X[msk]\n",
    "dev_data = X[~msk]\n",
    "\n",
    "# create output dataframe Y of train_labels\n",
    "train_labels = train_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete train_labels from input dataframe of train_data\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "# create output dataframe of dev_labels\n",
    "dev_labels = dev_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete dev_labels from input dataframe of dev_data\n",
    "del dev_data[\"requester_received_pizza\"]\n",
    "\n",
    "# print (labels, shapes, and feature names\n",
    "print (\"train_labels\" )\n",
    "print (\"----------\")\n",
    "print (list(train_labels))\n",
    "print (train_labels.shape), \"/n\"\n",
    "print (\"train_data\" )\n",
    "print (\"----------\")\n",
    "print (list(train_data))\n",
    "print (train_data.shape), \"/n\"\n",
    "\n",
    "\n",
    "print (\"dev_labels\") \n",
    "print (\"----------\")\n",
    "print (list(dev_labels))\n",
    "print (dev_labels.shape), \"/n\"\n",
    "\n",
    "print (\"dev_data\")\n",
    "print (\"----------\")\n",
    "print (list(dev_data))\n",
    "print (dev_data.shape), \"/n\"\n",
    "\n",
    "# print (percent of train_data and dev_data whose posts led to receipt of pizza\n",
    "print (\"train labels\")\n",
    "print (\"----------\")\n",
    "print (np.mean(train_labels)), \"/n\"\n",
    "print (\"dev labels\")\n",
    "print (\"----------\")\n",
    "print (np.mean(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  A\n",
      "1  B\n",
      "2  C\n",
      "numpy.ndarray:\n",
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "['A' 'B' 'C']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function before use\n",
    "\n",
    "data = ['A','B','C']\n",
    "df = pd.DataFrame(data=data)\n",
    "print (df)\n",
    "\n",
    "df.shape\n",
    "\n",
    "def to_np(d):\n",
    "    \n",
    "    # convert to numpy array\n",
    "    print (\"numpy.ndarray:\")\n",
    "    d = array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    print (type(d))\n",
    "    print (d.shape)\n",
    "    print (d[:3])\n",
    "    return d\n",
    "\n",
    "df = to_np(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(train_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(train_labels)', <class 'pandas.core.frame.DataFrame'>)\n",
      "('type(dev_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(dev_labels)', <class 'pandas.core.frame.DataFrame'>) /n\n",
      "pandas dataframe:\n",
      "0     I'm not in College, or a starving artist or an...\n",
      "2     I'm sure there are needier people on this subr...\n",
      "3     I've been unemployed going on three months now...\n",
      "4     I ran out of money on my meal card a while bac...\n",
      "5                                                      \n",
      "6     Hi amazing people! I've known of this subreddi...\n",
      "9     So it's finals week and I haven't eaten all da...\n",
      "11    I'll write a poem, sing a song, do a dance, pl...\n",
      "12              Thank you so much for the pizza Trisha!\n",
      "14    Hi RAoP\\n\\nAs the title suggests, I have been ...\n",
      "Name: request_text, dtype: object /n\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(3975,)\n",
      "[ u\"I'm not in College, or a starving artist or anything like that. I've just been a bit unlucky lately. I'm a 36 year old single guy with a job. But rent, and other bills killed me this month. I thought I had enough funds in my account to at least keep me set with Mr. Noodles, I forgot about my monthly banking fee.\\n\\nI had a small bag of chips Wednesday afternoon, and I get paid Monday, so I'll be fine then.. It's just really painful at this point and food is something I'm constantly thinking about.\\n\\nI've got a few bucks to get on the bus to work on Saturday, so I can't really use that.\\n\\nI'm really embarrassed to even be asking like this and I'm not sure how it works, so please be patient with me.\\n\\nI guess that covers it. Thank you in advance.\\n\\nCheers folks.\"\n",
      " u\"I'm sure there are needier people on this subreddit, but I had to borrow 30 bucks to pay the rest of my share of rent yesterday and now I'm broke until next friday.  Pizza for me and my housemate that lent me the 30 bucks would be much appreciated for dinner.  Thanks in advance, it's subreddits like this that make Reddit such an awesome place.\\nEDIT:CayucosKid got me covered, 2 pizzas, breadsticks, and a 2 liter of coke to boot!  I'll be eating good for days!  Thanks again.  (=\"\n",
      " u\"I've been unemployed going on three months now, and unfortunately my student girlfriend's part time job at the pharmacy doesn't leave much left over for anything but rice. Here's hoping for a hot pizza.\"]\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(3975,)\n",
      "[ True  True  True]\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(1696,)\n",
      "[ u\"Hello! It's been a hard 2 months with money and I listed some goodies on craigslist so I could make some food money for my family and the people never showed up =(  The cupboards are empty, no bread or P&amp;J. Is there anyone that wouldn't mind helping me out tonight? My family would appreciate it so very much. Thank you for your time in reading our plea. \\n\\nEdit: Tarn33 Came to are rescue and soon my family will be enjoying some much needed pizza!\"\n",
      " u\"I have a couple babysitting gigs lined up next week, but I won't have a single dollar until next week. I'm trying to figure something out until then, but a pizza would last me the rest of the week. I'm in Missouri. Please, for the love of pizza, show me the good side of reddit! You all have saved me in so many ways, but this is my first time asking for a real life helping hand. Thank You!\"\n",
      " u'']\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(1696,)\n",
      "[ True  True  True]\n",
      "\n",
      "('train_labels.shape:', (3975,))\n",
      "('dev_labels.shape:', (1696,))\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "# transform X to numpy array, and Y to 1-D numpy array\n",
    "# view actual text and labels\n",
    "\n",
    "train_text = train_data[\"request_text\"]\n",
    "dev_text = dev_data[\"request_text\"]\n",
    "\n",
    "print (\"type(train_text)\", type(train_text))\n",
    "print (\"type(train_labels)\", type(train_labels))\n",
    "print (\"type(dev_text)\", type(dev_text))\n",
    "print (\"type(dev_labels)\", type(dev_labels)), \"/n\"\n",
    "\n",
    "\n",
    "#from numpy import *\n",
    "\n",
    "# view as pandas dataframe\n",
    "print (\"pandas dataframe:\")\n",
    "print (train_text[:10]), \"/n\"\n",
    "\n",
    "# convert to numpy (np) array\n",
    "\n",
    "def to_np(d):\n",
    "\n",
    "    print (\"numpy.ndarray:\")\n",
    "    d = array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    print (type(d))\n",
    "    print (d.shape)\n",
    "    print (d[:3])\n",
    "    print\n",
    "    return d\n",
    "\n",
    "train_text = to_np(train_text)\n",
    "train_labels = to_np(train_labels)\n",
    "dev_text = to_np(dev_text)\n",
    "dev_labels = to_np(dev_labels)\n",
    "\n",
    "# convert to list\n",
    "\n",
    "def to_list(d):\n",
    "\n",
    "\n",
    "    print (\"list:\")\n",
    "    d = list(d)\n",
    "    print (type(d))\n",
    "    print (d[:3])\n",
    "    print ()\n",
    "    return d\n",
    "\n",
    "#train_text = to_list(train_text)\n",
    "#dev_text = to_list(dev_text)\n",
    "\n",
    "print (\"train_labels.shape:\", train_labels.shape)\n",
    "print (\"dev_labels.shape:\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is the numerica part of the model: utilize all numeric information from the dataset to \n",
    "predict the labels.  This model will be combined the text based model to improve predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on numeric data: F-Score = 0.0222, Sum of Squared Weights: 0.00\n",
      "Accuracy = 0.7400\n",
      "Coefficients for text length, month, day, hour:  [[ 0.00068276 -0.00622247 -0.0074596  -0.0104081 ]]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "#Created new df for training data(numeric only)\n",
    "train_data_numeric = np.zeros((len(train_data), 4))\n",
    "dev_data_numeric = np.zeros((len(dev_data), 4))\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "        temp = int(train_data.iloc[i,30])  #unix time for the Reddit request\n",
    "        \n",
    "        train_data_numeric[i,0]= len(train_data.iloc[i,7])  #Length of the text\n",
    "        train_data_numeric[i,1]= int(datetime.datetime.fromtimestamp(temp).strftime('%m')) # Month in integer\n",
    "        train_data_numeric[i,2]= int(datetime.datetime.fromtimestamp(temp).strftime('%d')) # Day in integer\n",
    "        train_data_numeric[i,3]= int(datetime.datetime.fromtimestamp(temp).strftime('%H')) # Hour in integer\n",
    "        \n",
    "\n",
    "#Created new df for dev data(numeric only)\n",
    "for i in range(len(dev_data)):\n",
    "        temp = int(dev_data.iloc[i,30])\n",
    "        dev_data_numeric[i,0]= len(dev_data.iloc[i,7])\n",
    "        dev_data_numeric[i,1]= int(datetime.datetime.fromtimestamp(temp).strftime('%m'))\n",
    "        dev_data_numeric[i,2]= int(datetime.datetime.fromtimestamp(temp).strftime('%d'))\n",
    "        dev_data_numeric[i,3]= int(datetime.datetime.fromtimestamp(temp).strftime('%H'))\n",
    "        \n",
    "\n",
    "#for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "logit = LogisticRegression(C=1, penalty = 'l2')\n",
    "result = logit.fit(train_data_numeric.astype(int), train_labels)\n",
    "\n",
    "preds = logit.predict(dev_data_numeric.astype(int))\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "\n",
    "print \"Logistic regression on numeric data: F-Score = %0.4f, Sum of Squared Weights: %0.2f\" \\\n",
    "%(F_Score, np.power(logit.coef_,2).sum())  #Sum up Squared Weights\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print \"Accuracy = %0.4f\" % (accuracy)\n",
    "print \"Coefficients for text length, month, day, hour: \",  result.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vectors.shape: (3975, 12313)\n",
      "dev_vectors.shape: (1696, 12313)\n",
      "------------------------------\n",
      "K Nearest Neighbors (K-NN)\n",
      "------------------------------\n",
      "K-NN: f1_score = 0.4133, k = 1\n",
      "K-NN: f1_score = 0.0259, k = 5\n",
      "K-NN: f1_score = 0.0, k = 15\n",
      "K-NN: f1_score = 0.0, k = 16\n",
      "K-NN: f1_score = 0.0, k = 17\n",
      "K-NN: f1_score = 0.0, k = 18\n",
      "K-NN: f1_score = 0.0, k = 19\n",
      "K-NN: f1_score = 0.0, k = 20\n",
      "K-NN: f1_score = 0.0, k = 28\n",
      "K-NN: f1_score = 0.0, k = 29\n",
      "K-NN: f1_score = 0.0, k = 30\n",
      "K-NN: f1_score = 0.0, k = 31\n",
      "K-NN: f1_score = 0.0, k = 32\n",
      "K-NN: f1_score = 0.0, k = 150\n",
      "K-NN: f1_score = 0.0, k = 300\n",
      "K-NN: optimal k = 1\n",
      "-----------------------------\n",
      "Bernoulli Naive Bayes (BNB)\n",
      "-----------------------------\n",
      "BNB: f1_score = 0.0, alpha = 0.0\n",
      "BNB: f1_score = 0.0844, alpha = 1e-05\n",
      "BNB: f1_score = 0.0844, alpha = 0.0001\n",
      "BNB: f1_score = 0.0844, alpha = 0.001\n",
      "BNB: f1_score = 0.0125, alpha = 0.01\n",
      "BNB: f1_score = 0.0126, alpha = 0.094\n",
      "BNB: f1_score = 0.0126, alpha = 0.095\n",
      "BNB: f1_score = 0.0126, alpha = 0.096\n",
      "BNB: f1_score = 0.0126, alpha = 0.1\n",
      "BNB: f1_score = 0.0126, alpha = 0.105\n",
      "BNB: f1_score = 0.0044, alpha = 0.2\n",
      "BNB: f1_score = 0.0045, alpha = 0.3\n",
      "BNB: f1_score = 0.0, alpha = 0.4\n",
      "BNB: f1_score = 0.0, alpha = 0.5\n",
      "BNB: f1_score = 0.0, alpha = 0.6\n",
      "BNB: f1_score = 0.0, alpha = 0.7\n",
      "BNB: f1_score = 0.0, alpha = 1.0\n",
      "BNB: f1_score = 0.0, alpha = 10.0\n",
      "Bernoulli Naive Bayes: optimal alpha = 1e-05\n",
      "------------------------\n",
      "Logistic Regression (LR)\n",
      "------------------------\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.01\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1EF0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0045, C = 0.2\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1128>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0135, C = 0.3\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0178, C = 0.4\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.5\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1390>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.54\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1EF0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.55\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.56\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.57\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1128>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.58\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0263, C = 0.59\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1EF0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0306, C = 0.6\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0475, C = 0.7\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1780>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0641, C = 0.8\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1390>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0761, C = 0.9\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0795, C = 1.0\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0828, C = 1.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1128>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2324, C = 10\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1EF0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2319, C = 12\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1390>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2415, C = 20\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2541, C = 30\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1748>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2638, C = 40\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1780>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2701, C = 50\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1390>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2739, C = 100\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE16A0>\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2755, C = 1000\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = <map object at 0x000000000EBE1128>\n",
      "Logistic Regression: optimal C = 1000\n",
      "max accuracy = 74.233490566\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "# http://stackoverflow.com/questions/209840/map-two-lists-into-a-dictionary-in-python\n",
    "# http://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "\n",
    "# Notes\n",
    "# Classifier precision--when a positive value is predicted, proportion of time the prediction is correct--equals (TP) / (TP + FP)\n",
    "# Classifier recall--when the actual value is positive, the proportion of time the prediction is correct--equals (TP) / (TP + FN)\n",
    "\n",
    "def explore_models():\n",
    "### STUDENT START ###\n",
    "\n",
    "    # create empty vector\n",
    "    accuracies = []\n",
    "\n",
    "    # Source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "    # where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "    # The relative contribution of precision and recall to the F1 score are equal. \n",
    "    # The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "    \n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    print (\"dev_vectors.shape:\", dev_vectors.shape)\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # K Nearest Neighbors\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------------\")\n",
    "    print (\"K Nearest Neighbors (K-NN)\")\n",
    "    print (\"------------------------------\")\n",
    "    \n",
    "    # Euclidean distance, when you go to 10 to 20+ dimensions, too many examples can be close to each other\n",
    "    # With K-NN on text, Cosine or Manhattan distance might be better. Cosine distance measures the angle between examples,\n",
    "    # more robust for high-dimensional problems. \n",
    "    # Dot product measures length of vectors AND angle between these vectors. \n",
    "    # With Cosine distance, you can get a value 0 to 1.\n",
    "    \n",
    "    # create two vectors\n",
    "    # ks refers to a vector of k nearest neighbor values\n",
    "    \n",
    "    ks = [1, 5, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 150, 300]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, distance='cosine', algorithm='brute')\n",
    "        knn.fit(train_vectors, train_labels)\n",
    "        pred_1 = knn.predict(dev_vectors)\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "        # f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]¶\n",
    "            # y_true = Ground truth (correct) target values \n",
    "            # y_pred = Estimated targets as returned by a classifier.\n",
    "            # average = required for multiclass/multilabel targets.\n",
    "                # 'weighted': Calculate metrics for each label, and find their average, weighted by \n",
    "                # the number of true instances for each label. This alters ‘macro’ to account for label imbalance; \n",
    "                # it can result in an F-score that is not between precision and recall.\n",
    "            \n",
    "        print (\"K-NN: f1_score = %s, k = %s\" %(round(metrics.f1_score(dev_labels, pred_1, average='binary'),4), k))\n",
    "\n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_1))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_knn = dict(zip(ks, f1_scores))\n",
    "    #print (results_knn)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"K-NN: optimal k =\", max(results_knn.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "\n",
    "    #------------------------\n",
    "    # Bernoulli Naive Bayes\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"-----------------------------\")\n",
    "    print (\"Bernoulli Naive Bayes (BNB)\")\n",
    "    print (\"-----------------------------\")\n",
    "    \n",
    "    # create two vectors\n",
    "    \n",
    "    alphas = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.094, 0.095, 0.096, 0.1, 0.105, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0, 10.0]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for a in alphas:\n",
    "        bnb = BernoulliNB(alpha=a,binarize=0.5)\n",
    "        bnb.fit(train_vectors, train_labels)\n",
    "        pred_2 = bnb.predict(dev_vectors)\n",
    "        print (\"BNB: f1_score = %s, alpha = %s\" % (round(metrics.f1_score(dev_labels, pred_2, average='binary'), 4), a))\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_2))\n",
    "        \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_bnb = dict(zip(alphas, f1_scores))\n",
    "    #print (results_mnb)\n",
    "    \n",
    "    # print (the key wit)h the max fl_score\n",
    "    print (\"Bernoulli Naive Bayes: optimal alpha =\", max(results_bnb.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # Logistic Regression\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------\")\n",
    "    print (\"Logistic Regression (LR)\")\n",
    "    print (\"------------------------\")\n",
    "    print\n",
    "    \n",
    "    # create two vectors\n",
    "    # cs refers to the vector of C (inverse of regularization strength) values\n",
    "    \n",
    "    cs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, \\\n",
    "          10, 12, 20, 30, 40, 50, 100, 1000]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for c in cs:\n",
    "        \n",
    "        # logistic regression fits a line like linear regression, but instead of predicting any number, \n",
    "        # it predicts a number between 0 and 1 (sigmoid function).\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # C (inverse of regularization strength) controls how much the weights influence the loss, and\n",
    "        # penalizes the sum of squared weights if very different weights exist between different tokens.\n",
    "  \n",
    "        # use l2 regularization, per instructions\n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "        pred_3 = lr.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-------------------------------\")\n",
    "        print (\"LR: f1_score = %s, C = %s\" % (round(metrics.f1_score(dev_labels, pred_3, average='binary'),4), c))\n",
    "        print (\"-------------------------------\")\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_3, average='binary'))\n",
    "        \n",
    "        accuracies.append((lr.score(dev_vectors, dev_labels))*100) \n",
    "\n",
    "        #print (lr.coef_.shape)\n",
    "        \n",
    "        # first define function that squares a given value, for later use in the 'for loop' below\n",
    "        fun_sq_wts = lambda x: x**2\n",
    "        \n",
    "        # use map function, likely faster (because written in C) than list comprehension.\n",
    "        # map function itself applies a function, specifically the first argument on the second argument.\n",
    "        # from coef_, take raw weights (coefficient of the features in the decision function), \n",
    "        # and sum the squares of these weights.\n",
    "\n",
    "        # note: averege=weight vs. average=default should be about same score if similar number of examples across classes\n",
    "        sq_wts = map(fun_sq_wts, lr.coef_[0])\n",
    "        sum_sq_wts =  sum(sq_wts)\n",
    "        print (\"Label = 1, sum of squared weights = %s\" % (sum_sq_wts))\n",
    "\n",
    "        print\n",
    "        \n",
    "    # map two vectors into a dictionary\n",
    "    results_lr = dict(zip(cs, f1_scores))\n",
    "    #print (results_lr)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"Logistic Regression: optimal C =\", max(results_lr.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    print (\"max accuracy =\", max(accuracies))\n",
    "        \n",
    "### STUDENT END ###\n",
    "\n",
    "explore_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "unigram\n",
      "----------\n",
      "train_vectors.shape: (3975, 12313)\n",
      "lr.coef_:\n",
      "[[-1.48092905 -0.39311615 -0.12825177 ...,  0.44453487  1.63180069\n",
      "  -0.87946286]]\n",
      "top 20:\n",
      "[3697, 4147, 6786, 8221, 1039, 6858, 3236, 1286, 6712, 10159, 1523, 10688, 4918, 10689, 9907, 7050, 3901, 2573, 2155, 9349]\n",
      "       Feature      word\n",
      "0         edit  4.824324\n",
      "1       father  4.627494\n",
      "2         mean  4.376547\n",
      "3      pockets  4.057164\n",
      "4          ass  3.995387\n",
      "5    mentioned  3.977975\n",
      "6          die  3.930683\n",
      "7        basic  3.899384\n",
      "8      married  3.850098\n",
      "9     southern -3.846223\n",
      "10       bloke  3.807984\n",
      "11    surprise  3.805289\n",
      "12   graveyard  3.749854\n",
      "13   surprised  3.697465\n",
      "14     sitting -3.697393\n",
      "15       mommy  3.658543\n",
      "16  especially  3.627059\n",
      "17  constantly  3.593246\n",
      "18      cheesy  3.548595\n",
      "19     running  3.539010\n",
      "----------\n",
      "bigram\n",
      "----------\n",
      "train_vectors.shape: (3975, 90693)\n",
      "lr.coef_:\n",
      "[[-0.31775435  0.42001296 -0.29407174 ..., -0.26401881 -0.22234693\n",
      "   0.57766744]]\n",
      "top 20:\n",
      "[23002, 38997, 37806, 32065, 28212, 73029, 33880, 44381, 54471, 23001, 81228, 41801, 77046, 2781, 47597, 9433, 84630, 16480, 57349, 59245]\n",
      "                Feature      word\n",
      "0           edit thanks  3.882043\n",
      "1             imgur com  3.879634\n",
      "2            http imgur  3.547851\n",
      "3             got pizza  3.038731\n",
      "4         forward money  2.984111\n",
      "5        sounds amazing  2.960338\n",
      "6        happy birthday  2.947813\n",
      "7   letsfytinglove best  2.947813\n",
      "8        north carolina  2.936326\n",
      "9            edit thank  2.927036\n",
      "10      tonight greatly  2.813269\n",
      "11           just spent  2.809594\n",
      "12         surprise son  2.768559\n",
      "13         afford ramen  2.674511\n",
      "14             love pie  2.633101\n",
      "15         broke payday  2.627687\n",
      "16               ve got  2.625031\n",
      "17        craving pizza -2.599092\n",
      "18          pay forward  2.518712\n",
      "19       pizza actually  2.512937\n"
     ]
    }
   ],
   "source": [
    "from pandas import *\n",
    "from sklearn.feature_selection import *\n",
    "\n",
    "# Feature Selection Notes:\n",
    "'''\n",
    "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py\n",
    "\n",
    "These objects take as input a scoring function that returns univariate p-values:\n",
    "-For regression: f_regression\n",
    "-For classification: chi2 or f_classif\n",
    "\n",
    "Feature selection with sparse data:\n",
    "-If you use sparse data (i.e. data represented as sparse matrices), \n",
    "only chi2 will deal with the data without making it dense.\n",
    "-Warning: Beware not to use a regression scoring function with a classification problem, \n",
    "you will get useless results.\n",
    "\n",
    "With SVMs and logistic-regression, the parameter C controls the sparsity: \n",
    "the smaller C the less features selected. \n",
    "'''\n",
    "\n",
    "def top20(type):\n",
    "### STUDENT START ###\n",
    "\n",
    "    if type == \"unigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words. \n",
    "        # only applies if default analyzer='word'.\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"unigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "        \n",
    "    elif type == \"bigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words from the resulting tokens. \n",
    "        # only applies if default analyzer='word'.\n",
    "        # set bigrams to be 2 words only\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"bigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "      \n",
    "    # use C=12\n",
    "    for c in [12]:\n",
    "        \n",
    "        # in the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the default ‘multi_class’ option is set to ‘ovr’ \n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        #print (lr)\n",
    "        \n",
    "        # fit the model and generate coef_\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "         \n",
    "        # interested in magnitude of the weights (coefficients), so take absolute value.\n",
    "        # sort absolute values in descending order.\n",
    "        # important to know if negative or positive weight, so still output the positive/negative sign.\n",
    "        # after fitting logistic regression for class vs. all other classes, negative weight of a token \n",
    "        # indicates a class other than class of interest.\n",
    "        # (visual example of negative and positive on a sigmoid function helps) \n",
    "        \n",
    "        print (\"lr.coef_:\")\n",
    "        print (lr.coef_)\n",
    "        print\n",
    "        \n",
    "        # for each label, store the column indices of the top 5 weights \n",
    "        top20 = sorted(range(len(lr.coef_[0])), key=lambda i: abs(lr.coef_[0][i]), reverse=True)[:20]\n",
    "       \n",
    "        col_1 = []\n",
    "        \n",
    "        # for each label, access and store weights via column indices\n",
    "        for index in (top20):\n",
    "\n",
    "            col_1.append(lr.coef_[0][index])\n",
    "           \n",
    "        print (\"top 20:\" )\n",
    "        print (top20)\n",
    "        print\n",
    "        \n",
    "        # store feature names, after converting to an array\n",
    "        feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "       \n",
    "        # create a Pandas dataframe with 20 rows and 4 columns, plus descriptive headers\n",
    "        df = DataFrame({'Feature': feature_names[top20], 'word': col_1})\n",
    "        print (df    )\n",
    "\n",
    "#-----\n",
    "         \n",
    "### STUDENT END ###\n",
    "top20(\"unigram\")\n",
    "top20(\"bigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "C =  0.01\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L1 regularization: number of non-zero weights = 0\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "-----------------\n",
      "C =  0.03\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L1 regularization: number of non-zero weights = 0\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "-----------------\n",
      "C =  0.05\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L1 regularization: number of non-zero weights = 0\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "-----------------\n",
      "C =  0.07\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L1 regularization: number of non-zero weights = 0\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "-----------------\n",
      "C =  0.1\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L1 regularization: number of non-zero weights = 0\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "-----------------\n",
      "C =  0.3\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0135\n",
      "LR L2 regularization: f1_score = 0.0135\n",
      "LR L1 regularization: number of non-zero weights = 8\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 74.06%\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L2 regularization: number of non-zero weights: 8\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: vocab size: 8\n",
      "-----------------\n",
      "C =  0.5\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.022\n",
      "LR L2 regularization: f1_score = 0.0177\n",
      "LR L1 regularization: number of non-zero weights = 36\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 73.76%\n",
      "LR L2 regularization: accuracy = 73.88%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L2 regularization: number of non-zero weights: 36\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: vocab size: 36\n",
      "-----------------\n",
      "C =  0.57\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0306\n",
      "LR L2 regularization: f1_score = 0.0177\n",
      "LR L1 regularization: number of non-zero weights = 43\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 73.88%\n",
      "LR L2 regularization: accuracy = 73.76%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L2 regularization: number of non-zero weights: 43\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: vocab size: 43\n",
      "-----------------\n",
      "C =  0.7\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0472\n",
      "LR L2 regularization: f1_score = 0.0475\n",
      "LR L1 regularization: number of non-zero weights = 64\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 73.82%\n",
      "LR L2 regularization: accuracy = 74.00%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L2 regularization: number of non-zero weights: 64\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: vocab size: 64\n",
      "-----------------\n",
      "C =  1\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.0708\n",
      "LR L2 regularization: f1_score = 0.0833\n",
      "LR L1 regularization: number of non-zero weights = 129\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 73.70%\n",
      "LR L2 regularization: accuracy = 74.06%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.0\n",
      "LR L2 regularization: number of non-zero weights: 129\n",
      "LR L2 regularization: accuracy = 74.12%\n",
      "LR L2 regularization: vocab size: 129\n",
      "-----------------\n",
      "C =  10\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2667\n",
      "LR L2 regularization: f1_score = 0.2371\n",
      "LR L1 regularization: number of non-zero weights = 1785\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 66.92%\n",
      "LR L2 regularization: accuracy = 70.40%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.244\n",
      "LR L2 regularization: number of non-zero weights: 1785\n",
      "LR L2 regularization: accuracy = 70.05%\n",
      "LR L2 regularization: vocab size: 1785\n",
      "-----------------\n",
      "C =  12\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2772\n",
      "LR L2 regularization: f1_score = 0.2376\n",
      "LR L1 regularization: number of non-zero weights = 1853\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 67.10%\n",
      "LR L2 regularization: accuracy = 70.11%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2543\n",
      "LR L2 regularization: number of non-zero weights: 1853\n",
      "LR L2 regularization: accuracy = 69.58%\n",
      "LR L2 regularization: vocab size: 1853\n",
      "-----------------\n",
      "C =  30\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2765\n",
      "LR L2 regularization: f1_score = 0.2521\n",
      "LR L1 regularization: number of non-zero weights = 2247\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.45%\n",
      "LR L2 regularization: accuracy = 68.16%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2869\n",
      "LR L2 regularization: number of non-zero weights: 2247\n",
      "LR L2 regularization: accuracy = 68.34%\n",
      "LR L2 regularization: vocab size: 2247\n",
      "-----------------\n",
      "C =  50\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2753\n",
      "LR L2 regularization: f1_score = 0.2642\n",
      "LR L1 regularization: number of non-zero weights = 2723\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.86%\n",
      "LR L2 regularization: accuracy = 67.81%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2751\n",
      "LR L2 regularization: number of non-zero weights: 2723\n",
      "LR L2 regularization: accuracy = 67.69%\n",
      "LR L2 regularization: vocab size: 2723\n",
      "-----------------\n",
      "C =  70\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.279\n",
      "LR L2 regularization: f1_score = 0.2703\n",
      "LR L1 regularization: number of non-zero weights = 2679\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.57%\n",
      "LR L2 regularization: accuracy = 67.22%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2835\n",
      "LR L2 regularization: number of non-zero weights: 2679\n",
      "LR L2 regularization: accuracy = 67.51%\n",
      "LR L2 regularization: vocab size: 2679\n",
      "-----------------\n",
      "C =  100\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2741\n",
      "LR L2 regularization: f1_score = 0.276\n",
      "LR L1 regularization: number of non-zero weights = 2876\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.33%\n",
      "LR L2 regularization: accuracy = 67.22%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2901\n",
      "LR L2 regularization: number of non-zero weights: 2876\n",
      "LR L2 regularization: accuracy = 67.39%\n",
      "LR L2 regularization: vocab size: 2876\n",
      "-----------------\n",
      "C =  200\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2613\n",
      "LR L2 regularization: f1_score = 0.2754\n",
      "LR L1 regularization: number of non-zero weights = 4153\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.33%\n",
      "LR L2 regularization: accuracy = 66.80%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2478\n",
      "LR L2 regularization: number of non-zero weights: 4153\n",
      "LR L2 regularization: accuracy = 65.27%\n",
      "LR L2 regularization: vocab size: 4153\n",
      "-----------------\n",
      "C =  300\n",
      "-----------------\n",
      "LR L1 regularization: f1_score = 0.2609\n",
      "LR L2 regularization: f1_score = 0.2727\n",
      "LR L1 regularization: number of non-zero weights = 4473\n",
      "LR L2 regularization: number of non-zero weights = 12313\n",
      "\n",
      "LR L1 regularization: accuracy = 65.92%\n",
      "LR L2 regularization: accuracy = 66.04%\n",
      "***Re-trained model w/ L1 non-zero features***\n",
      "LR L2 regularization: f1_score = 0.2645\n",
      "LR L2 regularization: number of non-zero weights: 4473\n",
      "LR L2 regularization: accuracy = 65.57%\n",
      "LR L2 regularization: vocab size: 4473\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzEocHMzMrMTBwczMShwczMys5P8DV8VJhZATYM4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd020668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define function fs (feature selection)\n",
    "\n",
    "def fs():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    \n",
    "    # create two empty vectors\n",
    "    accuracies = []\n",
    "    vocab_size = []\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "\n",
    "    ### Logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size.\n",
    "    ### For this size, the default regularization, L2, computes the sum of the squared weights (see P3, above), while \n",
    "    ### L1 regularization computes the sum of the absolute values of the weights. \n",
    "    ### L2 regularization makes all the weights relatively small, whereas\n",
    "    ### L1 regularization drives lots of the weights to 0, effectively removing unimportant features [for feature selection].\n",
    "\n",
    "    ### http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html\n",
    "     \n",
    "    # set min_df=10 to ignore words that appear in less than 10 documents\n",
    "    # use stop_words='english' to remove less meaningful words from the resulting tokens, only applies if default analyzer='word'.\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    dev_vectors = vectorizer.transform(dev_text)    \n",
    "    \n",
    "    cs = [0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.57, 0.7, 1, 10, 12, 30, 50, 70, 100, 200, 300]\n",
    "    # no longer use np.linspace to return evenly spaced numbers over a specified interval.\n",
    "    # it offers less control.\n",
    "    \n",
    "    for c in cs:\n",
    "\n",
    "        # fit l1 and l2 models\n",
    "        lr_l1 = LogisticRegression(C=c, penalty='l1', tol=0.01)\n",
    "        lr_l2 = LogisticRegression(C=c, penalty='l2', tol=0.01)\n",
    "        lr_l1.fit(train_vectors, train_labels)\n",
    "        lr_l2.fit(train_vectors, train_labels)\n",
    "        \n",
    "        # store predictions\n",
    "        pred_l1 = lr_l1.predict(dev_vectors)\n",
    "        pred_l2 = lr_l2.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-----------------\")\n",
    "        print (\"C = \", round(c,3))\n",
    "        print (\"-----------------\")\n",
    "        \n",
    "        print (\"LR L1 regularization: f1_score = %s\" % (round(metrics.f1_score(dev_labels, pred_l1, average='binary'),4)))\n",
    "        print (\"LR L2 regularization: f1_score = %s\" % (round(metrics.f1_score(dev_labels, pred_l2, average='binary'),4)))\n",
    "        print\n",
    "        \n",
    "        #print (\"lr_l1.coef_:\", lr_l1.coef_\n",
    "        #print (\"lr_l2.coef_:\", lr_l2.coef_\n",
    "        \n",
    "        # take mean weight for each class\n",
    "        # axis=0 refers to mean of each column across 4 rows in coef_\n",
    "        # use as definition of sparsity\n",
    "        vec1 = np.mean(lr_l1.coef_, axis=0)\n",
    "        vec2 = np.mean(lr_l2.coef_, axis=0)\n",
    "        \n",
    "        #print (\"vec1:\", vec1\n",
    "        #print (\"vec2:\", vec2\n",
    "        \n",
    "        print (\"LR L1 regularization: number of non-zero weights =\", (vec1 != 0).sum())\n",
    "        print (\"LR L2 regularization: number of non-zero weights =\", (vec2 != 0).sum())\n",
    "        print ()\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score\n",
    "        # score(X, y, sample_weight=None)\n",
    "        # Returns the mean accuracy on the given dev or test data and labels\n",
    "        # In multi-label classification, this is the subset accuracy which is a harsh metric \n",
    "        # since you require for each sample that each label set be correctly predicted.\n",
    "        \n",
    "        print (\"LR L1 regularization: accuracy = %.2f%%\" % ((lr_l1.score(dev_vectors, dev_labels))*100))\n",
    "        print (\"LR L2 regularization: accuracy = %.2f%%\" % ((lr_l2.score(dev_vectors, dev_labels))*100))\n",
    "        print\n",
    "        \n",
    "        #print (\"recheck\", train_vectors.shape\n",
    "        #print (\"recheck\", train_labels.shape\n",
    "        \n",
    "        #---------------\n",
    "        # re-train model\n",
    "        #---------------\n",
    "        \n",
    "        # likely no need to use fit_transform again, as we still have our vocabulary in matrix format with token counts.\n",
    "        # we simply select non-zero weighted features (from columns), and leave documents (from rows) as is.\n",
    "        \n",
    "        # first, only select features that have non-zero weights from L1 regularization.\n",
    "        # vec1 includes weights for each feature (column).\n",
    "        train_vectors_rt = train_vectors[:, vec1 != 0]\n",
    "        dev_vectors_rt = dev_vectors[:, vec1 != 0]\n",
    "        \n",
    "        #print (train_vectors_rt\n",
    "        \n",
    "        #print (\"recheck\", train_vectors_rt.shape\n",
    "        #print (\"recheck\", train_labels.shape\n",
    "        \n",
    "        lr_l2_rt = LogisticRegression(C=c, penalty='l2', tol=0.1)\n",
    "    \n",
    "        # refit our classifier to the model, so it can learn from the model\n",
    "        # if the number of features >= 1 from L1 for L2\n",
    "        if train_vectors_rt.shape[1] >= 1:\n",
    "        \n",
    "            lr_l2_rt.fit(train_vectors_rt, train_labels)\n",
    "            pred_l2_rt = lr_l2_rt.predict(dev_vectors_rt)\n",
    "\n",
    "            # take mean weight for each class\n",
    "            # axis=0 refers to mean of each column across 4 rows in coef_\n",
    "            # use as definition of sparsity\n",
    "            vec_rt = np.mean(lr_l2_rt.coef_, axis=0)\n",
    "\n",
    "            # append to vectors\n",
    "            # note: try .score method (mean accuracy on the given test data and labels) rather than f1_score method,\n",
    "            #        partly because sometimes the output cell shows a system automated warning about the f1_score\n",
    "            accuracies.append((lr_l2_rt.score(dev_vectors_rt, dev_labels))*100)  \n",
    "            vocab_size.append(train_vectors_rt.shape[1])\n",
    "\n",
    "            print (\"***Re-trained model w/ L1 non-zero features***\" )\n",
    "            print (\"LR L2 regularization: f1_score = %s\" % (round(metrics.f1_score(dev_labels, pred_l2_rt, average='binary'),4)))\n",
    "            print (\"LR L2 regularization: number of non-zero weights:\", (vec_rt != 0).sum())\n",
    "            print (\"LR L2 regularization: accuracy = %.2f%%\" % ((lr_l2_rt.score(dev_vectors_rt, dev_labels))*100))\n",
    "            print\n",
    "            print (\"LR L2 regularization: vocab size:\", (train_vectors_rt.shape[1]))\n",
    "            print\n",
    "\n",
    "        #print (accuracies\n",
    "        #print (vocab_size\n",
    "        #print\n",
    "    \n",
    "    plt.scatter(vocab_size, accuracies)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.title('Relationship between Accuracy and Vocabulary Size')\n",
    "\n",
    "    ### STUDENT END ###\n",
    "fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 12313\n",
      "[[ 0.21105621  0.78894379]\n",
      " [ 0.99504166  0.00495834]\n",
      " [ 0.93734475  0.06265525]\n",
      " ..., \n",
      " [ 0.98867734  0.01132266]\n",
      " [ 0.4827782   0.5172218 ]\n",
      " [ 0.18666835  0.81333165]]\n",
      "************************************************\n",
      "dev indices of top 3 R_rates: [469, 1221, 489]\n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 10674.1932626\n",
      "label probabilities: [  9.99906325e-01   9.36751191e-05]\n",
      "Max probability dev_label -> False\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "Hi first time posting here , I'm extremely hungry right now haven't eaten much today except junk food because there isn't much food at home I like.\n",
      "\n",
      "I have a decent amount of allergies one of them being soy sauce but my mom still made Stir Fry which has it in it , No she is not a horrible person I just don't like getting heartburn anymore so I generally don't eat it. Fun Fact : I am actually allergic to chicken but can eat it 'Most of the time'. \n",
      "\n",
      "I am also transgender not really looking for sympathy but the bar on the right says to list my current situation I also love Parkour that's about all there is to Know I am asking for just a Pepperoni pizza from Papa John's.\n",
      "\n",
      "If I messed up at all in here please correct me I read the right side bar a couple times but I feel like I am fucking up.\n",
      "\n",
      "Also my family is not poor feel free to give to someone else in actually NEED for the food I am just hungry and bored.\n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 4945.36566736\n",
      "label probabilities: [  9.99797831e-01   2.02168636e-04]\n",
      "Max probability dev_label -> False\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "Last few days alone in my student house before i'm homeless. Would really appreciate a solid meal to cheer me up and give me the energy to carry on looking for jobs/somewhere to live. I'm sure there are people more worthy than I, but i feel i've exhausted everything else bar soup kitchens. I'm pretty new to all this,, but i'm sure i'll be back on my feet soon and will be happy to repay the favour to another worthy redditor.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 4667.48548845\n",
      "label probabilities: [  9.99785798e-01   2.14202229e-04]\n",
      "Max probability dev_label -> False\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "i think we all know why we are here, and mabey this is the first step to admitting how depressed i am, but ive run out of food and could use a bite to eat to make it through the week &lt;3. Im in Manchester NH. Not goin anywhere so feel free AMA heh.\n",
      "************************************************\n",
      "dev indices of bottom 3 R_rates: [0, 11, 12]\n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 1.0\n",
      "label probabilities: [ 0.21105621  0.78894379]\n",
      "Max probability dev_label -> True\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "Hello! It's been a hard 2 months with money and I listed some goodies on craigslist so I could make some food money for my family and the people never showed up =(  The cupboards are empty, no bread or P&amp;J. Is there anyone that wouldn't mind helping me out tonight? My family would appreciate it so very much. Thank you for your time in reading our plea. \n",
      "\n",
      "Edit: Tarn33 Came to are rescue and soon my family will be enjoying some much needed pizza!\n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 1.0\n",
      "label probabilities: [ 0.37796944  0.62203056]\n",
      "Max probability dev_label -> True\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "I'm 30 weeks pregnant and desperately craving pizza, but I can't really justify spending the money on it while I'm saving up for the baby's arrival.  If anyone wants to help me out it would be incredible! \n",
      "---------------------------------------------------------------------\n",
      "W207 Results\n",
      "------------\n",
      "R_rate: 1.0\n",
      "label probabilities: [ 0.36572751  0.63427249]\n",
      "Max probability dev_label -> True\n",
      "Correct dev_label -> True\n",
      "dev_text below:\n",
      "---------------------------------------------------------------------\n",
      "My bank account is completely empty (http://i.imgur.com/Sl7GO.jpg), and I'm eating my last can of pork and beans right now. Financial aid doesn't come in for another month, and I could really use something delicious for dinner tonight.\n",
      "\n",
      "I mean, I have other foodstuffs, I'd just rather I didn't have to eat rice pilaf again.\n"
     ]
    }
   ],
   "source": [
    "# define function fs2 (feature selection, method 2)\n",
    "\n",
    "def fs2():\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    # CountVectorizer:\n",
    "    # Tokenize the documents and count the occurrences of token and return them as a sparse matrix\n",
    "\n",
    "    # TfidfTransformer:\n",
    "    # Apply Term Frequency Inverse Document Frequency normalization to a sparse matrix of occurrence counts\n",
    "    \n",
    "    # Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency\n",
    "    # This is a common term weighting scheme in information retrieval, \n",
    "    # that has also found good use in document classification.\n",
    "    # The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to \n",
    "    # scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically \n",
    "    # less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    \n",
    "    # apply the CountVectorizer fit_transform method -- which includes two methods in one -- on the train_text.\n",
    "    # learn the vocabulary dictionary (all tokens from the raw documents) and return a matrix, \n",
    "    # extracting token counts to the cells.\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    \n",
    "    # apply the transform method to the dev_text\n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    \n",
    "    # transform train_text to matrix and print (count of rows and columns\n",
    "    # 2034 documents, 3064 words\n",
    "    print (\"vocabulary size:\", train_vectors.toarray().shape[1] )\n",
    "    print\n",
    "    \n",
    "    lr = LogisticRegression(penalty='l2', C=100)\n",
    "    lr.fit(train_vectors, train_labels)\n",
    "    pred_4 = lr.predict(dev_vectors)\n",
    "\n",
    "    # for each documents, store and print (predicted probabilities that it belongs to each class\n",
    "    \n",
    "    # use the method, predict_proba\n",
    "    \n",
    "    # for each document in dev_vectors, get their probability estimates for all classes \n",
    "    p = lr.predict_proba(dev_vectors)\n",
    "    print (p)\n",
    "\n",
    "    # create an empty vector\n",
    "    \n",
    "    p_max_rates = []\n",
    "    R_rates = []\n",
    "    \n",
    "    # iterate over each row (document) of p\n",
    "    for i, p_docs in enumerate(p):\n",
    "        # p_docs is a 1x2 vector from p with a document's probability to each class on one row\n",
    "        # take the document's probability of the correct label\n",
    "        p_correct_class = p_docs[dev_labels[i]]\n",
    "        # take the document's max probability across the 4 labels\n",
    "        p_max = p_docs.max()\n",
    "\n",
    "        p_max_rates.append(p_max)\n",
    "        \n",
    "        # calculate R\n",
    "        R = p_max / p_correct_class\n",
    "\n",
    "        # append to the R_rates vector\n",
    "        R_rates.append(R)\n",
    "\n",
    "    # create vector that have indices of top 3 R_rates\n",
    "    top3_index = sorted(range(dev_vectors.shape[0]), key=lambda i: R_rates[i], reverse=True)[:3]\n",
    "    print (\"************************************************\")\n",
    "    print\n",
    "    print (\"dev indices of top 3 R_rates:\", top3_index)\n",
    "    print\n",
    "\n",
    "    adhoc_label = ['False','True']\n",
    "    \n",
    "    for i in top3_index:\n",
    "        \n",
    "        # find index of max probability within each row\n",
    "        # np.argmax returns the indices of the maximum values along an axis\n",
    "        index_max_prob = np.argmax(p[i,:])\n",
    "                                   \n",
    "        print (\"---------------------------------------------------------------------\")\n",
    "        print (\"W207 Results\")\n",
    "        print (\"------------\")\n",
    "        print (\"R_rate:\", R_rates[i])\n",
    "        print (\"label probabilities:\", p[i,:])\n",
    "        print (\"Max probability dev_label -> %s\" % (adhoc_label[index_max_prob]))\n",
    "        print (\"Correct dev_label -> %s\" % (dev_labels[i]))\n",
    "        print (\"dev_text below:\")\n",
    "        print (\"---------------------------------------------------------------------\")\n",
    "        print\n",
    "        print (dev_text[i])\n",
    "        print\n",
    "\n",
    "    # create vector that have indices of bottom 3 R_rates\n",
    "    bottom3_index = sorted(range(dev_vectors.shape[0]), key=lambda i: R_rates[i])[:3]\n",
    "    print (\"************************************************\")\n",
    "    print\n",
    "    print (\"dev indices of bottom 3 R_rates:\", bottom3_index)\n",
    "    print\n",
    "    \n",
    "    for i in bottom3_index:\n",
    "        \n",
    "        # find index of max probability within each row\n",
    "        # np.argmax returns the indices of the maximum values along an axis\n",
    "        index_max_prob = np.argmax(p[i,:])\n",
    "                                   \n",
    "        print (\"---------------------------------------------------------------------\")\n",
    "        print (\"W207 Results\")\n",
    "        print (\"------------\")\n",
    "        print (\"R_rate:\", R_rates[i])\n",
    "        print (\"label probabilities:\", p[i,:])\n",
    "        print (\"Max probability dev_label -> %s\" % (adhoc_label[index_max_prob]))\n",
    "        print (\"Correct dev_label -> %s\" % (dev_labels[i]))\n",
    "        print (\"dev_text below:\")\n",
    "        print (\"---------------------------------------------------------------------\")\n",
    "        print \n",
    "        print (dev_text[i])\n",
    "        print\n",
    "        \n",
    "        \n",
    "    ### STUDENT END ###\n",
    "fs2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1)Baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301158\n",
      "precision_score: 0.346154\n",
      "recall_score: 0.266515\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary'))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2)Add preprocessing and L1 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "    #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "    #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "    #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "    #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "    #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "    #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "    #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=L1)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=L2 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "    \n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=550 ,l2=16 , f1 score is 0.330368\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,700,10):\n",
    "    for j in range(1,50,1):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "#         print('When l1=%i ,l2=%i , taccuracy is %f' %(i,j,acc))\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>Add Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=520 ,l2=5 , f1 score is 0.338812\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor = first_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,600,10):\n",
    "    for j in range(1,50,2):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3)NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.300908\n",
      "accuracy_score: 0.349398\n",
      "recall_score: 0.264237\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = empty_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=520)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=5)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process[:,index]),average='binary'))\n",
    "print('accuracy_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process[:,index])))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process[:,index])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of features based on the L1 regularization, instead of inputting all words into PCA to improve the speed\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560) #C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first 1000 element, 0.962043 of the total variance in the training data is explained \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xcc877f0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFWRJREFUeJzt3X+sHeV95/H3F1//4EdihyQ18Y8IiE2LkQC3DThZurlN\nk+JaFU4rtcFqIOuVEqTESrartCzpH3HUSrustAogtNTaQOUkbJw2NNTROgs04VaorQzOGkKwDb4h\nDraRTYvtlBDwD/zdP2YuPpz43nOvOdczvs/7JY3OzDPPnHnmkX0+95kf50RmIkkq11lNN0CS1CyD\nQJIKZxBIUuEMAkkqnEEgSYUzCCSpcD2DICKWR8SOiNgZETePUueOev0TEbG0o3xXRPwgIrZGxKP9\nbLgkqT8GxloZEdOAO4EPAXuBxyJiY2Zu76izAliUmYsj4mrgLmBZvTqBwcw8MCmtlyS9ab1GBFcB\nw5m5KzOPAhuAlV11rgPWA2TmZmBORMztWB/9aqwkqf96BcF8YHfH8p66bLx1Evj7iNgSEZ94Mw2V\nJE2OMU8NUX2Qj8dof/Vfk5nPR8Q7gYciYkdmPjL+5kmSJluvINgLLOxYXkj1F/9YdRbUZWTm8/Xr\nv0TEt6hONb0hCCLCLzuSpFOQmX059d7r1NAWYHFEXBgRM4CPAhu76mwEbgSIiGXAoczcHxHnRMRb\n6vJzgd8GnjzZTjLTKZMvfOELjbehLZN9YV/YF2NP/TTmiCAzj0XEGuABYBpwd2Zuj4ib6vXrMnNT\nRKyIiGHgZWB1vfkFwN9GxMh+7s3MB/vaeknSm9br1BCZ+R3gO11l67qW15xku2eBK99sAyVJk8sn\ni1tkcHCw6Sa0hn1xgn1xgn0xOaLf55om3ICIbLoNknSmiQjyNF0sliRNcQaBJBXOIJCkwhkEklQ4\ng0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMI\nJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CS\nCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVrmcQRMTyiNgRETsj4uZR6txRr38iIpZ2rZsWEVsj4tv9\narQkqX/GDIKImAbcCSwHlgCrIuLSrjorgEWZuRj4JHBX19t8FtgGZL8aLUnqn14jgquA4czclZlH\ngQ3Ayq461wHrATJzMzAnIuYCRMQCYAXwZSD62XBJUn/0CoL5wO6O5T112XjrfAn4E+D4m2ijJGkS\nDfRYP97TOd1/7UdE/C7wQmZujYjBsTZeu3bt6/ODg4MMDo5ZXZKKMzQ0xNDQ0KS8d2SO/lkfEcuA\ntZm5vF6+BTiembd21PlLYCgzN9TLO4BB4DPADcAxYBbwVuC+zLyxax85VhskSb8oIsjMvpxy73Vq\naAuwOCIujIgZwEeBjV11NgI31g1bBhzKzH2Z+fnMXJiZFwHXA9/rDgFJUvPGPDWUmcciYg3wADAN\nuDszt0fETfX6dZm5KSJWRMQw8DKwerS362fDJUn9MeapodPSAE8NSdKEnc5TQ5KkKc4gkKTCGQSS\nVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBWuFUHgg8WS1ByDQJIK14ogOO7P1khSY1oRBI4I\nJKk5BoEkFc4gkKTCGQSSVDiDQJIK14og8K4hSWpOK4LAEYEkNccgkKTCGQSSVDiDQJIKZxBIUuFa\nEQTeNSRJzWlFEDgikKTmGASSVDiDQJIKZxBIUuEMAkkqnEEgSYVrRRB4+6gkNacVQeCIQJKaYxBI\nUuEMAkkqnEEgSYXrGQQRsTwidkTEzoi4eZQ6d9Trn4iIpXXZrIjYHBGPR8S2iPivo+3DIJCk5owZ\nBBExDbgTWA4sAVZFxKVddVYAizJzMfBJ4C6AzHwV+M3MvBK4HPjNiLjmZPvxriFJak6vEcFVwHBm\n7srMo8AGYGVXneuA9QCZuRmYExFz6+Wf13VmANOAAyfbiSMCSWpOryCYD+zuWN5Tl/WqswCqEUVE\nPA7sBx7OzG0n24lBIEnN6RUE4/2IjpNtl5mv1aeGFgD/PiIGT7oTg0CSGjPQY/1eYGHH8kKqv/jH\nqrOgLntdZv40Iv4P8OvAUPdObr99LW9/ezU/ODjI4OBg75ZLUkGGhoYYGhqalPeOHOPP8YgYAJ4G\nfgt4HngUWJWZ2zvqrADWZOaKiFgG3JaZyyLiHcCxzDwUEWcDDwBfzMzvdu0jn346ueSSvh+bJE1Z\nEUFmdp+NOSVjjggy81hErKH6EJ8G3J2Z2yPipnr9uszcFBErImIYeBlYXW/+LmB9RJxFdQrqq90h\nMMK7hiSpOWOOCE5LAyJy27bk0kt715UkVfo5IvDJYkkqnEEgSYVrRRB4jUCSmtOKIHBEIEnNMQgk\nqXAGgSQVrhVB4DUCSWpOK4LAEYEkNccgkKTCGQSSVLhWBIHXCCSpOa0IAkcEktQcg0CSCmcQSFLh\nDAJJKlwrgsCLxZLUnFYEgSMCSWqOQSBJhTMIJKlwrQgCrxFIUnNaEQSOCCSpOQaBJBXOIJCkwrUi\nCLxGIEnNaUUQOCKQpOYYBJJUOINAkgrXiiDwGoEkNacVQeCIQJKaYxBIUuEMAkkqnEEgSYVrRRB4\nsViSmtOKIHBEIEnNMQgkqXAGgSQVblxBEBHLI2JHROyMiJtHqXNHvf6JiFhaly2MiIcj4qmI+GFE\nfOZk23qNQJKa0zMIImIacCewHFgCrIqIS7vqrAAWZeZi4JPAXfWqo8AfZ+ZlwDLg093bgiMCSWrS\neEYEVwHDmbkrM48CG4CVXXWuA9YDZOZmYE5EzM3MfZn5eF3+M2A7MK97BwaBJDVnPEEwH9jdsbyn\nLutVZ0FnhYi4EFgKbO7egUEgSc0ZTxCM92M6RtsuIs4Dvgl8th4ZvIHXCCSpOQPjqLMXWNixvJDq\nL/6x6iyoy4iI6cB9wNcy8/6T7eC++9YyPFzNDw4OMjg4OI5mSVI5hoaGGBoampT3juxxXiYiBoCn\ngd8CngceBVZl5vaOOiuANZm5IiKWAbdl5rKICKprBy9m5h+P8v65fn1y4439OSBJKkFEkJndZ2JO\nSc8RQWYei4g1wAPANODuzNweETfV69dl5qaIWBERw8DLwOp6838HfAz4QURsrctuycz/+8Z99ONQ\nJEmnoueIYNIbEJH33JOsXt27riSp0s8RgU8WS1LhDAJJKpxBIEmFMwgkqXCtCAIfKJOk5rQiCBwR\nSFJzDAJJKpxBIEmFa0UQeI1AkprTiiBwRCBJzTEIJKlwBoEkFa4VQeA1AklqTiuCwBGBJDXHIJCk\nwhkEklS4VgSB1wgkqTmtCAJHBJLUHINAkgpnEEhS4VoRBF4jkKTmtCIIXn216RZIUrlaEQQvvdR0\nCySpXAaBJBXOIJCkwhkEklQ4g0CSCteKIPjXf226BZJUrlYEwe7djgokqSmtCIIrroDvf7/pVkhS\nmVoRBO99L2zZ0nQrJKlMrQiCiy+G555ruhWSVKZWBMG8efD88023QpLK1Jog2Lu36VZIUpkiG/4O\n6IjIF19MLrwQDhyAgYFGmyNJZ4SIIDOjH+/VihHB+efD/PmwbVvTLZGk8vQMgohYHhE7ImJnRNw8\nSp076vVPRMTSjvJ7ImJ/RDzZaz+XXWYQSFITxgyCiJgG3AksB5YAqyLi0q46K4BFmbkY+CRwV8fq\nv6q37enyy+GxxybQcklSX/QaEVwFDGfmrsw8CmwAVnbVuQ5YD5CZm4E5EXFBvfwIcHA8DVm+HB58\ncCJNlyT1Q68gmA/s7ljeU5dNtE5Pv/zL8OMf+/vFknS69bpHZ7wfy91Xrif0cb527VoAjh2DjRsH\nWblycCKbS9KUNzQ0xNDQ0KS8d68g2Ass7FheSPUX/1h1FtRl4zYSBA8+CGefPZEtJakMg4ODDA4O\nvr78xS9+sW/v3evU0BZgcURcGBEzgI8CG7vqbARuBIiIZcChzNx/Ko358IfhH/7hVLaUJJ2qMYMg\nM48Ba4AHgG3ANzJze0TcFBE31XU2Ac9GxDCwDvjUyPYR8XXgn4BLImJ3RKwea38XXww/+cmbOh5J\n0gS14snikTYMDcGf/Rn84z822iRJar0p92TxiKVL4ZlnYOfOplsiSeVoVRDMng3LlsFTTzXdEkkq\nR6uCAGDxYkcEknQ6GQSSVLjWBcGiRTA83HQrJKkcrQuCJUvghz/0qyYk6XRpXRDMmwcR/oaxJJ0u\nrQuCCPjIR+Dee5tuiSSVoXVBAPD7vw/f+U7TrZCkMrTqyeIRP/85zJ1bfd3E+ec31DBJarEp+2Tx\niHPOgY99DD73uaZbIklTXytHBAAHD8K73w379sG55zbQMElqsSk/IgB429vgve+F73636ZZI0tTW\n2iAAuOEG+Iu/aLoVkjS1tToIPv5x2LsX7r+/6ZZI0tTV6iA46yz42tfg05+G/af0m2eSpF5ae7G4\n0+c+B9u2waZNp6lRktRy/bxYfEYEweHDcNll8Ad/UF0zmDbtNDVOklqqiLuGOs2cCf/8z9UP219+\nuaeJJKmfzogRwYhM+MQnYNYsuPPOSW6YJLVYcSOCERFw663VswVf+lLTrZGkqeGMGhGMeO45uOaa\n6veN77kHzjtvkhonSS1V7IhgxLvfDTt2VE8fX3YZfOMbTbdIks5cZ+SIoNPDD8P111cPn916a3X6\nSJKmuuJuH+1l9274wz+sRgh/93cwfXqfGidJLVX8qaFuCxfCI4/AwAB84APVj9r4m8eSND5TYkQw\n4tgx+Ju/gT//c3jnO+GOO+CKK/ry1pLUKo4IRjEwAKtWwQ9+AH/0R/C+98G118K3vgWvvNJ06ySp\nnabUiKDbq6/CV74Cf/3X8OijcPXV8OEPV9MVV1RfaidJZyIvFp+CQ4eqr6h46KFqOngQfu/34IMf\nhPe/v7rOIElnCoOgD37yE7j3Xti8ufoeo5kzq0C49lr4lV+BpUvh7LNPe7MkaVwMgj7LhB/9qBox\nfO978PTTsH07LFoEV15ZhcKSJTBvXlU2a1ajzZUkg+B0eOUVeOopePxx2Lq1Coe9e2HXLliwAC65\nBBYvhosvhve8pwqIiy/2GQZJp4dB0KAjR+DZZ+GZZ2Dnzmr+Rz+q5vfsgXe9q/oKjAsuqKa5c09M\nncszZzZ9JJLOZAZBSx0+XIXBc89Vv5mwb1/1OjKNLL/wAsyeXV2gfvvbq+n88984ve1tb5zmzIFz\nz/UrNCRVTmsQRMRy4DZgGvDlzLz1JHXuAH4H+DnwHzJz6wS2nTJBMF6ZVSjs3QsvvlhNBw/CgQPV\nNDJ/8OCJ6dChKmhmzz4xvfWt8Ja3VNN551VT5/zJljvLZs40WKQz1WkLgoiYBjwNfAjYCzwGrMrM\n7R11VgBrMnNFRFwN3J6Zy8azbb19cUEwmqGhIQYHB0ddf/Qo/PSn1XToELz00onpZz+rps75XmWv\nvVYFwjnnVHdIzZpVvZ5smjVr9Glk/cyZo08zZvzi68DA6EHUqy9KYl+cYF+c0M8gGOix/ipgODN3\n1TveAKwEOj/MrwPWA2Tm5oiYExEXABeNY1t16PWPfPp0eMc7qqkfjhypQuGVV05Mr776xuWR6fDh\nat3IdODAG8tH5keWR+YPH6720/165EgVRJ3BMH169TpjBhw6NMT8+YOvlw8MnHhtan609dOmVdNZ\nZ/3ifOfrqY6+/PA7wb6YHL2CYD6wu2N5D3D1OOrMB+aNY1s1aMaM6npEU157rRrlHD5cvY4ExJEj\ncNtt1c+SHj5cfYfU0aPV66nOj/d9TuW9X3sNjh+vXjvnO8syqyAYLSjGKjt4sHo6fqxtRguffk0R\nJ167p9HKT3XdWNts2QLr1vXv/frdvtP1fv3+VoReQTDeczaeadaEjXyAney5jF/6per5jaki8xcD\n4mThcbL1t98On/rUxLYZCZ/jx/s7ZZ58Gm3dWNudyvvt2wff/37/3q9X29v8fv3U6xrBMmBtZi6v\nl28Bjnde9I2IvwSGMnNDvbwD+ADVqaExt63LvUAgSafgdF0j2AIsjogLgeeBjwKruupsBNYAG+rg\nOJSZ+yPixXFs27cDkSSdmjGDIDOPRcQa4AGqW0DvzsztEXFTvX5dZm6KiBURMQy8DKwea9vJPBhJ\n0sQ1/kCZJKlZjX4jf0Qsj4gdEbEzIm5usi2TLSIWRsTDEfFURPwwIj5Tl58fEQ9FxDMR8WBEzOnY\n5pa6b3ZExG831/rJERHTImJrRHy7Xi6yL+pbrr8ZEdsjYltEXF1wX9xS/x95MiL+d0TMLKUvIuKe\niNgfEU92lE342CPi1+r+2xkRt49r55nZyER1umgYuBCYDjwOXNpUe07D8V4AXFnPn0f1sN2lwH8H\n/rQuvxn4b/X8krpPptd9NAyc1fRx9LlP/jNwL7CxXi6yL6iew/mP9fwAMLvEvqiP51lgZr38DeDj\npfQF8BvAUuDJjrKJHPvIGZ5Hgavq+U3A8l77bnJE8PrDapl5FBh54GxKysx9mfl4Pf8zqgfr5tPx\nQF79+pF6fiXw9cw8mtVDecNUfTYlRMQCYAXwZU7cflxcX0TEbOA3MvMeqK6tZeZPKbAvgH8DjgLn\nRMQAcA7VjSZF9EVmPgIc7CqeyLFfHRHvAt6SmY/W9b7Ssc2omgyC0R5Em/LqO6mWApuBuZm5v161\nH5hbz8+j6pMRU61/vgT8CXC8o6zEvrgI+JeI+KuI+H8R8b8i4lwK7IvMPAD8D+A5qgA4lJkPUWBf\ndJjosXeX72UcfdJkEBR5lToizgPuAz6bmS91rstqLDdWv0yJPouI3wVeyOrLCU96+3ApfUF1KuhX\ngf+Zmb9Kdefdf+msUEpfRMR7gP9EdapjHnBeRHyss04pfXEy4zj2U9ZkEOwFOn8peCFvTLIpJyKm\nU4XAVzPz/rp4f/3dTNTDuhfq8u7+WVCXTQXvB66LiB8DXwc+GBFfpcy+2APsyczH6uVvUgXDvgL7\n4teBf8rMFzPzGPC3wPsosy9GTOT/xJ66fEFXec8+aTIIXn9YLSJmUD1wtrHB9kyqiAjgbmBbZt7W\nsWoj1QUx6tf7O8qvj4gZEXERsJjqItAZLzM/n5kLM/Mi4Hrge5l5A2X2xT5gd0RcUhd9CHgK+DaF\n9QWwA1gWEWfX/18+BGyjzL4YMaH/E/W/p3+r7zwL4IaObUbX8FXy36G6e2YYuKXpq/aTfKzXUJ0P\nfxzYWk/LgfOBvweeAR4E5nRs8/m6b3YA1zZ9DJPULx/gxF1DRfYFcAXV17Q/QfVX8OyC++JPqYLw\nSaqLo9NL6Quq0fHzwBGq66erT+XYgV+r+28YuGM8+/aBMkkqXKMPlEmSmmcQSFLhDAJJKpxBIEmF\nMwgkqXAGgSQVziCQpMIZBJJUuP8PjTyeKUNW7IIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2d9ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n",
    "plt.plot(pca_mod.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.316069\n",
      "recall_score: 0.271071\n",
      "precision_score: 0.378981\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=19 )#C from the above test\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary'))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
