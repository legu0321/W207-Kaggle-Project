{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza (RAOP) Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('./pizza_request_dataset.json')\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly assign 70% to train_data, and 30% to dev_data\n",
    "msk = np.random.rand(len(X)) <= 0.7\n",
    "train_data = X[msk]\n",
    "dev_data = X[~msk]\n",
    "\n",
    "# create output dataframe Y of train_labels\n",
    "train_labels = train_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete train_labels from input dataframe of train_data\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "# create output dataframe of dev_labels\n",
    "dev_labels = dev_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete dev_labels from input dataframe of dev_data\n",
    "del dev_data[\"requester_received_pizza\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(train_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(train_labels)', <type 'numpy.ndarray'>)\n",
      "('type(dev_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(dev_labels)', <type 'numpy.ndarray'>) /n\n",
      "('train_labels.shape:', (3975,))\n",
      "('dev_labels.shape:', (1696,))\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "# transform X to numpy array, and Y to 1-D numpy array\n",
    "# view actual text and labels\n",
    "\n",
    "train_text = train_data[\"request_text\"]\n",
    "dev_text = dev_data[\"request_text\"]\n",
    "\n",
    "print (\"type(train_text)\", type(train_text))\n",
    "print (\"type(train_labels)\", type(train_labels))\n",
    "print (\"type(dev_text)\", type(dev_text))\n",
    "print (\"type(dev_labels)\", type(dev_labels)), \"/n\"\n",
    "\n",
    "\n",
    "# view as pandas dataframe\n",
    "#print (\"pandas dataframe:\")\n",
    "#print (train_text[:10]), \"/n\"\n",
    "\n",
    "# convert to numpy (np) array\n",
    "\n",
    "def to_np(d):\n",
    "\n",
    "    #print (\"numpy.ndarray:\")\n",
    "    d = np.array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    #print (type(d))\n",
    "    #print (d.shape)\n",
    "    #print (d[:3])\n",
    "    #print\n",
    "    return d\n",
    "\n",
    "train_text = to_np(train_text)\n",
    "train_labels = to_np(train_labels)\n",
    "dev_text = to_np(dev_text)\n",
    "dev_labels = to_np(dev_labels)\n",
    "\n",
    "# convert to list\n",
    "\n",
    "def to_list(d):\n",
    "\n",
    "    #print (\"list:\")\n",
    "    d = list(d)\n",
    "    #print (type(d))\n",
    "    #print (d[:3])\n",
    "    #print ()\n",
    "    return d\n",
    "\n",
    "#train_text = to_list(train_text)\n",
    "#dev_text = to_list(dev_text)\n",
    "\n",
    "print (\"train_labels.shape:\", train_labels.shape)\n",
    "print (\"dev_labels.shape:\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_vectors.shape:', (3975, 12313))\n",
      "('dev_vectors.shape:', (1696, 12313))\n",
      "\n",
      "------------------------------\n",
      "K Nearest Neighbors (K-NN)\n",
      "------------------------------\n",
      "K-NN: f1_score = 0.4133, k = 1\n",
      "K-NN: f1_score = 0.0259, k = 5\n",
      "K-NN: f1_score = 0.0, k = 15\n",
      "K-NN: f1_score = 0.0, k = 16\n",
      "K-NN: f1_score = 0.0, k = 17\n",
      "K-NN: f1_score = 0.0, k = 18\n",
      "K-NN: f1_score = 0.0, k = 19\n",
      "K-NN: f1_score = 0.0, k = 20\n",
      "K-NN: f1_score = 0.0, k = 28\n",
      "K-NN: f1_score = 0.0, k = 29\n",
      "K-NN: f1_score = 0.0, k = 30\n",
      "K-NN: f1_score = 0.0, k = 31\n",
      "K-NN: f1_score = 0.0, k = 32\n",
      "K-NN: f1_score = 0.0, k = 150\n",
      "K-NN: f1_score = 0.0, k = 300\n",
      "\n",
      "('K-NN: optimal k =', 1)\n",
      "\n",
      "-----------------------------\n",
      "Bernoulli Naive Bayes (BNB)\n",
      "-----------------------------\n",
      "BNB: f1_score = 0.0, alpha = 0.0\n",
      "BNB: f1_score = 0.0844, alpha = 1e-05\n",
      "BNB: f1_score = 0.0844, alpha = 0.0001\n",
      "BNB: f1_score = 0.0844, alpha = 0.001\n",
      "BNB: f1_score = 0.0125, alpha = 0.01\n",
      "BNB: f1_score = 0.0126, alpha = 0.094\n",
      "BNB: f1_score = 0.0126, alpha = 0.095\n",
      "BNB: f1_score = 0.0126, alpha = 0.096\n",
      "BNB: f1_score = 0.0126, alpha = 0.1\n",
      "BNB: f1_score = 0.0126, alpha = 0.105\n",
      "BNB: f1_score = 0.0044, alpha = 0.2\n",
      "BNB: f1_score = 0.0045, alpha = 0.3\n",
      "BNB: f1_score = 0.0, alpha = 0.4\n",
      "BNB: f1_score = 0.0, alpha = 0.5\n",
      "BNB: f1_score = 0.0, alpha = 0.6\n",
      "BNB: f1_score = 0.0, alpha = 0.7\n",
      "BNB: f1_score = 0.0, alpha = 1.0\n",
      "BNB: f1_score = 0.0, alpha = 10.0\n",
      "\n",
      "('Bernoulli Naive Bayes: optimal alpha =', 1e-05)\n",
      "\n",
      "------------------------\n",
      "Logistic Regression (LR)\n",
      "------------------------\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.01\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 0.109621390162\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 9.52499225203\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0045, C = 0.2\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 31.7216994044\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0135, C = 0.3\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 61.681429626\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0178, C = 0.4\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 97.3726951815\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.5\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 137.63336072\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.54\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 154.8503557\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.55\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 159.243434292\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.56\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 163.672952271\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.57\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 168.137600962\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.58\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 172.636591088\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0263, C = 0.59\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 177.169370513\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0306, C = 0.6\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 181.735454014\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0475, C = 0.7\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 229.126241576\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0641, C = 0.8\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 279.382526253\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0761, C = 0.9\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 332.181148169\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0795, C = 1.0\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 387.340589083\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0828, C = 1.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 444.464888316\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2324, C = 10\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 7103.77336434\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2319, C = 12\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 8552.28197252\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2415, C = 20\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 13863.7323633\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2541, C = 30\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 19607.8460883\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2638, C = 40\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 24623.210914\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2701, C = 50\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 29109.2238671\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2739, C = 100\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 46626.217287\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2755, C = 1000\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 151950.673399\n",
      "\n",
      "('Logistic Regression: optimal C =', 1000)\n",
      "\n",
      "('max accuracy =', 74.233490566037744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dschan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: Passing additional arguments to the metric function as **kwargs is deprecated and will no longer be supported in 0.18. Use metric_params instead.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Notes\n",
    "# Classifier precision--when a positive value is predicted, proportion of time the prediction is correct--equals (TP) / (TP + FP)\n",
    "# Classifier recall--when the actual value is positive, the proportion of time the prediction is correct--equals (TP) / (TP + FN)\n",
    "\n",
    "def explore_models():\n",
    "\n",
    "    # create empty vector\n",
    "    accuracies = []\n",
    "\n",
    "    # Source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "    # where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "    # The relative contribution of precision and recall to the F1 score are equal. \n",
    "    # The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "    \n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    print (\"dev_vectors.shape:\", dev_vectors.shape)\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # K Nearest Neighbors\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------------\")\n",
    "    print (\"K Nearest Neighbors (K-NN)\")\n",
    "    print (\"------------------------------\")\n",
    "    \n",
    "    # Euclidean distance, when you go to 10 to 20+ dimensions, too many examples can be close to each other\n",
    "    # With K-NN on text, Cosine or Manhattan distance might be better. Cosine distance measures the angle between examples,\n",
    "    # more robust for high-dimensional problems. \n",
    "    # Dot product measures length of vectors AND angle between these vectors. \n",
    "    # With Cosine distance, you can get a value 0 to 1.\n",
    "    \n",
    "    # create two vectors\n",
    "    # ks refers to a vector of k nearest neighbor values\n",
    "    \n",
    "    ks = [1, 5, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 150, 300]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, distance='cosine', algorithm='brute')\n",
    "        knn.fit(train_vectors, train_labels)\n",
    "        pred_1 = knn.predict(dev_vectors)\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "        # f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]¶\n",
    "            # y_true = Ground truth (correct) target values \n",
    "            # y_pred = Estimated targets as returned by a classifier.\n",
    "            # average = required for multiclass/multilabel targets.\n",
    "                # 'weighted': Calculate metrics for each label, and find their average, weighted by \n",
    "                # the number of true instances for each label. This alters ‘macro’ to account for label imbalance; \n",
    "                # it can result in an F-score that is not between precision and recall.\n",
    "            \n",
    "        print (\"K-NN: f1_score = %s, k = %s\" %(round(metrics.f1_score(dev_labels, pred_1, average='binary'),4), k))\n",
    "\n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_1))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_knn = dict(zip(ks, f1_scores))\n",
    "    #print (results_knn)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"K-NN: optimal k =\", max(results_knn.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "\n",
    "    \n",
    "    #------------------------\n",
    "    # Bernoulli Naive Bayes\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"-----------------------------\")\n",
    "    print (\"Bernoulli Naive Bayes (BNB)\")\n",
    "    print (\"-----------------------------\")\n",
    "    \n",
    "    # create two vectors\n",
    "    \n",
    "    alphas = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.094, 0.095, 0.096, 0.1, 0.105, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0, 10.0]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for a in alphas:\n",
    "        bnb = BernoulliNB(alpha=a,binarize=0.5)\n",
    "        bnb.fit(train_vectors, train_labels)\n",
    "        pred_2 = bnb.predict(dev_vectors)\n",
    "        print (\"BNB: f1_score = %s, alpha = %s\" % (round(metrics.f1_score(dev_labels, pred_2, average='binary'), 4), a))\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_2))\n",
    "        \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_bnb = dict(zip(alphas, f1_scores))\n",
    "    #print (results_mnb)\n",
    "    \n",
    "    # print (the key wit)h the max fl_score\n",
    "    print (\"Bernoulli Naive Bayes: optimal alpha =\", max(results_bnb.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # Logistic Regression\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------\")\n",
    "    print (\"Logistic Regression (LR)\")\n",
    "    print (\"------------------------\")\n",
    "    print\n",
    "    \n",
    "    # create two vectors\n",
    "    # cs refers to the vector of C (inverse of regularization strength) values\n",
    "    \n",
    "    cs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, \\\n",
    "          10, 12, 20, 30, 40, 50, 100, 1000]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for c in cs:\n",
    "        \n",
    "        # logistic regression fits a line like linear regression, but instead of predicting any number, \n",
    "        # it predicts a number between 0 and 1 (sigmoid function).\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # C (inverse of regularization strength) controls how much the weights influence the loss, and\n",
    "        # penalizes the sum of squared weights if very different weights exist between different tokens.\n",
    "  \n",
    "        # use l2 regularization, per instructions\n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "        pred_3 = lr.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-------------------------------\")\n",
    "        print (\"LR: f1_score = %s, C = %s\" % (round(metrics.f1_score(dev_labels, pred_3, average='binary'),4), c))\n",
    "        print (\"-------------------------------\")\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_3, average='binary'))\n",
    "        \n",
    "        accuracies.append((lr.score(dev_vectors, dev_labels))*100) \n",
    "\n",
    "        #print (lr.coef_.shape)\n",
    "        \n",
    "        # first define function that squares a given value, for later use in the 'for loop' below\n",
    "        fun_sq_wts = lambda x: x**2\n",
    "        \n",
    "        # use map function, likely faster (because written in C) than list comprehension.\n",
    "        # map function itself applies a function, specifically the first argument on the second argument.\n",
    "        # from coef_, take raw weights (coefficient of the features in the decision function), \n",
    "        # and sum the squares of these weights.\n",
    "\n",
    "        # note: averege=weight vs. average=default should be about same score if similar number of examples across classes\n",
    "        sq_wts = map(fun_sq_wts, lr.coef_[0])\n",
    "        sum_sq_wts =  sum(sq_wts)\n",
    "        print (\"Label = 1, sum of squared weights = %s\" % (sum_sq_wts))\n",
    "\n",
    "        print\n",
    "        \n",
    "    # map two vectors into a dictionary\n",
    "    results_lr = dict(zip(cs, f1_scores))\n",
    "    #print (results_lr)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"Logistic Regression: optimal C =\", max(results_lr.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    print (\"max accuracy =\", max(accuracies))\n",
    "        \n",
    "explore_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on initial results above, proceed with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "unigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 12313))\n",
      "\n",
      "lr.coef_:\n",
      "[[-1.48092905 -0.39311615 -0.12825177 ...,  0.44453487  1.63180069\n",
      "  -0.87946286]]\n",
      "top 20:\n",
      "[3697, 4147, 6786, 8221, 1039, 6858, 3236, 1286, 6712, 10159, 1523, 10688, 4918, 10689, 9907, 7050, 3901, 2573, 2155, 9349]\n",
      "       Feature      word\n",
      "0         edit  4.824324\n",
      "1       father  4.627494\n",
      "2         mean  4.376547\n",
      "3      pockets  4.057164\n",
      "4          ass  3.995387\n",
      "5    mentioned  3.977975\n",
      "6          die  3.930683\n",
      "7        basic  3.899384\n",
      "8      married  3.850098\n",
      "9     southern -3.846223\n",
      "10       bloke  3.807984\n",
      "11    surprise  3.805289\n",
      "12   graveyard  3.749854\n",
      "13   surprised  3.697465\n",
      "14     sitting -3.697393\n",
      "15       mommy  3.658543\n",
      "16  especially  3.627059\n",
      "17  constantly  3.593246\n",
      "18      cheesy  3.548595\n",
      "19     running  3.539010\n",
      "\n",
      "----------\n",
      "bigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 90693))\n",
      "\n",
      "lr.coef_:\n",
      "[[-0.31775435  0.42001296 -0.29407174 ..., -0.26401881 -0.22234693\n",
      "   0.57766744]]\n",
      "top 20:\n",
      "[23002, 38997, 37806, 32065, 28212, 73029, 33880, 44381, 54471, 23001, 81228, 41801, 77046, 2781, 47597, 9433, 84630, 16480, 57349, 59245]\n",
      "                Feature      word\n",
      "0           edit thanks  3.882043\n",
      "1             imgur com  3.879634\n",
      "2            http imgur  3.547851\n",
      "3             got pizza  3.038731\n",
      "4         forward money  2.984111\n",
      "5        sounds amazing  2.960338\n",
      "6        happy birthday  2.947813\n",
      "7   letsfytinglove best  2.947813\n",
      "8        north carolina  2.936326\n",
      "9            edit thank  2.927036\n",
      "10      tonight greatly  2.813269\n",
      "11           just spent  2.809594\n",
      "12         surprise son  2.768559\n",
      "13         afford ramen  2.674511\n",
      "14             love pie  2.633101\n",
      "15         broke payday  2.627687\n",
      "16               ve got  2.625031\n",
      "17        craving pizza -2.599092\n",
      "18          pay forward  2.518712\n",
      "19       pizza actually  2.512937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "\n",
    "# Feature Selection Notes:\n",
    "'''\n",
    "These objects take as input a scoring function that returns univariate p-values:\n",
    "-For regression: f_regression\n",
    "-For classification: chi2 or f_classif\n",
    "\n",
    "Feature selection with sparse data:\n",
    "-If you use sparse data (i.e. data represented as sparse matrices), \n",
    "only chi2 will deal with the data without making it dense.\n",
    "-Warning: Beware not to use a regression scoring function with a classification problem, \n",
    "you will get useless results.\n",
    "\n",
    "With SVMs and logistic-regression, the parameter C controls the sparsity: \n",
    "the smaller C the less features selected. \n",
    "'''\n",
    "def top20(type):\n",
    "\n",
    "    if type == \"unigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words. \n",
    "        # only applies if default analyzer='word'.\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"unigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "        \n",
    "    elif type == \"bigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words from the resulting tokens. \n",
    "        # only applies if default analyzer='word'.\n",
    "        # set bigrams to be 2 words only\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"bigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "      \n",
    "    # use C=12\n",
    "    for c in [12]:\n",
    "        \n",
    "        # in the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the default ‘multi_class’ option is set to ‘ovr’ \n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        #print (lr)\n",
    "        \n",
    "        # fit the model and generate coef_\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "         \n",
    "        # interested in magnitude of the weights (coefficients), so take absolute value.\n",
    "        # sort absolute values in descending order.\n",
    "        # important to know if negative or positive weight, so still output the positive/negative sign.\n",
    "        # after fitting logistic regression for class vs. all other classes, negative weight of a token \n",
    "        # indicates a class other than class of interest.\n",
    "        # (visual example of negative and positive on a sigmoid function helps) \n",
    "        \n",
    "        print (\"lr.coef_:\")\n",
    "        print (lr.coef_)\n",
    "\n",
    "        # for each label, store the column indices of the top 5 weights \n",
    "        top20 = sorted(range(len(lr.coef_[0])), key=lambda i: abs(lr.coef_[0][i]), reverse=True)[:20]\n",
    "       \n",
    "        col_1 = []\n",
    "        \n",
    "        # for each label, access and store weights via column indices\n",
    "        for index in (top20):\n",
    "\n",
    "            col_1.append(lr.coef_[0][index])\n",
    "           \n",
    "        print (\"top 20:\" )\n",
    "        print (top20)\n",
    "        \n",
    "        # store feature names, after converting to an array\n",
    "        feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "       \n",
    "        # create a Pandas dataframe with 20 rows and 4 columns, plus descriptive headers\n",
    "        df = pd.DataFrame({'Feature': feature_names[top20], 'word': col_1})\n",
    "        print (df    )\n",
    "\n",
    "top20(\"unigram\")\n",
    "top20(\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis\n",
    "This section is the numerical part of the model: utilize all numeric information from the dataset to \n",
    "predict the labels.  This model will be combined the text based model to improve predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.tseries.holiday.USFederalHolidayCalendar object at 0x114d47a90>\n",
      "1341601084\n",
      "2012-07-06 11:58:04\n",
      "<type 'str'>\n",
      "2012-07-06 06:58:04-05:00\n"
     ]
    }
   ],
   "source": [
    "print USFederalHolidayCalendar()\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc'][0]\n",
    "print temp\n",
    "temp2 = datetime.datetime.fromtimestamp(temp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print temp2\n",
    "print type(temp2)\n",
    "\n",
    "# utc = datetime.utcnow()\n",
    "utc = datetime.datetime.strptime(temp2, '%Y-%m-%d %H:%M:%S')\n",
    "utc = utc.replace(tzinfo=from_zone)\n",
    "central = utc.astimezone(to_zone)\n",
    "print central\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc']\n",
    "temp =  temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =   temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "#central =  utc.apply(lambda x: x.astimezone(to_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "\n",
    "temp = pd.DataFrame(temp, columns=[\"State\"])\n",
    "a = temp.iloc[0,0]\n",
    "print a.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "t = train_data['unix_timestamp_of_request_utc'][0]\n",
    "t = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d %H:%M:%S')\n",
    "t = datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "#temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    "#print holidays\n",
    "\n",
    "\n",
    "#for i in temp:\n",
    "    #if i in holidays:\n",
    "       # print i\n",
    "\n",
    "df['test'] = temp\n",
    "#print temp\n",
    "df['out'] = df['test'].apply(lambda x: 1 if x in holidays else 0)\n",
    "print sum(df['out'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  train_data['unix_timestamp_of_request_utc'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    "#print holidays\n",
    "\n",
    "b=(train_data['unix_timestamp_of_request_utc'] \n",
    ".apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    ".apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "a =  temp.apply(lambda x: 1 if x in holidays else 0)\n",
    "print sum(a)\n",
    "print sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "Accuracy = 0.7406\n",
      "[[  1.01639314e-02   3.03067454e-03  -9.24846205e-03  -1.30223260e-01\n",
      "   -3.24493904e-01   2.52049384e-05   7.00755226e-04  -4.70434631e-02\n",
      "    7.48903165e-01   1.11411164e-01]]\n",
      "Model F-Score = 0.0638 \n",
      "Accuracy = 0.7406\n",
      "accuracy_score: 0.483871\n",
      "recall_score: 0.0342\n",
      "roc_auc_score: 0.5107\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt0VNW9B/Dvb8AgSN6UV+RlUVjkQSzPRUkZfJG4ZGHV\nmiJqkV5DVS61aBHXKhVYvtAqooKFK6i0ZaEShfQKBUUTGjAFNTwSeZVcCAREDAMUCEmY/O4fk4TJ\nMMlMJjNz5sx8P2vNMmdmz5mfZ01+2ez9O3uLqoKIiMKTxegAiIgocJjkiYjCGJM8EVEYY5InIgpj\nTPJERGGMSZ6IKIy1D+aHiQjrNYmIfKCq4sv7gt6TV1U+VPHMM88YHkOoPHgteC14LVp+tAWHa4iI\nwhiTPBFRGGOSN4jVajU6hJDBa3EZr8VlvBb+IW0d72nVh4loMD+PiCgciAg0UBOvIrJMRE6IyK4W\n2rwuIgdEZIeIpPsSCBER+Z83wzXvABjX3IsikgXgx6p6PYCpAP7sp9iIiKiNPCZ5VS0EYGuhyQQA\nK+rb/gtArIh08094RETUFv6YeE0CcMTpuKL+OSIiaqPz58+36f1BveMVAObMmdP4s9Vq5Qw6EZGL\n/Px85OfnY/584OLFLW06l1fVNSLSB8DfVTXNzWt/BvCFqr5ff7wXwBhVPeGmLatriChiJSQAtpYG\nv13ExwOnTgW4uqae1D/cyQPwIACIyEgAp90leCKiSJWQAEh9BlX1/nHqVNs/2+NwjYisBGAFkCgi\n5QCeARAFQFV1qaquE5HbReTfAM4DeKjtYRERmZO73np8vCNpt2Tz5s1QVYwZM8av8XhM8qp6nxdt\npvknHCIic2pI7t4kdGc2mw0zZ87E+vXrsWzZMr/HxWUNiIj8wGZr3RCLquKDDz5AcnIyoqKiUFpa\ninHjmr0lyWdBr64hIgo3CQmOHnxrPProo/jnP/+J1atXY9SoUYEJDFy7hojII09VMQ1VMK2xb98+\n9OvXD1FRUR7btqW6hkmeiAgtJ3Jfkrg/BaOEkogo7DSUNnoqb2xLgq+qqkJ1dbV/AvYBkzwRhRXn\nxO3pAfi3Jt3Vpk2bkJqaio8//tj/J/cSJ16JyPSch1paW8IYCJWVlXjiiSfwxRdfYNGiRbjjjjsM\ni4U9eSIyvYbyxUD1yL2lqli5ciVSUlIQGxuLkpISQxM8wJ48EZmUa+89VHzzzTdYs2YNRowYYXQo\nAFhdQ0Qm4Vr9YnTFSzCxuoaIwlJL1S+RkuDbikmeiEKGa2UMEJpJ/fz585g1axYOHDhgdCgeMckT\nUchwnkANtcTeYOPGjUhNTcXRo0cRFxdndDgeceKViAzjbpw9VJ08eRIzZsxAYWEh3nrrLWRmZhod\nkleY5InIEAkJjv+aoRajuroaw4cPx913342SkhJcc801RofkNVbXEFHQNST4UByOaU5lZSUSExMN\n+WxW1xCRKThvg2emBA/AsATfVkzyRBQ0rd1Ywwj79u1DOI04MMkTUVD4srFGMJ07dw4zZszAmDFj\ncPToUaPD8RsmeSIKCpstdHvw69atQ0pKCiorK1FSUoJevXoZHZLfsLqGiALCDOWRp0+fxiOPPIJt\n27bh7bffxi233GJ0SH7HnjwR+UVLd6uG6jj81VdfjfT0dOzevTssEzzAEkoiaqOGHnskLRgWbCyh\nJKKgcrdwGBN8aGKSJ6JWcb5T1SzJvaioCLfffjsuXLhgdChBxyRPRB659tzNkNgB4OzZs5g2bRp+\n/vOf41e/+hU6duxodEhBxyRPRC0yY88dANauXYvk5GRUVVWhtLQU2dnZEPFpWNvUOPFKFOFcSx1d\nmXFCtbi4GNnZ2ViyZAnGjh1rdDht1paJVyZ5oghmxoXCvFVbW4urrrrK6DD8gtU1RNQqZl4ozFvh\nkuDbikmeKEKEY9ljdXU1Pv/8c6PDCGlM8kRhwvWOU9cHYL7J05Zs2bIFN954IxYtWhRWq0b6G9eu\nITIpd2vDREKuO3PmDGbNmoW8vDwsXLgQd999d0RWzXjLq568iGSKyF4R2S8iT7l5PUZE8kRkh4js\nFpHJfo+UiNwOuYRT79yTrVu3Ijk5GaqK0tJS3HPPPUzwHnisrhERC4D9AG4GcAzAdgC/VNW9Tm2e\nBhCjqk+LSBcA+wB0U9VLLudidQ2RD7g+jENFRQXKysqQkZFhdChB1ZbqGm+Ga4YDOKCqh+s/bBWA\nCQD2OrVRANH1P0cDqHRN8ETku4YdlSJdUlISkpKSjA7DVLwZrkkCcMTp+Gj9c87eBDBIRI4B2Ang\nt/4JjyiyNQzPhOJa7IFWV1dndAhhwV/VNeMAFKtqTwA3AlgkIp39dG6iiONcxx4p4+0NLl68iNmz\nZ+Pee+81OpSw4M1wTQWA3k7H19Y/5+whAC8AgKoeFJH/AzAQwFeuJ5szZ07jz1arFVartVUBE4Uz\n57H3SByeKSgoQE5ODlJSUvD6668bHY5h8vPzkZ+f75dzeTPx2g6OidSbARwHsA3ARFXd49RmEYDv\nVXWuiHSDI7kPVtVTLufixCtRM8J5iQFPbDYbZs6cifXr1+PNN9/EnXfeaXRIISWgE6+qaheRaQA2\nwjG8s0xV94jIVMfLuhTAswDeFZFd9W+b6Zrgicg9Vs4AK1euRFRUFEpLSxEbG2t0OGGFC5QRGYTJ\nnbwV6BJKIvIz5zXaiQKJa9cQBYlZd1fyp127dmHDhg1GhxFRmOSJgqThhqZIK4kEgKqqKjz99NO4\n5ZZbUFlZaXQ4EYVJnijAIvmGJgDYtGkTUlNTUVZWhl27duG+++4zOqSIwolXogATidyx97lz52L5\n8uVYtGgR7rjjDqPDMS1u/0cUglg9A+zfvx89evRAdHS058bULCZ5ohDgbn33SE3u5F8soSQyWKSX\nRF66dAm1tbXo2LGj0aGQC068EvmBzRa5vfbi4mKMHDkSS5cuNToUcoNJnshHznXvkVg5c+HCBfz+\n979HZmYmpk2bhunTpxsdErnBJE/kBXebZAORW/e+ceNGpKSk4NixY9i9ezcmT57MbfhCFMfkibzA\nnZmaKigowKJFi5CVlWV0KOQBq2uIPIjkJYApNLSluobDNUTNcN6diQmezIpJnsiJ6yJikTje3qC2\nthYvvfQSiouLjQ6F2oBJnghX7qkayckdALZv345hw4bhs88+Q1xcnNHhUBswyVNEi+QNs905d+4c\nfve732H8+PF48sknsWHDBvTr18/osKgNWF1DEcd5+YFI3TDbnbq6OmRkZCAtLQ0lJSXo0qWL0SGR\nH7C6hiIKK2VaduLECXTr1s3oMMgFq2uIXLi7eYmVMp4xwYcfJnkKK+4mUJ0fTPAOhw4dQl1dndFh\nUBAwyVNY4ASqd2pqavD8889j6NChKC0tNTocCgImeTI1JnfvFRUVYciQISgsLMTXX3+N1NRUo0Oi\nIGB1DZmS865LnMtvWVVVFWbOnInc3FwsWLAA9957LxcTiyBM8mQ6kb5BR2tFRUWha9euKCkpQULD\nxaOIwRJKMhWWQFIkYgklhTXX9WSY4Im8xyRPIc15aIYTqy3bs2cPJkyYgMrKSqNDoRDCJE8hLZL3\nTvVWdXU15s6di4yMDNx6661cUIya4MQrhSTn6hlqXmFhIXJyctC/f38UFxejV69eRodEIYYTrxRy\nOLnqncOHD2P06NFYsGAB7r77bpZFhrG2TLwyyZPhnFeFBBy9dyZ471RXV6NDhw5Gh0EBxiRPpibC\nmneilrCEkkypoTSS4+4tq6urQ35+vtFhkEl5leRFJFNE9orIfhF5qpk2VhEpFpESEfnCv2FSuHEu\njeTQTPNKS0sxevRozJ49GzU1NUaHQybkMcmLiAXAmwDGAUgGMFFEBrq0iQWwCMAdqpoC4BcBiJVM\njjc1ee/ixYuYPXs2rFYrHnzwQRQUFCAqKsrosMiEvCmhHA7ggKoeBgARWQVgAoC9Tm3uA5CrqhUA\noKo/+DtQMidutdd6paWluOuuu5CSkoIdO3YgKSnJ6JDIxLwZrkkCcMTp+Gj9c85uAJAgIl+IyHYR\necBfAZL5uPbYebdq6/To0QMvvfQScnNzmeCpzfx1M1R7AD8BcBOAawB8KSJfquq/XRvOmTOn8Wer\n1Qqr1eqnEMhI7LH7T0JCAiZMmGB0GGSg/Px8v022eyyhFJGRAOaoamb98SwAqqrzndo8BeBqVZ1b\nf/w2gPWqmutyLpZQhimWQfpGVXkTE3kU6BLK7QD6i0gfEYkC8EsAeS5t1gIYLSLtRKQTgBEA9vgS\nEJkLyyB9Y7fb8frrr+O2224DOz4USB6Ha1TVLiLTAGyE44/CMlXdIyJTHS/rUlXdKyIbAOwCYAew\nVFW/DWjkZDhu3uGbXbt24eGHH8bVV1+NpUuXsidPAcU7XslnHKJpnaqqKsybNw/Lli3D888/jylT\npsBi4f2I5Flbhmu4CiW1GleI9M2aNWtQVlaGXbt2oXv37kaHQxGCPXlqFa4Q6TtOspKvuHYNBVzD\nBCvABO8rJngyApM8tcg5ufOGJu8cOnQIf//7340OgwgAkzx5YLMxuXvr0qVLeOWVVzB06FCUlZUZ\nHQ4RAE68UgsSEji56q3i4mI8/PDDiI2NRVFREfr37290SEQA2JOnFnATbe8sWbIEmZmZmDZtGj77\n7DMmeAoprK6hZrEO3jsHDx5EdHQ0unbtanQoFKa4/R8FBJM8UWhgCSX5Hcfjr6SquHDhgtFhELUK\nkzw1wXp49w4ePIhbb70Vzz77rNGhELUKkzw1wZLJpmprazF//nyMGDECmZmZmDdvntEhEbUKSygJ\nANejcWf79u14+OGH0bVrV2zbtg3XXXed0SERtRonXonr0TTj2WefRd++fTFp0iQuSUCGYnUN+cS5\n984ETxS6mOTJJyyRJDIHllCS1xqqZ7hln4OqYvny5di8ebPRoRAFBJN8BHHero8VNMD+/ftx0003\n4a233kJcXJzR4RAFBJN8BGDte1M1NTV47rnnMGrUKEyYMAFFRUVIS0szOiyigGAJZZjjZttXGj9+\nPNq1a4evv/4affr0MTocooDixGuYaaiYacDKmStVVFSgZ8+eLIsk02B1TYRzTuxM6kThh9U1EYyT\nqc377rvvUFNTY3QYRIZikjc5buxxpbq6OixduhRpaWnYunWr0eEQGYoTrybG5YCvtGfPHuTk5KC2\nthabNm1Camqq0SERGYo9eRNjL/6yS5cuYe7cucjIyEB2dja2bNnCBE8E9uRNiStGXqldu3YAHBtq\n9+rVy+BoiEIHq2tMhitGEkUeVtdEAN61SkS+YJIPYc6LiQEskQQcNzJNnDgR5eXlRodCZApM8iGs\nYSs+JndHWeTixYuRnp6OAQMGoFu3bkaHRGQKnHgNUSyPvKykpAQ5OTmwWCwoKCjAoEGDjA6JyDTY\nkw9BnFy9zGazITMzEw8++CA2b97MBE/USl5V14hIJoDX4PijsExV5zfTbhiArQCyVfUjN6+zusYL\n3LGpqaqqKnTs2NHoMIgME9DqGhGxAHgTwDgAyQAmisjAZtq9CGCDL4GQA4dprsQET+Q7b4ZrhgM4\noKqHVbUWwCoAE9y0+28AqwF878f4IkokD9OoKgoLC40OgyjseJPkkwAccTo+Wv9cIxHpCeBOVX0L\nABfpbqVIr4EvLy/H+PHjMXXqVJw5c8bocIjCir8mXl8D8JTTMRO9l5yXCo60BG+327Fw4UL85Cc/\nwciRI1FcXIzY2FijwyIKK96UUFYA6O10fG39c86GAlgljq12ugDIEpFaVc1zPdmcOXMaf7ZarbBa\nra0MOXxE8vBMeXk5fvGLX+Dqq6/Gli1bMGDAAKNDIgoZ+fn5yM/P98u5PFbXiEg7APsA3AzgOIBt\nACaq6p5m2r8D4O+srvEskqtoLly4gNzcXEyaNAkWCyt5iVrSluoajz15VbWLyDQAG3G5hHKPiEx1\nvKxLXd/iSyAUWTp16oQHHnjA6DCIwh5XoTRIJA3VqCo3zSZqA65CaUKRsOGHqmLlypXIyMiA3W43\nOhyiiMS1ayggDh06hEceeQQVFRV4++23Gzf1IKLgYk+e/OrSpUt45ZVXMHToUIwZMwZff/01hg8f\nbnRYRBGLPXkDhPPSBfn5+Vi3bh2KiorQv39/o8MhiniceA2ySJhw5UQrkX9x4tUkIiHBA2CCJwoh\nTPJBEm4J/uTJk1i7dq3RYRCRB0zyQRIuJZOqihUrViA1NRX/+te/jA6HiDzgxCt57eDBg/jNb36D\nyspKfPLJJxgyZIjRIRGRB+zJk1dyc3MxYsQIjBs3Dtu2bWOCJzIJVtcEUEKCY5gGcJRMmnm45siR\nI6itrcV1111ndChEEact1TVM8gESbhOtRGQcllCGiIYdnsy+y9PFixeNDoGI/IRJ3k+cd3gy6y5P\nJ06cwMSJE/HYY48ZHQoR+QmTfBuES89dVbF8+XKkpqaiT58+eOONN4wOiYj8hCWUPmiYUI2PN//O\nTgcOHEBOTg7OnTuHjRs3Ij093eiQiMiPOPHaSuE2ofrqq69CRDB9+nQuB0wUolhdE0SRvC8rERmD\n1TVEROQWk7wXnCdYzboOfF5eHtavX290GEQUZEzyXrDZzFsaefz4cdxzzz148skn0blzZ6PDIaIg\nY5L3wKy7ONXV1WHJkiVIS0vDwIEDsXPnTmRkZBgdFhEFGUsoW2DmSpopU6Zg7969+Pzzz5Gammp0\nOERkEFbXNMPMCR5wLCjWs2dPlkUShQGWUAYASyWJKFSwhNLPzDQOf+bMGZw/f97oMIgoRDHJuzDT\nMM1HH32E5ORklkYSUbM48Qrzbe5RUVGBadOmYc+ePVi5ciV+9rOfGR0SEYUo9uRhnjp4VcXixYuR\nnp6OwYMHY+fOnUzwRNSiiO3Ju/bezUBEUFlZiYKCAgwaNMjocIjIBCKyusZM4+5ERKyu8UK4bPBB\nRNQaEZPkzTLuDgA2mw1Tp05FaWmp0aEQkclFTJI3A1XF+++/j+TkZFx11VXo1auX0SERkcl5NfEq\nIpkAXoPjj8IyVZ3v8vp9AJ6qP/wPgEdUdbc/Aw135eXlePTRR3Ho0CGsXr0ao0aNMjokIgoDHnvy\nImIB8CaAcQCSAUwUkYEuzcoA/ExVBwN4FsD/+DtQXzWMxYdyBU11dTXGjBmDESNG4JtvvmGCJyK/\n8aYnPxzAAVU9DAAisgrABAB7GxqoapFT+yIASf4M0lcNVTQhUNDTog4dOmD37t1c752I/M6bJJ8E\n4IjT8VE4En9z/gtASNxn3zDZagZM8EQUCH69GUpExgJ4CMDo5trMmTOn8Wer1Qqr1erPEBqF6iJj\nX331FYYMGQIRn0peiSgC5OfnIz8/3y/n8ngzlIiMBDBHVTPrj2cBUDeTr2kAcgFkqurBZs4VtJuh\nQm2p4MrKSjz55JPYtGkTtm7dimuvvdbokIjIJAJ9M9R2AP1FpI+IRAH4JYA8lwB6w5HgH2guwQdT\nKPXiVRUrV65ESkoKYmJiUFpaygRPREHjcbhGVe0iMg3ARlwuodwjIlMdL+tSALMBJABYLI5xiFpV\nbWncPqBCZSy+srIS999/P44dO4a1a9di+HDDLgkRRaiwW7smlNalqa2txfLlyzFlyhRcddVVRodD\nRCbF7f/qhVKCJyLyFyb5xvOHxjANEZE/RfwqlEbf1bpx40aMGjUKFy5cMCYAIqJmmH7TECPvaj15\n8iRmzJiBwsJCLF68GJ06dQp+EERELTB9T95mC/4YvKpixYoVSElJQdeuXVFSUoKsrKzgBkFE5AXT\n9+SNsGPHDixcuBDr1q3DkCFDjA6HiKhZpp14bdijNT7emGqauro6WCym/4cQEZlARFbXsJKGiCJF\nxFXXBGvZgnPnzmHNmjWB/yAiogAxXZIP1g1P69atQ0pKCvLy8hDMf+1QZOrbty9EhI8If/Tt29fv\n3y1TDdcEI8GfOHECjz/+OLZt24YlS5bglltuCdyHEdUTEXYmqNnvQf3z4T9cE+hyyfz8fKSmpqJP\nnz7YvXs3EzwRmZ6pevIS4MnW77//HseOHUN6enrgPoTIDfbkCQhMT55JnigEMMkTwOEav6qtrTU6\nBCKigDNFkvfnAmT/+c9/MH36dNxzzz1tPxkRUYgL+STvvABZWydd8/LykJycjPPnz+Odd95pe3BE\nFJJqamowZcoUxMbGomfPnliwYEGL7X/44QdMmjQJcXFxSExMxAMPPND42kMPPYQOHTogJiYG0dHR\niImJaRxSqaysxOjRo9GlSxfEx8fjpz/9KbZu3er2M26++WZYLBbU1dX573/UCyG/do0/tvI7fvw4\npk+fjp07d+K9997D2LFj/RMcETVht9vRrl07o8PAM888g4MHD+LIkSM4duwYxo4di+TkZNx2221u\n2991110YMWIEjh49io4dO6KkpKTJ60899RTmzZt3xfs6d+6MZcuW4frrr4fFYsHatWsxfvx4nDx5\nssmyJytXrsSlS5cg4tOwepuEbE/en0M0n3zyCW644Qbs3LmTCZ6olebPn4/+/fsjJiYGKSkpTe4C\nf++99zB69GjMmDEDXbp0wdy5cwEAy5cvx6BBg5CYmIisrCyUl5c3vufxxx9H7969ERsbi2HDhqGw\nsNDvMa9YsQJ//OMfERMTg4EDByInJwfvvvuu27affvopjh49ipdeegmdO3dGu3btMHjwYK8+p0OH\nDhgwYAAsFgtUFRaLBadPn8Ypp2GHs2fPYt68eXj55Zf98b/WeqoatIfj47zTiqZEptea341gW716\ntX733XeqqvrBBx/oNddc03j87rvvavv27XXRokVqt9v14sWLumbNGr3++ut13759arfb9bnnntNR\no0Y1nu9vf/ub2mw2tdvt+uqrr2r37t21urra7We/+OKLGhcXp/Hx8RoXF9fk5/j4eLfvsdlsKiL6\n/fffNz6Xm5uraWlpbtvPmzdPx40bp/fff78mJibq8OHDtaCgoPH1yZMna2JioiYmJurQoUM1Nzf3\ninOkpaVpVFSUWiwWnTp1apPXHnvsMV24cKEeOnRILRaL2u12t3GoNv89qH/et7zr6xt9+jAvv8jx\n8Y4HUaTw9LvhGLRs+8Mf0tPTNS8vT1UdSb5Pnz5NXs/KytLly5c3Htvtdu3UqZOWl5e7PV98fLzu\n2rXLP8Gp6pEjR9RisTT5w/Hpp59qv3793LbPyclRi8Wi77zzjl66dElXrVqlcXFxWllZqaqqxcXF\neurUKbXb7bpu3TqNjo7WrVu3XnGe6upqXbVqla5YsaLxue3bt+uNN96odXV1hiX5kBuuacvSBYWF\nhfjoo4/8GxBRCPBXmvfFihUrcOONNyI+Ph7x8fEoLS3FDz/80Ph6r169mrQ/fPgwfvvb3yIhIQEJ\nCQlITEyEiKCiogIA8Kc//QmDBg1qPN/Zs2ebnK+tOnfuDMAxTNLgzJkziI6Odtu+Y8eO6Nu3LyZP\nnox27dohOzsbvXr1wpYtWwAA6enpiI+Ph8ViQVZWFiZNmuQ2z0RFRSE7OxsvvPACdu/eDVXFY489\nhoULFxp6H0TIJXlfli44c+YMHnnkEWRnZ6N9+5CfSyYyjfLycuTk5GDx4sWw2Wyw2WxITk5ukrBc\nJxN79+6NJUuW4NSpUzh16hRsNhvOnTuHkSNHorCwEC+//DJWr17deD7nahVXL7zwQmNFi/Oj4Tl3\n4uLi0KNHD+zcubPxuZ07dyI5Odlt+7S0tCv+H1qaIPWUsGtra1FWVoazZ8/iq6++QnZ2Nnr06IHh\nw4dDVXHttdc2/gEJCl//CeDLAy38ezE+3tHXaO0wTW5uriYlJWlOTo7abLbWvZkoRLT0u2Gkb7/9\nVjt27Kj79+9Xu92uy5cv1/bt2+uyZctU1TFck5GR0eQ9H3/8saakpGhpaamqqp4+fVo//PBDVVVd\nt26dJiUl6XfffafV1dU6d+5cbd++vW7atMmvcc+aNUutVqvabDb99ttvtXv37rpx40a3bU+dOqUJ\nCQm6YsUKtdvt+uGHH2piYmLjcM3q1av13LlzWldXpxs2bNCYmBjdvHmzqqoWFRVpYWGh1tTUaFVV\nlb744osaExOjx48fV1XVEydOND62b9+uIqLHjx/X2tpat7E09z1AOIzJ+/IdnzVrlg4YMKDJJAmR\nGYVqkldV/cMf/qAJCQn6ox/9SJ944gm1Wq0tJnlV1b/+9a+ampqqsbGx2rt3b/31r3+tqo7x+SlT\npmhMTIz27NlTX375Ze3Xr5/fk3x1dXXj53Tv3l1fe+21Jq937txZCwsLG48LCws1NTVVo6Ojddiw\nYbply5bG1zIyMjQuLk5jY2M1PT1dP/jgg8bXCgoKdPDgwRoTE6OJiYlqtVqbnNeZUWPyIbN2jS/r\n0pSXl6Nbt27o0KGDH6IjMg7XriEgjBcoC9ZGIEShikmegDBO8p568RcvXkRNTU2zEy1EZsckT0CY\nrkLpab/WgoICpKen4y9/+UvwgiIiChOG9+Sb68XbbDbMnDkT//jHP/DGG2/gzjvvDFKURMHHnjwB\nYdiTb64X/+GHHyI5ORkdOnRAaWkpEzwRkY8MvXOouRUmy8rKsHr1aowaNSr4QRERhRHDhmtYUUN0\nWd++fXH48GGjwyCD9enTB4cOHbri+YBX14hIJoDX4BjeWaaq8920eR1AFoDzACar6g43bRqTPPdr\nJSLyTkDH5EXEAuBNAOMAJAOYKCIDXdpkAfixql4PYCqAP7d0zoQEIC6uCk8//TS+/PJLX+I2vfz8\nfKNDCBm8FpfxWlzGa+Ef3ky8DgdwQFUPq2otgFUAJri0mQBgBQCo6r8AxIpIN3cnS0gAams3ITEx\nFWVlZejbt6/v0ZsYv8CX8VpcxmtxGa+Ff3gz8ZoE4IjT8VE4En9LbSrqnzvhejKb7SH06rUJCxYs\nwvjx41sZLhERtUbQq2s6dIhBaWlps2s7ExGR/3iceBWRkQDmqGpm/fEsOFZEm+/U5s8AvlDV9+uP\n9wIYo6onXM7FqVYiIh/4OvHqTU9+O4D+ItIHwHEAvwQw0aVNHoDHALxf/0fhtGuCb0uQRETkG49J\nXlXtIjKRAYorAAADTElEQVQNwEZcLqHcIyJTHS/rUlVdJyK3i8i/4SihfCiwYRMRkTeCejMUEREF\nV0DWrhGRTBHZKyL7ReSpZtq8LiIHRGSHiKQHIo5Q4OlaiMh9IrKz/lEoIqlGxBkM3nwv6tsNE5Fa\nEbkrmPEFk5e/I1YRKRaREhH5ItgxBosXvyMxIpJXnyt2i8hkA8IMOBFZJiInRGRXC21anzd93VKq\nuQccfzj+DaAPgKsA7AAw0KVNFoBP6n8eAaDI33GEwsPLazESQGz9z5mRfC2c2m0C8L8A7jI6bgO/\nF7EASgEk1R93MTpuA6/F0wBeaLgOACoBtDc69gBci9EA0gHsauZ1n/JmIHryfr15yuQ8XgtVLVLV\nM/WHRXDcXxCOvPleAMB/A1gN4PtgBhdk3lyL+wDkqmoFAKjqD0GOMVi8uRYKoKHmOhpApapeCmKM\nQaGqhQBsLTTxKW8GIsm7u3nKNXE1d/NUuPHmWjj7LwDrAxqRcTxeCxHpCeBOVX0LQDhXYnnzvbgB\nQIKIfCEi20XkgaBFF1zeXIs3AQwSkWMAdgL4bZBiCzU+5U1Dlxqmy0RkLBxVSaONjsVArwFwHpMN\n50TvSXsAPwFwE4BrAHwpIl+q6r+NDcsQ4wAUq+pNIvJjAJ+KSJqqnjM6MDMIRJKvANDb6fja+udc\n2/Ty0CYceHMtICJpAJYCyFTVlv65ZmbeXIuhAFaJiMAx9polIrWqmhekGIPFm2txFMAPqnoRwEUR\n2QxgMBzj1+HEm2vxEIAXAEBVD4rI/wEYCOCroEQYOnzKm4EYrmm8eUpEouC4ecr1lzQPwINA4x21\nbm+eCgMer4WI9AaQC+ABVT1oQIzB4vFaqOp19Y9+cIzLPxqGCR7w7ndkLYDRItJORDrBMdG2J8hx\nBoM31+IwgFsAoH4M+gYAZUGNMngEzf8L1qe86feevPLmqUbeXAsAswEkAFhc34OtVVXXBeBMz8tr\n0eQtQQ8ySLz8HdkrIhsA7AJgB7BUVb81MOyA8PJ78SyAd51KC2eqathtNyQiKwFYASSKSDmAZwBE\noY15kzdDERGFMUM38iYiosBikiciCmNM8kREYYxJnogojDHJExGFMSZ5IqIwxiRPRBTGmOSJiMLY\n/wNdUpbMfsBgvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114e3ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "np.random.seed(0)\n",
    "\n",
    "#Setting up logic to identify timezone, weekend, and holidays\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "weekend = set([5, 6])\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    " \n",
    "#Created new df for training data(numeric only)\n",
    "train_data_numeric = np.zeros((len(train_data), 10))\n",
    "dev_data_numeric =  np.zeros((len(dev_data), 10))\n",
    "\n",
    "find_image = re.compile(r'(imgur\\.com|\\.jpg)')  #compile this to combine with other function\n",
    "\n",
    "#####Process train data set##############\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =   temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "temp = pd.DataFrame(temp, columns=[\"T\"])\n",
    "    \n",
    "train_data_numeric[:,0] = train_data['request_title'].apply(lambda x: len(x.split(' ')))  #Title length\n",
    "train_data_numeric[:,1] = train_data['request_text'].apply(lambda x: len(x.split(' ')))   #Text length\n",
    "train_data_numeric[:,2] = temp['T'].dt.month      #Month in integer\n",
    "train_data_numeric[:,3] = temp['T'].dt.day > 15   #2nd 1/2 month\n",
    "train_data_numeric[:,4] = temp['T'].dt.hour > 12  #Passed noon\n",
    "train_data_numeric[:,5] = train_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "train_data_numeric[:,6] = train_data['requester_account_age_in_days_at_retrieval']\n",
    "#Request during weekend\n",
    "train_data_numeric[:,7] = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).weekday() in weekend)\n",
    "#Has Image\n",
    "train_data_numeric[:,8] = train_data['request_text_edit_aware'].apply(lambda x: 1 if find_image.findall(x) else 0 )\n",
    "#Requested date a holiday\n",
    "train_data_numeric[:,9] = (train_data['unix_timestamp_of_request_utc']\n",
    "                           .apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    "                           .apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "\n",
    "#######process dev dataset#############\n",
    "ut = dev_data['unix_timestamp_of_request_utc']\n",
    "temp = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp = temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =  temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "temp = pd.DataFrame(temp, columns=[\"T\"])\n",
    "\n",
    "dev_data_numeric[:,0] = dev_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "dev_data_numeric[:,1] = dev_data['request_text'].apply(lambda x: len(x.split(' ')))\n",
    "dev_data_numeric[:,2] = temp['T'].dt.month\n",
    "dev_data_numeric[:,3] = temp['T'].dt.day > 15\n",
    "dev_data_numeric[:,4] = temp['T'].dt.hour > 12 \n",
    "dev_data_numeric[:,5] = dev_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "dev_data_numeric[:,6] = dev_data['requester_account_age_in_days_at_retrieval']\n",
    "#Request during weekend\n",
    "dev_data_numeric[:,7] = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).weekday() in weekend) \n",
    "#Include Image\n",
    "dev_data_numeric[:,8] =  dev_data['request_text_edit_aware'].apply(lambda x: 1 if find_image.findall(x) else 0 )\n",
    "dev_data_numeric[:,9] = (dev_data['unix_timestamp_of_request_utc']\n",
    "                         .apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    "                         .apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "#[:, [0,1,2,3,4,7,8, 9]]\n",
    "###Start Logistic Regresson on numeric data\n",
    "logit = LogisticRegression(C=1, penalty = 'l2')\n",
    "result = logit.fit(train_data_numeric, train_labels)\n",
    "\n",
    "preds = logit.predict(dev_data_numeric)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit.predict_proba(dev_data_numeric)\n",
    "\n",
    "#How many positive case\n",
    "print preds.sum()\n",
    "\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print \"Accuracy = %0.4f\" % (accuracy)\n",
    "print result.coef_\n",
    "\n",
    "model_output(pred_probas, F_Score, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "    #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "    #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "    #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "    #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "    #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "    #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "    #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "    s = BeautifulSoup(s).get_text() # Newly addition\n",
    "\n",
    "    return s\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=L1)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=L2 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "    \n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "    \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "giver_username_if_known\n",
      "in_test_set\n",
      "number_of_downvotes_of_request_at_retrieval\n",
      "number_of_upvotes_of_request_at_retrieval\n",
      "post_was_edited\n",
      "request_id\n",
      "request_number_of_comments_at_retrieval\n",
      "request_text\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "requester_account_age_in_days_at_request\n",
      "requester_account_age_in_days_at_retrieval\n",
      "requester_days_since_first_post_on_raop_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval\n",
      "requester_number_of_comments_at_request\n",
      "requester_number_of_comments_at_retrieval\n",
      "requester_number_of_comments_in_raop_at_request\n",
      "requester_number_of_comments_in_raop_at_retrieval\n",
      "requester_number_of_posts_at_request\n",
      "requester_number_of_posts_at_retrieval\n",
      "requester_number_of_posts_on_raop_at_request\n",
      "requester_number_of_posts_on_raop_at_retrieval\n",
      "requester_number_of_subreddits_at_request\n",
      "requester_subreddits_at_request\n",
      "requester_upvotes_minus_downvotes_at_request\n",
      "requester_upvotes_minus_downvotes_at_retrieval\n",
      "requester_upvotes_plus_downvotes_at_request\n",
      "requester_upvotes_plus_downvotes_at_retrieval\n",
      "requester_user_flair\n",
      "requester_username\n",
      "unix_timestamp_of_request\n",
      "unix_timestamp_of_request_utc\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print train_data.loc[0,'requester_upvotes_minus_downvotes_at_retrieval']\n",
    "c = 0\n",
    "for i in list(train_data):\n",
    "    c = c +1\n",
    "    print i\n",
    "print len(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1)Baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301158\n",
      "precision_score: 0.346154\n",
      "recall_score: 0.266515\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary'))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2)Add preprocessing and L1 feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>Add Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=580 ,l2=47 , f1 score is 0.336075\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor = first_preprocessor,analyzer='word',stop_words='english', tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,600,10):\n",
    "    for j in range(1,50,2):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Roc curve:\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def model_output(pred_probas, F_Score, preds):\n",
    "\n",
    "    print (\"Model F-Score = %0.4f \"%(F_Score))  #Sum up Squared Weights\n",
    "    accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "    print (\"Accuracy = %0.4f\"    % (accuracy))\n",
    "    print ('accuracy_score: %f'  % metrics.precision_score(dev_labels, preds))\n",
    "    print ('recall_score: %0.4f' % metrics.recall_score(dev_labels, preds))\n",
    "    print ('roc_auc_score: %0.4f'% metrics.roc_auc_score(dev_labels, preds))\n",
    "\n",
    "#Create ROC Curve\n",
    "\n",
    "    fpr,tpr,_ = roc_curve(dev_labels, pred_probas[:,1])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label='area = %.4f' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Best Model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      "Logistic regression on numeric data: F-Score = 0.3252: \n",
      "Accuracy = 0.6745\n",
      "accuracy_score: 0.350923\n",
      "recall_score: 0.302961\n",
      "roc_auc_score: 0.553628609174\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEgpiQvIhLElORFRIKYkryISBD7/1MJzHbs54WuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137fe210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best non-ensemble method so far\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = model_LG.predict_proba(dev_matrix_process[:,index])\n",
    "\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "Logistic regression on numeric data: F-Score = 0.3285: \n",
      "Accuracy = 0.6698\n",
      "accuracy_score: 0.346835\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.55341114814\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c31890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Basic Ensemble Method:\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "\n",
    "#######Ensemble Method Starts###################################\n",
    "logit = LogisticRegression()\n",
    "logit_en = LogisticRegression()\n",
    "logit.fit(train_data_numeric[:,2:5], train_labels)\n",
    "\n",
    "#Create a new set of step 2 logistic regression based on step 1 probability outputs\n",
    "#X1: predicted prob. of positive labels with text-based model\n",
    "#X2: predicted prob. of positive labels with text-based model\n",
    "En_X1_train = logit.predict_proba(train_data_numeric[:,2:5])[:,1]\n",
    "En_X2_train = model_LG.predict_proba(word_matrix_process[:,index])[:,1]\n",
    "En_X1_dev = logit.predict_proba(dev_data_numeric[:,2:5])[:,1]\n",
    "En_X2_dev = model_LG.predict_proba(dev_matrix_process[:,index])[:,1]\n",
    "\n",
    "t_size = len(En_X1_train)\n",
    "d_size = len(En_X1_dev)\n",
    "En_train = np.zeros((t_size, 2))\n",
    "En_dev = np.zeros((d_size, 2))\n",
    "\n",
    "En_train[:,0] = np.round(En_X1_train, 2)\n",
    "En_train[:,1] = np.round(En_X2_train, 2)\n",
    "En_dev[:,0] =   np.round(En_X1_dev, 2)\n",
    "En_dev[:,1] =   np.round(En_X2_dev, 2)\n",
    "\n",
    "logit_en.fit(En_train, train_labels)\n",
    "\n",
    "preds = logit_en.predict(En_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit_en.predict_proba(En_dev)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3)NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on numeric data: F-Score = 0.3224: \n",
      "Accuracy = 0.6604\n",
      "accuracy_score: 0.333333\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.547046788554\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=580)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=47)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "\n",
    "\n",
    "print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of features based on the L1 regularization, instead of inputting all words into PCA to improve the speed\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560) #C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n",
    "plt.plot(pca_mod.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.316069\n",
      "recall_score: 0.271071\n",
      "precision_score: 0.378981\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=19 )#C from the above test\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary'))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
