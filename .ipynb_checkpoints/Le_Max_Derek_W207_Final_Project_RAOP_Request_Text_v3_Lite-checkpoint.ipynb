{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza (RAOP) Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('./pizza_request_dataset.json')\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly assign 70% to train_data, and 30% to dev_data\n",
    "msk = np.random.rand(len(X)) <= 0.7\n",
    "train_data = X[msk]\n",
    "dev_data = X[~msk]\n",
    "\n",
    "# create output dataframe Y of train_labels\n",
    "train_labels = train_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete train_labels from input dataframe of train_data\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "# create output dataframe of dev_labels\n",
    "dev_labels = dev_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete dev_labels from input dataframe of dev_data\n",
    "del dev_data[\"requester_received_pizza\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(train_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(train_labels)', <type 'numpy.ndarray'>)\n",
      "('type(dev_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(dev_labels)', <type 'numpy.ndarray'>) /n\n",
      "('train_labels.shape:', (3975,))\n",
      "('dev_labels.shape:', (1696,))\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "# transform X to numpy array, and Y to 1-D numpy array\n",
    "# view actual text and labels\n",
    "\n",
    "train_text = train_data[\"request_text\"]\n",
    "dev_text = dev_data[\"request_text\"]\n",
    "\n",
    "print (\"type(train_text)\", type(train_text))\n",
    "print (\"type(train_labels)\", type(train_labels))\n",
    "print (\"type(dev_text)\", type(dev_text))\n",
    "print (\"type(dev_labels)\", type(dev_labels)), \"/n\"\n",
    "\n",
    "\n",
    "# view as pandas dataframe\n",
    "#print (\"pandas dataframe:\")\n",
    "#print (train_text[:10]), \"/n\"\n",
    "\n",
    "# convert to numpy (np) array\n",
    "\n",
    "def to_np(d):\n",
    "\n",
    "    #print (\"numpy.ndarray:\")\n",
    "    d = np.array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    #print (type(d))\n",
    "    #print (d.shape)\n",
    "    #print (d[:3])\n",
    "    #print\n",
    "    return d\n",
    "\n",
    "train_text = to_np(train_text)\n",
    "train_labels = to_np(train_labels)\n",
    "dev_text = to_np(dev_text)\n",
    "dev_labels = to_np(dev_labels)\n",
    "\n",
    "# convert to list\n",
    "\n",
    "def to_list(d):\n",
    "\n",
    "    #print (\"list:\")\n",
    "    d = list(d)\n",
    "    #print (type(d))\n",
    "    #print (d[:3])\n",
    "    #print ()\n",
    "    return d\n",
    "\n",
    "#train_text = to_list(train_text)\n",
    "#dev_text = to_list(dev_text)\n",
    "\n",
    "print (\"train_labels.shape:\", train_labels.shape)\n",
    "print (\"dev_labels.shape:\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_vectors.shape:', (3975, 12313))\n",
      "('dev_vectors.shape:', (1696, 12313))\n",
      "\n",
      "------------------------------\n",
      "K Nearest Neighbors (K-NN)\n",
      "------------------------------\n",
      "K-NN: f1_score = 0.4133, k = 1\n",
      "K-NN: f1_score = 0.0259, k = 5\n",
      "K-NN: f1_score = 0.0, k = 15\n",
      "K-NN: f1_score = 0.0, k = 16\n",
      "K-NN: f1_score = 0.0, k = 17\n",
      "K-NN: f1_score = 0.0, k = 18\n",
      "K-NN: f1_score = 0.0, k = 19\n",
      "K-NN: f1_score = 0.0, k = 20\n",
      "K-NN: f1_score = 0.0, k = 28\n",
      "K-NN: f1_score = 0.0, k = 29\n",
      "K-NN: f1_score = 0.0, k = 30\n",
      "K-NN: f1_score = 0.0, k = 31\n",
      "K-NN: f1_score = 0.0, k = 32\n",
      "K-NN: f1_score = 0.0, k = 150\n",
      "K-NN: f1_score = 0.0, k = 300\n",
      "\n",
      "('K-NN: optimal k =', 1)\n",
      "\n",
      "-----------------------------\n",
      "Bernoulli Naive Bayes (BNB)\n",
      "-----------------------------\n",
      "BNB: f1_score = 0.0, alpha = 0.0\n",
      "BNB: f1_score = 0.0844, alpha = 1e-05\n",
      "BNB: f1_score = 0.0844, alpha = 0.0001\n",
      "BNB: f1_score = 0.0844, alpha = 0.001\n",
      "BNB: f1_score = 0.0125, alpha = 0.01\n",
      "BNB: f1_score = 0.0126, alpha = 0.094\n",
      "BNB: f1_score = 0.0126, alpha = 0.095\n",
      "BNB: f1_score = 0.0126, alpha = 0.096\n",
      "BNB: f1_score = 0.0126, alpha = 0.1\n",
      "BNB: f1_score = 0.0126, alpha = 0.105\n",
      "BNB: f1_score = 0.0044, alpha = 0.2\n",
      "BNB: f1_score = 0.0045, alpha = 0.3\n",
      "BNB: f1_score = 0.0, alpha = 0.4\n",
      "BNB: f1_score = 0.0, alpha = 0.5\n",
      "BNB: f1_score = 0.0, alpha = 0.6\n",
      "BNB: f1_score = 0.0, alpha = 0.7\n",
      "BNB: f1_score = 0.0, alpha = 1.0\n",
      "BNB: f1_score = 0.0, alpha = 10.0\n",
      "\n",
      "('Bernoulli Naive Bayes: optimal alpha =', 1e-05)\n",
      "\n",
      "------------------------\n",
      "Logistic Regression (LR)\n",
      "------------------------\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.01\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 0.109621390162\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 9.52499225203\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0045, C = 0.2\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 31.7216994044\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0135, C = 0.3\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 61.681429626\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0178, C = 0.4\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 97.3726951815\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.5\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 137.63336072\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.54\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 154.8503557\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.55\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 159.243434292\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.56\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 163.672952271\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.57\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 168.137600962\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.58\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 172.636591088\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0263, C = 0.59\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 177.169370513\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0306, C = 0.6\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 181.735454014\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0475, C = 0.7\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 229.126241576\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0641, C = 0.8\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 279.382526253\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0761, C = 0.9\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 332.181148169\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0795, C = 1.0\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 387.340589083\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0828, C = 1.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 444.464888316\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2324, C = 10\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 7103.77336434\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2319, C = 12\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 8552.28197252\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2415, C = 20\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 13863.7323633\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2541, C = 30\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 19607.8460883\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2638, C = 40\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 24623.210914\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2701, C = 50\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 29109.2238671\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2739, C = 100\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 46626.217287\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2755, C = 1000\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 151950.673399\n",
      "\n",
      "('Logistic Regression: optimal C =', 1000)\n",
      "\n",
      "('max accuracy =', 74.233490566037744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dschan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: Passing additional arguments to the metric function as **kwargs is deprecated and will no longer be supported in 0.18. Use metric_params instead.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Notes\n",
    "# Classifier precision--when a positive value is predicted, proportion of time the prediction is correct--equals (TP) / (TP + FP)\n",
    "# Classifier recall--when the actual value is positive, the proportion of time the prediction is correct--equals (TP) / (TP + FN)\n",
    "\n",
    "def explore_models():\n",
    "\n",
    "    # create empty vector\n",
    "    accuracies = []\n",
    "\n",
    "    # Source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "    # where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "    # The relative contribution of precision and recall to the F1 score are equal. \n",
    "    # The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "    \n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    print (\"dev_vectors.shape:\", dev_vectors.shape)\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # K Nearest Neighbors\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------------\")\n",
    "    print (\"K Nearest Neighbors (K-NN)\")\n",
    "    print (\"------------------------------\")\n",
    "    \n",
    "    # Euclidean distance, when you go to 10 to 20+ dimensions, too many examples can be close to each other\n",
    "    # With K-NN on text, Cosine or Manhattan distance might be better. Cosine distance measures the angle between examples,\n",
    "    # more robust for high-dimensional problems. \n",
    "    # Dot product measures length of vectors AND angle between these vectors. \n",
    "    # With Cosine distance, you can get a value 0 to 1.\n",
    "    \n",
    "    # create two vectors\n",
    "    # ks refers to a vector of k nearest neighbor values\n",
    "    \n",
    "    ks = [1, 5, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 150, 300]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, distance='cosine', algorithm='brute')\n",
    "        knn.fit(train_vectors, train_labels)\n",
    "        pred_1 = knn.predict(dev_vectors)\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "        # f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]¶\n",
    "            # y_true = Ground truth (correct) target values \n",
    "            # y_pred = Estimated targets as returned by a classifier.\n",
    "            # average = required for multiclass/multilabel targets.\n",
    "                # 'weighted': Calculate metrics for each label, and find their average, weighted by \n",
    "                # the number of true instances for each label. This alters ‘macro’ to account for label imbalance; \n",
    "                # it can result in an F-score that is not between precision and recall.\n",
    "            \n",
    "        print (\"K-NN: f1_score = %s, k = %s\" %(round(metrics.f1_score(dev_labels, pred_1, average='binary'),4), k))\n",
    "\n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_1))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_knn = dict(zip(ks, f1_scores))\n",
    "    #print (results_knn)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"K-NN: optimal k =\", max(results_knn.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "\n",
    "    \n",
    "    #------------------------\n",
    "    # Bernoulli Naive Bayes\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"-----------------------------\")\n",
    "    print (\"Bernoulli Naive Bayes (BNB)\")\n",
    "    print (\"-----------------------------\")\n",
    "    \n",
    "    # create two vectors\n",
    "    \n",
    "    alphas = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.094, 0.095, 0.096, 0.1, 0.105, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0, 10.0]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for a in alphas:\n",
    "        bnb = BernoulliNB(alpha=a,binarize=0.5)\n",
    "        bnb.fit(train_vectors, train_labels)\n",
    "        pred_2 = bnb.predict(dev_vectors)\n",
    "        print (\"BNB: f1_score = %s, alpha = %s\" % (round(metrics.f1_score(dev_labels, pred_2, average='binary'), 4), a))\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_2))\n",
    "        \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_bnb = dict(zip(alphas, f1_scores))\n",
    "    #print (results_mnb)\n",
    "    \n",
    "    # print (the key wit)h the max fl_score\n",
    "    print (\"Bernoulli Naive Bayes: optimal alpha =\", max(results_bnb.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # Logistic Regression\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------\")\n",
    "    print (\"Logistic Regression (LR)\")\n",
    "    print (\"------------------------\")\n",
    "    print\n",
    "    \n",
    "    # create two vectors\n",
    "    # cs refers to the vector of C (inverse of regularization strength) values\n",
    "    \n",
    "    cs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, \\\n",
    "          10, 12, 20, 30, 40, 50, 100, 1000]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for c in cs:\n",
    "        \n",
    "        # logistic regression fits a line like linear regression, but instead of predicting any number, \n",
    "        # it predicts a number between 0 and 1 (sigmoid function).\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # C (inverse of regularization strength) controls how much the weights influence the loss, and\n",
    "        # penalizes the sum of squared weights if very different weights exist between different tokens.\n",
    "  \n",
    "        # use l2 regularization, per instructions\n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "        pred_3 = lr.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-------------------------------\")\n",
    "        print (\"LR: f1_score = %s, C = %s\" % (round(metrics.f1_score(dev_labels, pred_3, average='binary'),4), c))\n",
    "        print (\"-------------------------------\")\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_3, average='binary'))\n",
    "        \n",
    "        accuracies.append((lr.score(dev_vectors, dev_labels))*100) \n",
    "\n",
    "        #print (lr.coef_.shape)\n",
    "        \n",
    "        # first define function that squares a given value, for later use in the 'for loop' below\n",
    "        fun_sq_wts = lambda x: x**2\n",
    "        \n",
    "        # use map function, likely faster (because written in C) than list comprehension.\n",
    "        # map function itself applies a function, specifically the first argument on the second argument.\n",
    "        # from coef_, take raw weights (coefficient of the features in the decision function), \n",
    "        # and sum the squares of these weights.\n",
    "\n",
    "        # note: averege=weight vs. average=default should be about same score if similar number of examples across classes\n",
    "        sq_wts = map(fun_sq_wts, lr.coef_[0])\n",
    "        sum_sq_wts =  sum(sq_wts)\n",
    "        print (\"Label = 1, sum of squared weights = %s\" % (sum_sq_wts))\n",
    "\n",
    "        print\n",
    "        \n",
    "    # map two vectors into a dictionary\n",
    "    results_lr = dict(zip(cs, f1_scores))\n",
    "    #print (results_lr)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"Logistic Regression: optimal C =\", max(results_lr.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    print (\"max accuracy =\", max(accuracies))\n",
    "        \n",
    "explore_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on initial results above, proceed with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "unigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 12313))\n",
      "\n",
      "lr.coef_:\n",
      "[[-1.48092905 -0.39311615 -0.12825177 ...,  0.44453487  1.63180069\n",
      "  -0.87946286]]\n",
      "top 20:\n",
      "[3697, 4147, 6786, 8221, 1039, 6858, 3236, 1286, 6712, 10159, 1523, 10688, 4918, 10689, 9907, 7050, 3901, 2573, 2155, 9349]\n",
      "       Feature      word\n",
      "0         edit  4.824324\n",
      "1       father  4.627494\n",
      "2         mean  4.376547\n",
      "3      pockets  4.057164\n",
      "4          ass  3.995387\n",
      "5    mentioned  3.977975\n",
      "6          die  3.930683\n",
      "7        basic  3.899384\n",
      "8      married  3.850098\n",
      "9     southern -3.846223\n",
      "10       bloke  3.807984\n",
      "11    surprise  3.805289\n",
      "12   graveyard  3.749854\n",
      "13   surprised  3.697465\n",
      "14     sitting -3.697393\n",
      "15       mommy  3.658543\n",
      "16  especially  3.627059\n",
      "17  constantly  3.593246\n",
      "18      cheesy  3.548595\n",
      "19     running  3.539010\n",
      "\n",
      "----------\n",
      "bigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 90693))\n",
      "\n",
      "lr.coef_:\n",
      "[[-0.31775435  0.42001296 -0.29407174 ..., -0.26401881 -0.22234693\n",
      "   0.57766744]]\n",
      "top 20:\n",
      "[23002, 38997, 37806, 32065, 28212, 73029, 33880, 44381, 54471, 23001, 81228, 41801, 77046, 2781, 47597, 9433, 84630, 16480, 57349, 59245]\n",
      "                Feature      word\n",
      "0           edit thanks  3.882043\n",
      "1             imgur com  3.879634\n",
      "2            http imgur  3.547851\n",
      "3             got pizza  3.038731\n",
      "4         forward money  2.984111\n",
      "5        sounds amazing  2.960338\n",
      "6        happy birthday  2.947813\n",
      "7   letsfytinglove best  2.947813\n",
      "8        north carolina  2.936326\n",
      "9            edit thank  2.927036\n",
      "10      tonight greatly  2.813269\n",
      "11           just spent  2.809594\n",
      "12         surprise son  2.768559\n",
      "13         afford ramen  2.674511\n",
      "14             love pie  2.633101\n",
      "15         broke payday  2.627687\n",
      "16               ve got  2.625031\n",
      "17        craving pizza -2.599092\n",
      "18          pay forward  2.518712\n",
      "19       pizza actually  2.512937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "\n",
    "# Feature Selection Notes:\n",
    "'''\n",
    "These objects take as input a scoring function that returns univariate p-values:\n",
    "-For regression: f_regression\n",
    "-For classification: chi2 or f_classif\n",
    "\n",
    "Feature selection with sparse data:\n",
    "-If you use sparse data (i.e. data represented as sparse matrices), \n",
    "only chi2 will deal with the data without making it dense.\n",
    "-Warning: Beware not to use a regression scoring function with a classification problem, \n",
    "you will get useless results.\n",
    "\n",
    "With SVMs and logistic-regression, the parameter C controls the sparsity: \n",
    "the smaller C the less features selected. \n",
    "'''\n",
    "def top20(type):\n",
    "\n",
    "    if type == \"unigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words. \n",
    "        # only applies if default analyzer='word'.\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"unigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "        \n",
    "    elif type == \"bigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words from the resulting tokens. \n",
    "        # only applies if default analyzer='word'.\n",
    "        # set bigrams to be 2 words only\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"bigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "      \n",
    "    # use C=12\n",
    "    for c in [12]:\n",
    "        \n",
    "        # in the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the default ‘multi_class’ option is set to ‘ovr’ \n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        #print (lr)\n",
    "        \n",
    "        # fit the model and generate coef_\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "         \n",
    "        # interested in magnitude of the weights (coefficients), so take absolute value.\n",
    "        # sort absolute values in descending order.\n",
    "        # important to know if negative or positive weight, so still output the positive/negative sign.\n",
    "        # after fitting logistic regression for class vs. all other classes, negative weight of a token \n",
    "        # indicates a class other than class of interest.\n",
    "        # (visual example of negative and positive on a sigmoid function helps) \n",
    "        \n",
    "        print (\"lr.coef_:\")\n",
    "        print (lr.coef_)\n",
    "\n",
    "        # for each label, store the column indices of the top 5 weights \n",
    "        top20 = sorted(range(len(lr.coef_[0])), key=lambda i: abs(lr.coef_[0][i]), reverse=True)[:20]\n",
    "       \n",
    "        col_1 = []\n",
    "        \n",
    "        # for each label, access and store weights via column indices\n",
    "        for index in (top20):\n",
    "\n",
    "            col_1.append(lr.coef_[0][index])\n",
    "           \n",
    "        print (\"top 20:\" )\n",
    "        print (top20)\n",
    "        \n",
    "        # store feature names, after converting to an array\n",
    "        feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "       \n",
    "        # create a Pandas dataframe with 20 rows and 4 columns, plus descriptive headers\n",
    "        df = pd.DataFrame({'Feature': feature_names[top20], 'word': col_1})\n",
    "        print (df    )\n",
    "\n",
    "top20(\"unigram\")\n",
    "top20(\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis\n",
    "This section is the numerical part of the model: utilize all numeric information from the dataset to \n",
    "predict the labels.  This model will be combined the text based model to improve predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.tseries.holiday.USFederalHolidayCalendar object at 0x114d47a90>\n",
      "1341601084\n",
      "2012-07-06 11:58:04\n",
      "<type 'str'>\n",
      "2012-07-06 06:58:04-05:00\n"
     ]
    }
   ],
   "source": [
    "print USFederalHolidayCalendar()\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc'][0]\n",
    "print temp\n",
    "temp2 = datetime.datetime.fromtimestamp(temp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print temp2\n",
    "print type(temp2)\n",
    "\n",
    "# utc = datetime.utcnow()\n",
    "utc = datetime.datetime.strptime(temp2, '%Y-%m-%d %H:%M:%S')\n",
    "utc = utc.replace(tzinfo=from_zone)\n",
    "central = utc.astimezone(to_zone)\n",
    "print central\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc']\n",
    "temp =  temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =   temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "#central =  utc.apply(lambda x: x.astimezone(to_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "\n",
    "temp = pd.DataFrame(temp, columns=[\"State\"])\n",
    "a = temp.iloc[0,0]\n",
    "print a.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "t = train_data['unix_timestamp_of_request_utc'][0]\n",
    "t = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d %H:%M:%S')\n",
    "t = datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "#temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    "#print holidays\n",
    "\n",
    "\n",
    "#for i in temp:\n",
    "    #if i in holidays:\n",
    "       # print i\n",
    "\n",
    "df['test'] = temp\n",
    "#print temp\n",
    "df['out'] = df['test'].apply(lambda x: 1 if x in holidays else 0)\n",
    "print sum(df['out'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  train_data['unix_timestamp_of_request_utc'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    "#print holidays\n",
    "\n",
    "b=(train_data['unix_timestamp_of_request_utc'] \n",
    ".apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    ".apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "a =  temp.apply(lambda x: 1 if x in holidays else 0)\n",
    "print sum(a)\n",
    "print sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82293605  0.17706395]\n",
      " [ 0.71357893  0.28642107]\n",
      " [ 0.82636333  0.17363667]\n",
      " ..., \n",
      " [ 0.86501361  0.13498639]\n",
      " [ 0.73894092  0.26105908]\n",
      " [ 0.81463055  0.18536945]]\n",
      "31\n",
      "[[  9.81734618e-03   3.03840634e-03  -9.82729629e-03  -1.29110551e-01\n",
      "   -3.31832038e-01   2.52833748e-05   7.00367536e-04  -5.00497122e-02\n",
      "    7.30543122e-01   9.81395306e-02]]\n",
      "Model F-Score = 0.0638 \n",
      "Accuracy = 0.7406\n",
      "precision_score: 0.483871\n",
      "recall_score: 0.0342\n",
      "roc_auc_score: 0.5107\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVrtdBHVuiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1006357d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "np.random.seed(0)\n",
    "\n",
    "#Setting up logic to identify timezone, weekend, and holidays\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "weekend = set([5, 6])\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2011-04-07', end='2013-10-11')\n",
    " \n",
    "#Created new df for training data(numeric only)\n",
    "train_data_numeric = np.zeros((len(train_data), 10))\n",
    "dev_data_numeric =  np.zeros((len(dev_data), 10))\n",
    "\n",
    "find_image = re.compile(r'(imgur\\.com|\\.jpg)')  #compile this to combine with other function\n",
    "\n",
    "#####Process train data set##############\n",
    "ut = train_data['unix_timestamp_of_request_utc']\n",
    "temp =  ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp =  temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =   temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "temp = pd.DataFrame(temp, columns=[\"T\"])\n",
    "    \n",
    "train_data_numeric[:,0] = train_data['request_title'].apply(lambda x: len(x.split(' ')))  #Title length\n",
    "train_data_numeric[:,1] = train_data['request_text'].apply(lambda x: len(x.split(' ')))   #Text length\n",
    "train_data_numeric[:,2] = temp['T'].dt.month      #Month in integer\n",
    "train_data_numeric[:,3] = temp['T'].dt.day > 15   #2nd 1/2 month\n",
    "train_data_numeric[:,4] = temp['T'].dt.hour > 12  #Passed noon\n",
    "train_data_numeric[:,5] = train_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "train_data_numeric[:,6] = train_data['requester_account_age_in_days_at_retrieval']\n",
    "#Request during weekend\n",
    "train_data_numeric[:,7] = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).weekday() in weekend)\n",
    "#Has Image\n",
    "train_data_numeric[:,8] = train_data['request_text_edit_aware'].apply(lambda x: 1 if find_image.findall(x) else 0 )\n",
    "#Requested date a holiday\n",
    "train_data_numeric[:,9] = (train_data['unix_timestamp_of_request_utc']\n",
    "                           .apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    "                           .apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "\n",
    "#######process dev dataset#############\n",
    "ut = dev_data['unix_timestamp_of_request_utc']\n",
    "temp = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "temp = temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =  temp.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "temp = pd.DataFrame(temp, columns=[\"T\"])\n",
    "\n",
    "dev_data_numeric[:,0] = dev_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "dev_data_numeric[:,1] = dev_data['request_text'].apply(lambda x: len(x.split(' ')))\n",
    "dev_data_numeric[:,2] = temp['T'].dt.month\n",
    "dev_data_numeric[:,3] = temp['T'].dt.day > 15\n",
    "dev_data_numeric[:,4] = temp['T'].dt.hour > 12 \n",
    "dev_data_numeric[:,5] = dev_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "dev_data_numeric[:,6] = dev_data['requester_account_age_in_days_at_retrieval']\n",
    "#Request during weekend\n",
    "dev_data_numeric[:,7] = ut.apply(lambda x: datetime.datetime.fromtimestamp(x).weekday() in weekend) \n",
    "#Include Image\n",
    "dev_data_numeric[:,8] =  dev_data['request_text_edit_aware'].apply(lambda x: 1 if find_image.findall(x) else 0 )\n",
    "dev_data_numeric[:,9] = (dev_data['unix_timestamp_of_request_utc']\n",
    "                         .apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d')) \n",
    "                         .apply(lambda x: 1 if x in holidays else 0))\n",
    "\n",
    "#[:, [0,1,2,3,4,7,8, 9]]\n",
    "###Start Logistic Regresson on numeric data\n",
    "logit = LogisticRegression(C=1, penalty = 'l2')\n",
    "result = logit.fit(train_data_numeric, train_labels)\n",
    "\n",
    "preds = logit.predict(dev_data_numeric)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit.predict_proba(dev_data_numeric)\n",
    "print pred_probas\n",
    "#How many positive case\n",
    "print preds.sum()\n",
    "\n",
    "#accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "#print \"Accuracy = %0.4f\" % (accuracy)\n",
    "print result.coef_\n",
    "\n",
    "model_output(pred_probas, F_Score, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "    #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "    #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "    #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "    #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "    #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "    #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "    #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "    s = BeautifulSoup(s).get_text() # Newly addition\n",
    "\n",
    "    return s\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=L1)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=L2 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "    \n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "    \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "giver_username_if_known\n",
      "in_test_set\n",
      "number_of_downvotes_of_request_at_retrieval\n",
      "number_of_upvotes_of_request_at_retrieval\n",
      "post_was_edited\n",
      "request_id\n",
      "request_number_of_comments_at_retrieval\n",
      "request_text\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "requester_account_age_in_days_at_request\n",
      "requester_account_age_in_days_at_retrieval\n",
      "requester_days_since_first_post_on_raop_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval\n",
      "requester_number_of_comments_at_request\n",
      "requester_number_of_comments_at_retrieval\n",
      "requester_number_of_comments_in_raop_at_request\n",
      "requester_number_of_comments_in_raop_at_retrieval\n",
      "requester_number_of_posts_at_request\n",
      "requester_number_of_posts_at_retrieval\n",
      "requester_number_of_posts_on_raop_at_request\n",
      "requester_number_of_posts_on_raop_at_retrieval\n",
      "requester_number_of_subreddits_at_request\n",
      "requester_subreddits_at_request\n",
      "requester_upvotes_minus_downvotes_at_request\n",
      "requester_upvotes_minus_downvotes_at_retrieval\n",
      "requester_upvotes_plus_downvotes_at_request\n",
      "requester_upvotes_plus_downvotes_at_retrieval\n",
      "requester_user_flair\n",
      "requester_username\n",
      "unix_timestamp_of_request\n",
      "unix_timestamp_of_request_utc\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print train_data.loc[0,'requester_upvotes_minus_downvotes_at_retrieval']\n",
    "c = 0\n",
    "for i in list(train_data):\n",
    "    c = c +1\n",
    "    print i\n",
    "print len(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1)Baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301158\n",
      "precision_score: 0.346154\n",
      "recall_score: 0.266515\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary'))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2)Add preprocessing and L1 feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>Add Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=580 ,l2=47 , f1 score is 0.336075\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor = first_preprocessor,analyzer='word',stop_words='english', tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,600,10):\n",
    "    for j in range(1,50,2):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Roc curve:\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def model_output(pred_probas, F_Score, preds):\n",
    "\n",
    "    print (\"Model F-Score = %0.4f \"%(F_Score))  #Sum up Squared Weights\n",
    "    accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "    print (\"Accuracy = %0.4f\"    % (accuracy))\n",
    "    print ('precision_score: %f'  % metrics.precision_score(dev_labels, preds))\n",
    "    print ('recall_score: %0.4f' % metrics.recall_score(dev_labels, preds))\n",
    "    print ('roc_auc_score: %0.4f'% metrics.roc_auc_score(dev_labels, preds))\n",
    "\n",
    "#Create ROC Curve\n",
    "\n",
    "    fpr,tpr,_ = roc_curve(dev_labels, pred_probas[:,1])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label='area = %.4f' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Best Model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions: 44\n",
      "Model F-Score = 0.1077 \n",
      "Accuracy = 0.7459\n",
      "precision_score: 0.590909\n",
      "recall_score: 0.0592\n",
      "roc_auc_score: 0.5225\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115811e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best non-ensemble method so far\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=0.01)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "\n",
    "all_data_train = np.append(word_matrix_process[:,index].toarray(), train_data_numeric, 1)\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=0.1)#C from the above test\n",
    "model_LG.fit(all_data_train, train_labels)\n",
    "\n",
    "all_data_dev = np.append(dev_matrix_process[:,index].toarray(), dev_data_numeric, 1)\n",
    "\n",
    "preds = model_LG.predict(all_data_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = model_LG.predict_proba(all_data_dev)\n",
    "\n",
    "print \"Number of positive predictions: %d\" % (sum(preds))\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions: 51\n",
      "Model F-Score = 0.1184 \n",
      "Accuracy = 0.7453\n",
      "precision_score: 0.568627\n",
      "recall_score: 0.0661\n",
      "roc_auc_score: 0.5243\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRV4ppQKYFnmllApg/w8LLQFwFn+VkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f28150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the random forest package\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "#Best non-ensemble method so far\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=0.1)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "\n",
    "all_data_train = np.append(word_matrix_process[:,index].toarray(), train_data_numeric, 1)\n",
    "\n",
    "\n",
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit\n",
    "forest = RandomForestClassifier(n_estimators = 7000)\n",
    "\n",
    "# Fit the training data to the Survived labels and create the decision trees\n",
    "forest = forest.fit(all_data_train, train_labels)\n",
    "\n",
    "\n",
    "#model_LG = LogisticRegression(penalty ='l2',C=0.1)#C from the above test\n",
    "#model_LG.fit(all_data_train, train_labels)\n",
    "\n",
    "all_data_dev = np.append(dev_matrix_process[:,index].toarray(), dev_data_numeric, 1)\n",
    "\n",
    "# Take the same decision trees and run it on the test data\n",
    "preds = forest.predict(all_data_dev)\n",
    "\n",
    "#preds = model_LG.predict(all_data_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = forest.predict_proba(all_data_dev)\n",
    "\n",
    "print \"Number of positive predictions: %d\" % (sum(preds))\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3975, 11018)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "all_data = np.append(word_matrix_process.toarray(), train_data_numeric, 1)\n",
    "print all_data.shape\n",
    "print all_data[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic Ensemble Method:\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process.toarray(), train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "all_data = np.append(word_matrix_process[:,index], train_data_numeric, 1)\n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "\n",
    "\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = model_LG.predict_proba(dev_matrix_process[:,index])\n",
    "\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "394\n",
      "Model F-Score = 0.3289 \n",
      "Accuracy = 0.6704\n",
      "precision_score: 0.347716\n",
      "recall_score: 0.3121\n",
      "roc_auc_score: 0.5538\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154e9cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Basic Ensemble Method:\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "print type(word_matrix_process.toarray())\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process.toarray(), train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "\n",
    "#######Ensemble Method Starts###################################\n",
    "logit = LogisticRegression()\n",
    "logit_en = LogisticRegression()\n",
    "logit.fit(train_data_numeric[:,:], train_labels)\n",
    "\n",
    "#Create a new set of step 2 logistic regression based on step 1 probability outputs\n",
    "#X1: predicted prob. of positive labels with text-based model\n",
    "#X2: predicted prob. of positive labels with text-based model\n",
    "En_X1_train = logit.predict_proba(train_data_numeric[:,:])[:,1]\n",
    "En_X2_train = model_LG.predict_proba(word_matrix_process[:,index])[:,1]\n",
    "En_X1_dev = logit.predict_proba(dev_data_numeric[:,:])[:,1]\n",
    "En_X2_dev = model_LG.predict_proba(dev_matrix_process[:,index])[:,1]\n",
    "\n",
    "t_size = len(En_X1_train)\n",
    "d_size = len(En_X1_dev)\n",
    "En_train = np.zeros((t_size, 2))\n",
    "En_dev = np.zeros((d_size, 2))\n",
    "\n",
    "En_train[:,0] = np.round(En_X1_train, 2)\n",
    "En_train[:,1] = np.round(En_X2_train, 2)\n",
    "En_dev[:,0] =   np.round(En_X1_dev, 2)\n",
    "En_dev[:,1] =   np.round(En_X2_dev, 2)\n",
    "\n",
    "logit_en.fit(En_train, train_labels)\n",
    "\n",
    "preds = logit_en.predict(En_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit_en.predict_proba(En_dev)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3)NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on numeric data: F-Score = 0.3224: \n",
      "Accuracy = 0.6604\n",
      "accuracy_score: 0.333333\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.547046788554\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=580)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=47)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "\n",
    "\n",
    "print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of features based on the L1 regularization, instead of inputting all words into PCA to improve the speed\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560) #C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n",
    "plt.plot(pca_mod.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.316069\n",
      "recall_score: 0.271071\n",
      "precision_score: 0.378981\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=19 )#C from the above test\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary'))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
