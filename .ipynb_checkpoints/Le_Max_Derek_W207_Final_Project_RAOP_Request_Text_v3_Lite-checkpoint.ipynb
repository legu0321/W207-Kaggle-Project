{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza (RAOP) Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maximus/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.19.\n",
      "  \"This module will be removed in 0.19.\", DeprecationWarning)\n",
      "/Users/Maximus/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.19.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_json('./pizza_request_dataset.json')\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly assign 70% to train_data, and 30% to dev_data\n",
    "msk = np.random.rand(len(X)) <= 0.7\n",
    "train_data = X[msk]\n",
    "dev_data = X[~msk]\n",
    "\n",
    "# create output dataframe Y of train_labels\n",
    "train_labels = train_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete train_labels from input dataframe of train_data\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "# create output dataframe of dev_labels\n",
    "dev_labels = dev_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete dev_labels from input dataframe of dev_data\n",
    "del dev_data[\"requester_received_pizza\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(train_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(train_labels)', <class 'pandas.core.frame.DataFrame'>)\n",
      "('type(dev_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(dev_labels)', <class 'pandas.core.frame.DataFrame'>) /n\n",
      "pandas dataframe:\n",
      "0     I'm not in College, or a starving artist or an...\n",
      "2     I'm sure there are needier people on this subr...\n",
      "3     I've been unemployed going on three months now...\n",
      "4     I ran out of money on my meal card a while bac...\n",
      "5                                                      \n",
      "6     Hi amazing people! I've known of this subreddi...\n",
      "9     So it's finals week and I haven't eaten all da...\n",
      "11    I'll write a poem, sing a song, do a dance, pl...\n",
      "12              Thank you so much for the pizza Trisha!\n",
      "14    Hi RAoP\\n\\nAs the title suggests, I have been ...\n",
      "Name: request_text, dtype: object /n\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(3975,)\n",
      "[ u\"I'm not in College, or a starving artist or anything like that. I've just been a bit unlucky lately. I'm a 36 year old single guy with a job. But rent, and other bills killed me this month. I thought I had enough funds in my account to at least keep me set with Mr. Noodles, I forgot about my monthly banking fee.\\n\\nI had a small bag of chips Wednesday afternoon, and I get paid Monday, so I'll be fine then.. It's just really painful at this point and food is something I'm constantly thinking about.\\n\\nI've got a few bucks to get on the bus to work on Saturday, so I can't really use that.\\n\\nI'm really embarrassed to even be asking like this and I'm not sure how it works, so please be patient with me.\\n\\nI guess that covers it. Thank you in advance.\\n\\nCheers folks.\"\n",
      " u\"I'm sure there are needier people on this subreddit, but I had to borrow 30 bucks to pay the rest of my share of rent yesterday and now I'm broke until next friday.  Pizza for me and my housemate that lent me the 30 bucks would be much appreciated for dinner.  Thanks in advance, it's subreddits like this that make Reddit such an awesome place.\\nEDIT:CayucosKid got me covered, 2 pizzas, breadsticks, and a 2 liter of coke to boot!  I'll be eating good for days!  Thanks again.  (=\"\n",
      " u\"I've been unemployed going on three months now, and unfortunately my student girlfriend's part time job at the pharmacy doesn't leave much left over for anything but rice. Here's hoping for a hot pizza.\"]\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(3975,)\n",
      "[ True  True  True]\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(1696,)\n",
      "[ u\"Hello! It's been a hard 2 months with money and I listed some goodies on craigslist so I could make some food money for my family and the people never showed up =(  The cupboards are empty, no bread or P&amp;J. Is there anyone that wouldn't mind helping me out tonight? My family would appreciate it so very much. Thank you for your time in reading our plea. \\n\\nEdit: Tarn33 Came to are rescue and soon my family will be enjoying some much needed pizza!\"\n",
      " u\"I have a couple babysitting gigs lined up next week, but I won't have a single dollar until next week. I'm trying to figure something out until then, but a pizza would last me the rest of the week. I'm in Missouri. Please, for the love of pizza, show me the good side of reddit! You all have saved me in so many ways, but this is my first time asking for a real life helping hand. Thank You!\"\n",
      " u'']\n",
      "\n",
      "numpy.ndarray:\n",
      "<type 'numpy.ndarray'>\n",
      "(1696,)\n",
      "[ True  True  True]\n",
      "\n",
      "('train_labels.shape:', (3975,))\n",
      "('dev_labels.shape:', (1696,))\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "# transform X to numpy array, and Y to 1-D numpy array\n",
    "# view actual text and labels\n",
    "\n",
    "train_text = train_data[\"request_text\"]\n",
    "dev_text = dev_data[\"request_text\"]\n",
    "\n",
    "print (\"type(train_text)\", type(train_text))\n",
    "print (\"type(train_labels)\", type(train_labels))\n",
    "print (\"type(dev_text)\", type(dev_text))\n",
    "print (\"type(dev_labels)\", type(dev_labels)), \"/n\"\n",
    "\n",
    "\n",
    "# view as pandas dataframe\n",
    "print (\"pandas dataframe:\")\n",
    "print (train_text[:10]), \"/n\"\n",
    "\n",
    "# convert to numpy (np) array\n",
    "\n",
    "def to_np(d):\n",
    "\n",
    "    print (\"numpy.ndarray:\")\n",
    "    d = np.array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    print (type(d))\n",
    "    print (d.shape)\n",
    "    print (d[:3])\n",
    "    print\n",
    "    return d\n",
    "\n",
    "train_text = to_np(train_text)\n",
    "train_labels = to_np(train_labels)\n",
    "dev_text = to_np(dev_text)\n",
    "dev_labels = to_np(dev_labels)\n",
    "\n",
    "# convert to list\n",
    "\n",
    "def to_list(d):\n",
    "\n",
    "\n",
    "    print (\"list:\")\n",
    "    d = list(d)\n",
    "    print (type(d))\n",
    "    print (d[:3])\n",
    "    print ()\n",
    "    return d\n",
    "\n",
    "#train_text = to_list(train_text)\n",
    "#dev_text = to_list(dev_text)\n",
    "\n",
    "print (\"train_labels.shape:\", train_labels.shape)\n",
    "print (\"dev_labels.shape:\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_vectors.shape:', (3975, 12313))\n",
      "('dev_vectors.shape:', (1696, 12313))\n",
      "\n",
      "------------------------------\n",
      "K Nearest Neighbors (K-NN)\n",
      "------------------------------\n",
      "K-NN: f1_score = 0.4133, k = 1\n",
      "K-NN: f1_score = 0.0259, k = 5\n",
      "K-NN: f1_score = 0.0, k = 15\n",
      "K-NN: f1_score = 0.0, k = 16\n",
      "K-NN: f1_score = 0.0, k = 17\n",
      "K-NN: f1_score = 0.0, k = 18\n",
      "K-NN: f1_score = 0.0, k = 19\n",
      "K-NN: f1_score = 0.0, k = 20\n",
      "K-NN: f1_score = 0.0, k = 28\n",
      "K-NN: f1_score = 0.0, k = 29\n",
      "K-NN: f1_score = 0.0, k = 30\n",
      "K-NN: f1_score = 0.0, k = 31\n",
      "K-NN: f1_score = 0.0, k = 32\n",
      "K-NN: f1_score = 0.0, k = 150\n",
      "K-NN: f1_score = 0.0, k = 300\n",
      "\n",
      "('K-NN: optimal k =', 1)\n",
      "\n",
      "-----------------------------\n",
      "Bernoulli Naive Bayes (BNB)\n",
      "-----------------------------\n",
      "BNB: f1_score = 0.0, alpha = 0.0\n",
      "BNB: f1_score = 0.0844, alpha = 1e-05\n",
      "BNB: f1_score = 0.0844, alpha = 0.0001\n",
      "BNB: f1_score = 0.0844, alpha = 0.001\n",
      "BNB: f1_score = 0.0125, alpha = 0.01\n",
      "BNB: f1_score = 0.0126, alpha = 0.094\n",
      "BNB: f1_score = 0.0126, alpha = 0.095\n",
      "BNB: f1_score = 0.0126, alpha = 0.096\n",
      "BNB: f1_score = 0.0126, alpha = 0.1\n",
      "BNB: f1_score = 0.0126, alpha = 0.105\n",
      "BNB: f1_score = 0.0044, alpha = 0.2\n",
      "BNB: f1_score = 0.0045, alpha = 0.3\n",
      "BNB: f1_score = 0.0, alpha = 0.4\n",
      "BNB: f1_score = 0.0, alpha = 0.5\n",
      "BNB: f1_score = 0.0, alpha = 0.6\n",
      "BNB: f1_score = 0.0, alpha = 0.7\n",
      "BNB: f1_score = 0.0, alpha = 1.0\n",
      "BNB: f1_score = 0.0, alpha = 10.0\n",
      "\n",
      "('Bernoulli Naive Bayes: optimal alpha =', 1e-05)\n",
      "\n",
      "------------------------\n",
      "Logistic Regression (LR)\n",
      "------------------------\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.01\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 0.109621390162\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 9.52499225203\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0045, C = 0.2\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 31.7216994044\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0135, C = 0.3\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 61.681429626\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0178, C = 0.4\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 97.3726951815\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.5\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 137.63336072\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.54\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 154.8503557\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.55\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 159.243434292\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.56\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 163.672952271\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.57\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 168.137600962\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.58\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 172.636591088\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0263, C = 0.59\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 177.169370513\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0306, C = 0.6\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 181.735454014\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0475, C = 0.7\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 229.126241576\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0641, C = 0.8\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 279.382526253\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0761, C = 0.9\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 332.181148169\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0795, C = 1.0\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 387.340589083\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0828, C = 1.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 444.464888316\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2324, C = 10\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 7103.77336434\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2319, C = 12\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 8552.28197252\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2415, C = 20\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 13863.7323633\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2541, C = 30\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 19607.8460883\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2638, C = 40\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 24623.210914\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2701, C = 50\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 29109.2238671\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2739, C = 100\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 46626.217287\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2755, C = 1000\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 151950.673399\n",
      "\n",
      "('Logistic Regression: optimal C =', 1000)\n",
      "\n",
      "('max accuracy =', 74.233490566037744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dschan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: Passing additional arguments to the metric function as **kwargs is deprecated and will no longer be supported in 0.18. Use metric_params instead.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "# http://stackoverflow.com/questions/209840/map-two-lists-into-a-dictionary-in-python\n",
    "# http://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "\n",
    "# Notes\n",
    "# Classifier precision--when a positive value is predicted, proportion of time the prediction is correct--equals (TP) / (TP + FP)\n",
    "# Classifier recall--when the actual value is positive, the proportion of time the prediction is correct--equals (TP) / (TP + FN)\n",
    "\n",
    "def explore_models():\n",
    "### STUDENT START ###\n",
    "\n",
    "    # create empty vector\n",
    "    accuracies = []\n",
    "\n",
    "    # Source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "    # where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "    # The relative contribution of precision and recall to the F1 score are equal. \n",
    "    # The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "    \n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    print (\"dev_vectors.shape:\", dev_vectors.shape)\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # K Nearest Neighbors\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------------\")\n",
    "    print (\"K Nearest Neighbors (K-NN)\")\n",
    "    print (\"------------------------------\")\n",
    "    \n",
    "    # Euclidean distance, when you go to 10 to 20+ dimensions, too many examples can be close to each other\n",
    "    # With K-NN on text, Cosine or Manhattan distance might be better. Cosine distance measures the angle between examples,\n",
    "    # more robust for high-dimensional problems. \n",
    "    # Dot product measures length of vectors AND angle between these vectors. \n",
    "    # With Cosine distance, you can get a value 0 to 1.\n",
    "    \n",
    "    # create two vectors\n",
    "    # ks refers to a vector of k nearest neighbor values\n",
    "    \n",
    "    ks = [1, 5, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 150, 300]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, distance='cosine', algorithm='brute')\n",
    "        knn.fit(train_vectors, train_labels)\n",
    "        pred_1 = knn.predict(dev_vectors)\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "        # f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]¶\n",
    "            # y_true = Ground truth (correct) target values \n",
    "            # y_pred = Estimated targets as returned by a classifier.\n",
    "            # average = required for multiclass/multilabel targets.\n",
    "                # 'weighted': Calculate metrics for each label, and find their average, weighted by \n",
    "                # the number of true instances for each label. This alters ‘macro’ to account for label imbalance; \n",
    "                # it can result in an F-score that is not between precision and recall.\n",
    "            \n",
    "        print (\"K-NN: f1_score = %s, k = %s\" %(round(metrics.f1_score(dev_labels, pred_1, average='binary'),4), k))\n",
    "\n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_1))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_knn = dict(zip(ks, f1_scores))\n",
    "    #print (results_knn)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"K-NN: optimal k =\", max(results_knn.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "\n",
    "    \n",
    "    #------------------------\n",
    "    # Bernoulli Naive Bayes\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"-----------------------------\")\n",
    "    print (\"Bernoulli Naive Bayes (BNB)\")\n",
    "    print (\"-----------------------------\")\n",
    "    \n",
    "    # create two vectors\n",
    "    \n",
    "    alphas = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.094, 0.095, 0.096, 0.1, 0.105, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0, 10.0]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for a in alphas:\n",
    "        bnb = BernoulliNB(alpha=a,binarize=0.5)\n",
    "        bnb.fit(train_vectors, train_labels)\n",
    "        pred_2 = bnb.predict(dev_vectors)\n",
    "        print (\"BNB: f1_score = %s, alpha = %s\" % (round(metrics.f1_score(dev_labels, pred_2, average='binary'), 4), a))\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_2))\n",
    "        \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_bnb = dict(zip(alphas, f1_scores))\n",
    "    #print (results_mnb)\n",
    "    \n",
    "    # print (the key wit)h the max fl_score\n",
    "    print (\"Bernoulli Naive Bayes: optimal alpha =\", max(results_bnb.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # Logistic Regression\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------\")\n",
    "    print (\"Logistic Regression (LR)\")\n",
    "    print (\"------------------------\")\n",
    "    print\n",
    "    \n",
    "    # create two vectors\n",
    "    # cs refers to the vector of C (inverse of regularization strength) values\n",
    "    \n",
    "    cs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, \\\n",
    "          10, 12, 20, 30, 40, 50, 100, 1000]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for c in cs:\n",
    "        \n",
    "        # logistic regression fits a line like linear regression, but instead of predicting any number, \n",
    "        # it predicts a number between 0 and 1 (sigmoid function).\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # C (inverse of regularization strength) controls how much the weights influence the loss, and\n",
    "        # penalizes the sum of squared weights if very different weights exist between different tokens.\n",
    "  \n",
    "        # use l2 regularization, per instructions\n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "        pred_3 = lr.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-------------------------------\")\n",
    "        print (\"LR: f1_score = %s, C = %s\" % (round(metrics.f1_score(dev_labels, pred_3, average='binary'),4), c))\n",
    "        print (\"-------------------------------\")\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_3, average='binary'))\n",
    "        \n",
    "        accuracies.append((lr.score(dev_vectors, dev_labels))*100) \n",
    "\n",
    "        #print (lr.coef_.shape)\n",
    "        \n",
    "        # first define function that squares a given value, for later use in the 'for loop' below\n",
    "        fun_sq_wts = lambda x: x**2\n",
    "        \n",
    "        # use map function, likely faster (because written in C) than list comprehension.\n",
    "        # map function itself applies a function, specifically the first argument on the second argument.\n",
    "        # from coef_, take raw weights (coefficient of the features in the decision function), \n",
    "        # and sum the squares of these weights.\n",
    "\n",
    "        # note: averege=weight vs. average=default should be about same score if similar number of examples across classes\n",
    "        sq_wts = map(fun_sq_wts, lr.coef_[0])\n",
    "        sum_sq_wts =  sum(sq_wts)\n",
    "        print (\"Label = 1, sum of squared weights = %s\" % (sum_sq_wts))\n",
    "\n",
    "        print\n",
    "        \n",
    "    # map two vectors into a dictionary\n",
    "    results_lr = dict(zip(cs, f1_scores))\n",
    "    #print (results_lr)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"Logistic Regression: optimal C =\", max(results_lr.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    print (\"max accuracy =\", max(accuracies))\n",
    "        \n",
    "### STUDENT END ###\n",
    "\n",
    "explore_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on initial results above, proceed with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "unigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 12313))\n",
      "\n",
      "lr.coef_:\n",
      "[[-1.48092905 -0.39311615 -0.12825177 ...,  0.44453487  1.63180069\n",
      "  -0.87946286]]\n",
      "\n",
      "top 20:\n",
      "[3697, 4147, 6786, 8221, 1039, 6858, 3236, 1286, 6712, 10159, 1523, 10688, 4918, 10689, 9907, 7050, 3901, 2573, 2155, 9349]\n",
      "\n",
      "       Feature      word\n",
      "0         edit  4.824324\n",
      "1       father  4.627494\n",
      "2         mean  4.376547\n",
      "3      pockets  4.057164\n",
      "4          ass  3.995387\n",
      "5    mentioned  3.977975\n",
      "6          die  3.930683\n",
      "7        basic  3.899384\n",
      "8      married  3.850098\n",
      "9     southern -3.846223\n",
      "10       bloke  3.807984\n",
      "11    surprise  3.805289\n",
      "12   graveyard  3.749854\n",
      "13   surprised  3.697465\n",
      "14     sitting -3.697393\n",
      "15       mommy  3.658543\n",
      "16  especially  3.627059\n",
      "17  constantly  3.593246\n",
      "18      cheesy  3.548595\n",
      "19     running  3.539010\n",
      "\n",
      "----------\n",
      "bigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 90693))\n",
      "\n",
      "lr.coef_:\n",
      "[[-0.31775435  0.42001296 -0.29407174 ..., -0.26401881 -0.22234693\n",
      "   0.57766744]]\n",
      "\n",
      "top 20:\n",
      "[23002, 38997, 37806, 32065, 28212, 73029, 33880, 44381, 54471, 23001, 81228, 41801, 77046, 2781, 47597, 9433, 84630, 16480, 57349, 59245]\n",
      "\n",
      "                Feature      word\n",
      "0           edit thanks  3.882043\n",
      "1             imgur com  3.879634\n",
      "2            http imgur  3.547851\n",
      "3             got pizza  3.038731\n",
      "4         forward money  2.984111\n",
      "5        sounds amazing  2.960338\n",
      "6        happy birthday  2.947813\n",
      "7   letsfytinglove best  2.947813\n",
      "8        north carolina  2.936326\n",
      "9            edit thank  2.927036\n",
      "10      tonight greatly  2.813269\n",
      "11           just spent  2.809594\n",
      "12         surprise son  2.768559\n",
      "13         afford ramen  2.674511\n",
      "14             love pie  2.633101\n",
      "15         broke payday  2.627687\n",
      "16               ve got  2.625031\n",
      "17        craving pizza -2.599092\n",
      "18          pay forward  2.518712\n",
      "19       pizza actually  2.512937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "\n",
    "# Feature Selection Notes:\n",
    "'''\n",
    "These objects take as input a scoring function that returns univariate p-values:\n",
    "-For regression: f_regression\n",
    "-For classification: chi2 or f_classif\n",
    "\n",
    "Feature selection with sparse data:\n",
    "-If you use sparse data (i.e. data represented as sparse matrices), \n",
    "only chi2 will deal with the data without making it dense.\n",
    "-Warning: Beware not to use a regression scoring function with a classification problem, \n",
    "you will get useless results.\n",
    "\n",
    "With SVMs and logistic-regression, the parameter C controls the sparsity: \n",
    "the smaller C the less features selected. \n",
    "'''\n",
    "\n",
    "def top20(type):\n",
    "### STUDENT START ###\n",
    "\n",
    "    if type == \"unigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words. \n",
    "        # only applies if default analyzer='word'.\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"unigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "        \n",
    "    elif type == \"bigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words from the resulting tokens. \n",
    "        # only applies if default analyzer='word'.\n",
    "        # set bigrams to be 2 words only\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"bigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "      \n",
    "    # use C=12\n",
    "    for c in [12]:\n",
    "        \n",
    "        # in the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the default ‘multi_class’ option is set to ‘ovr’ \n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        #print (lr)\n",
    "        \n",
    "        # fit the model and generate coef_\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "         \n",
    "        # interested in magnitude of the weights (coefficients), so take absolute value.\n",
    "        # sort absolute values in descending order.\n",
    "        # important to know if negative or positive weight, so still output the positive/negative sign.\n",
    "        # after fitting logistic regression for class vs. all other classes, negative weight of a token \n",
    "        # indicates a class other than class of interest.\n",
    "        # (visual example of negative and positive on a sigmoid function helps) \n",
    "        \n",
    "        print (\"lr.coef_:\")\n",
    "        print (lr.coef_)\n",
    "        print\n",
    "        \n",
    "        # for each label, store the column indices of the top 5 weights \n",
    "        top20 = sorted(range(len(lr.coef_[0])), key=lambda i: abs(lr.coef_[0][i]), reverse=True)[:20]\n",
    "       \n",
    "        col_1 = []\n",
    "        \n",
    "        # for each label, access and store weights via column indices\n",
    "        for index in (top20):\n",
    "\n",
    "            col_1.append(lr.coef_[0][index])\n",
    "           \n",
    "        print (\"top 20:\" )\n",
    "        print (top20)\n",
    "        print\n",
    "        \n",
    "        # store feature names, after converting to an array\n",
    "        feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "       \n",
    "        # create a Pandas dataframe with 20 rows and 4 columns, plus descriptive headers\n",
    "        df = pd.DataFrame({'Feature': feature_names[top20], 'word': col_1})\n",
    "        print (df    )\n",
    "\n",
    "#-----\n",
    "         \n",
    "### STUDENT END ###\n",
    "top20(\"unigram\")\n",
    "top20(\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis\n",
    "This section is the numerical part of the model: utilize all numeric information from the dataset to \n",
    "predict the labels.  This model will be combined the text based model to improve predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Accuracy = 0.7429\n",
      "[[  8.52762359e-03   3.15358948e-03  -1.22440571e-02  -6.71720133e-01\n",
      "   -6.71720133e-01   2.69088161e-05   6.63444429e-04]]\n",
      "Logistic regression on numeric data: F-Score = 0.0522: \n",
      "Accuracy = 0.7429\n",
      "accuracy_score: 0.571429\n",
      "recall_score: 0.027335\n",
      "roc_auc_score: 0.510087473701\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd3cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118444990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "np.random.seed(0)\n",
    "#Extra numerical data from text and text titles \n",
    "\n",
    "#Created new df for training data(numeric only)\n",
    "train_data_numeric = np.zeros((len(train_data), 7))\n",
    "dev_data_numeric =  np.zeros((len(dev_data), 7))\n",
    "\n",
    "temp = train_data['unix_timestamp_of_request']\n",
    "\n",
    "train_data_numeric[:,0]= train_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "train_data_numeric[:,1]= train_data['request_text'].apply(lambda x: len(x.split(' '))) \n",
    "train_data_numeric[:,2]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m')) # Month in integer\n",
    "train_data_numeric[:,3]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%d')) > 15  # Second Half\n",
    "train_data_numeric[:,4]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%H')) > 12\n",
    "train_data_numeric[:,5] = train_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "train_data_numeric[:,6] = train_data['requester_account_age_in_days_at_retrieval']\n",
    "\n",
    "#post_was_edited\n",
    "#Created new df for dev data(numeric only)\n",
    "\n",
    "temp = dev_data['unix_timestamp_of_request']\n",
    "\n",
    "dev_data_numeric[:,0]= dev_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "dev_data_numeric[:,1]= dev_data['request_text'].apply(lambda x: len(x.split(' ')))\n",
    "dev_data_numeric[:,2]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m'))\n",
    "dev_data_numeric[:,3]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%d')) > 15\n",
    "dev_data_numeric[:,4]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%H')) > 12 \n",
    "dev_data_numeric[:,5] = dev_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "dev_data_numeric[:,6] = dev_data['requester_account_age_in_days_at_retrieval']\n",
    "\n",
    "\n",
    "logit = LogisticRegression(C=1, penalty = 'l2')\n",
    "result = logit.fit(train_data_numeric, train_labels)\n",
    "\n",
    "preds = logit.predict(dev_data_numeric)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit.predict_proba(dev_data_numeric)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print \"Accuracy = %0.4f\" % (accuracy)\n",
    "print result.coef_\n",
    "\n",
    "model_output(pred_probas, F_Score, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "    #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "    #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "    #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "    #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "    #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "    #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "    #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "    s = BeautifulSoup(s).get_text() # Newly addition\n",
    "\n",
    "    return s\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=L1)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=L2 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "    \n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "    \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "giver_username_if_known\n",
      "in_test_set\n",
      "number_of_downvotes_of_request_at_retrieval\n",
      "number_of_upvotes_of_request_at_retrieval\n",
      "post_was_edited\n",
      "request_id\n",
      "request_number_of_comments_at_retrieval\n",
      "request_text\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "requester_account_age_in_days_at_request\n",
      "requester_account_age_in_days_at_retrieval\n",
      "requester_days_since_first_post_on_raop_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval\n",
      "requester_number_of_comments_at_request\n",
      "requester_number_of_comments_at_retrieval\n",
      "requester_number_of_comments_in_raop_at_request\n",
      "requester_number_of_comments_in_raop_at_retrieval\n",
      "requester_number_of_posts_at_request\n",
      "requester_number_of_posts_at_retrieval\n",
      "requester_number_of_posts_on_raop_at_request\n",
      "requester_number_of_posts_on_raop_at_retrieval\n",
      "requester_number_of_subreddits_at_request\n",
      "requester_subreddits_at_request\n",
      "requester_upvotes_minus_downvotes_at_request\n",
      "requester_upvotes_minus_downvotes_at_retrieval\n",
      "requester_upvotes_plus_downvotes_at_request\n",
      "requester_upvotes_plus_downvotes_at_retrieval\n",
      "requester_user_flair\n",
      "requester_username\n",
      "unix_timestamp_of_request\n",
      "unix_timestamp_of_request_utc\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print train_data.loc[0,'requester_upvotes_minus_downvotes_at_retrieval']\n",
    "c = 0\n",
    "for i in list(train_data):\n",
    "    c = c +1\n",
    "    print i\n",
    "print len(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1)Baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301158\n",
      "precision_score: 0.346154\n",
      "recall_score: 0.266515\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary'))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2)Add preprocessing and L1 feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>Add Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=580 ,l2=47 , f1 score is 0.336075\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor = first_preprocessor,analyzer='word',stop_words='english', tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,600,10):\n",
    "    for j in range(1,50,2):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Roc curve:\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def model_output(pred_probas, F_Score, preds):\n",
    "\n",
    "    print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "    accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "    print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "    print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "    print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "    print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)\n",
    "\n",
    "#Create ROC Curve\n",
    "\n",
    "    fpr,tpr,_ = roc_curve(dev_labels, pred_probas[:,1])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Best Model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      "Logistic regression on numeric data: F-Score = 0.3252: \n",
      "Accuracy = 0.6745\n",
      "accuracy_score: 0.350923\n",
      "recall_score: 0.302961\n",
      "roc_auc_score: 0.553628609174\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEgpiQvIhLElORFRIKYkryISBD7/1MJzHbs54WuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137fe210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best non-ensemble method so far\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = model_LG.predict_proba(dev_matrix_process[:,index])\n",
    "\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "Logistic regression on numeric data: F-Score = 0.3285: \n",
      "Accuracy = 0.6698\n",
      "accuracy_score: 0.346835\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.55341114814\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c31890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Basic Ensemble Method:\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "\n",
    "#######Ensemble Method Starts###################################\n",
    "logit = LogisticRegression()\n",
    "logit_en = LogisticRegression()\n",
    "logit.fit(train_data_numeric[:,2:5], train_labels)\n",
    "\n",
    "#Create a new set of step 2 logistic regression based on step 1 probability outputs\n",
    "#X1: predicted prob. of positive labels with text-based model\n",
    "#X2: predicted prob. of positive labels with text-based model\n",
    "En_X1_train = logit.predict_proba(train_data_numeric[:,2:5])[:,1]\n",
    "En_X2_train = model_LG.predict_proba(word_matrix_process[:,index])[:,1]\n",
    "En_X1_dev = logit.predict_proba(dev_data_numeric[:,2:5])[:,1]\n",
    "En_X2_dev = model_LG.predict_proba(dev_matrix_process[:,index])[:,1]\n",
    "\n",
    "t_size = len(En_X1_train)\n",
    "d_size = len(En_X1_dev)\n",
    "En_train = np.zeros((t_size, 2))\n",
    "En_dev = np.zeros((d_size, 2))\n",
    "\n",
    "En_train[:,0] = np.round(En_X1_train, 2)\n",
    "En_train[:,1] = np.round(En_X2_train, 2)\n",
    "En_dev[:,0] =   np.round(En_X1_dev, 2)\n",
    "En_dev[:,1] =   np.round(En_X2_dev, 2)\n",
    "\n",
    "logit_en.fit(En_train, train_labels)\n",
    "\n",
    "preds = logit_en.predict(En_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit_en.predict_proba(En_dev)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3)NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on numeric data: F-Score = 0.3224: \n",
      "Accuracy = 0.6604\n",
      "accuracy_score: 0.333333\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.547046788554\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=580)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=47)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "\n",
    "\n",
    "print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of features based on the L1 regularization, instead of inputting all words into PCA to improve the speed\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560) #C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n",
    "plt.plot(pca_mod.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.316069\n",
      "recall_score: 0.271071\n",
      "precision_score: 0.378981\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=19 )#C from the above test\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary'))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
