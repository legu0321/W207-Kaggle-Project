{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza (RAOP) Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('./pizza_request_dataset.json')\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly assign 70% to train_data, and 30% to dev_data\n",
    "msk = np.random.rand(len(X)) <= 0.7\n",
    "train_data = X[msk]\n",
    "dev_data = X[~msk]\n",
    "\n",
    "# create output dataframe Y of train_labels\n",
    "train_labels = train_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete train_labels from input dataframe of train_data\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "# create output dataframe of dev_labels\n",
    "dev_labels = dev_data[[\"requester_received_pizza\"]]\n",
    "\n",
    "# delete dev_labels from input dataframe of dev_data\n",
    "del dev_data[\"requester_received_pizza\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type(train_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(train_labels)', <type 'numpy.ndarray'>)\n",
      "('type(dev_text)', <class 'pandas.core.series.Series'>)\n",
      "('type(dev_labels)', <type 'numpy.ndarray'>) /n\n",
      "('train_labels.shape:', (3975,))\n",
      "('dev_labels.shape:', (1696,))\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "# transform X to numpy array, and Y to 1-D numpy array\n",
    "# view actual text and labels\n",
    "\n",
    "train_text = train_data[\"request_text\"]\n",
    "dev_text = dev_data[\"request_text\"]\n",
    "\n",
    "print (\"type(train_text)\", type(train_text))\n",
    "print (\"type(train_labels)\", type(train_labels))\n",
    "print (\"type(dev_text)\", type(dev_text))\n",
    "print (\"type(dev_labels)\", type(dev_labels)), \"/n\"\n",
    "\n",
    "\n",
    "# view as pandas dataframe\n",
    "#print (\"pandas dataframe:\")\n",
    "#print (train_text[:10]), \"/n\"\n",
    "\n",
    "# convert to numpy (np) array\n",
    "\n",
    "def to_np(d):\n",
    "\n",
    "    #print (\"numpy.ndarray:\")\n",
    "    d = np.array(d)\n",
    "    # http://stackoverflow.com/questions/13730468/from-2d-to-1d-arrays\n",
    "    d = d.flatten()\n",
    "    #print (type(d))\n",
    "    #print (d.shape)\n",
    "    #print (d[:3])\n",
    "    #print\n",
    "    return d\n",
    "\n",
    "train_text = to_np(train_text)\n",
    "train_labels = to_np(train_labels)\n",
    "dev_text = to_np(dev_text)\n",
    "dev_labels = to_np(dev_labels)\n",
    "\n",
    "# convert to list\n",
    "\n",
    "def to_list(d):\n",
    "\n",
    "    #print (\"list:\")\n",
    "    d = list(d)\n",
    "    #print (type(d))\n",
    "    #print (d[:3])\n",
    "    #print ()\n",
    "    return d\n",
    "\n",
    "#train_text = to_list(train_text)\n",
    "#dev_text = to_list(dev_text)\n",
    "\n",
    "print (\"train_labels.shape:\", train_labels.shape)\n",
    "print (\"dev_labels.shape:\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_vectors.shape:', (3975, 12313))\n",
      "('dev_vectors.shape:', (1696, 12313))\n",
      "\n",
      "------------------------------\n",
      "K Nearest Neighbors (K-NN)\n",
      "------------------------------\n",
      "K-NN: f1_score = 0.4133, k = 1\n",
      "K-NN: f1_score = 0.0259, k = 5\n",
      "K-NN: f1_score = 0.0, k = 15\n",
      "K-NN: f1_score = 0.0, k = 16\n",
      "K-NN: f1_score = 0.0, k = 17\n",
      "K-NN: f1_score = 0.0, k = 18\n",
      "K-NN: f1_score = 0.0, k = 19\n",
      "K-NN: f1_score = 0.0, k = 20\n",
      "K-NN: f1_score = 0.0, k = 28\n",
      "K-NN: f1_score = 0.0, k = 29\n",
      "K-NN: f1_score = 0.0, k = 30\n",
      "K-NN: f1_score = 0.0, k = 31\n",
      "K-NN: f1_score = 0.0, k = 32\n",
      "K-NN: f1_score = 0.0, k = 150\n",
      "K-NN: f1_score = 0.0, k = 300\n",
      "\n",
      "('K-NN: optimal k =', 1)\n",
      "\n",
      "-----------------------------\n",
      "Bernoulli Naive Bayes (BNB)\n",
      "-----------------------------\n",
      "BNB: f1_score = 0.0, alpha = 0.0\n",
      "BNB: f1_score = 0.0844, alpha = 1e-05\n",
      "BNB: f1_score = 0.0844, alpha = 0.0001\n",
      "BNB: f1_score = 0.0844, alpha = 0.001\n",
      "BNB: f1_score = 0.0125, alpha = 0.01\n",
      "BNB: f1_score = 0.0126, alpha = 0.094\n",
      "BNB: f1_score = 0.0126, alpha = 0.095\n",
      "BNB: f1_score = 0.0126, alpha = 0.096\n",
      "BNB: f1_score = 0.0126, alpha = 0.1\n",
      "BNB: f1_score = 0.0126, alpha = 0.105\n",
      "BNB: f1_score = 0.0044, alpha = 0.2\n",
      "BNB: f1_score = 0.0045, alpha = 0.3\n",
      "BNB: f1_score = 0.0, alpha = 0.4\n",
      "BNB: f1_score = 0.0, alpha = 0.5\n",
      "BNB: f1_score = 0.0, alpha = 0.6\n",
      "BNB: f1_score = 0.0, alpha = 0.7\n",
      "BNB: f1_score = 0.0, alpha = 1.0\n",
      "BNB: f1_score = 0.0, alpha = 10.0\n",
      "\n",
      "('Bernoulli Naive Bayes: optimal alpha =', 1e-05)\n",
      "\n",
      "------------------------\n",
      "Logistic Regression (LR)\n",
      "------------------------\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.01\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 0.109621390162\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0, C = 0.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 9.52499225203\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0045, C = 0.2\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 31.7216994044\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0135, C = 0.3\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 61.681429626\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0178, C = 0.4\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 97.3726951815\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.5\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 137.63336072\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0177, C = 0.54\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 154.8503557\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.55\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 159.243434292\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0221, C = 0.56\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 163.672952271\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.57\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 168.137600962\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.022, C = 0.58\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 172.636591088\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0263, C = 0.59\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 177.169370513\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0306, C = 0.6\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 181.735454014\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0475, C = 0.7\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 229.126241576\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0641, C = 0.8\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 279.382526253\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0761, C = 0.9\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 332.181148169\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0795, C = 1.0\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 387.340589083\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.0828, C = 1.1\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 444.464888316\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2324, C = 10\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 7103.77336434\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2319, C = 12\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 8552.28197252\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2415, C = 20\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 13863.7323633\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2541, C = 30\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 19607.8460883\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2638, C = 40\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 24623.210914\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2701, C = 50\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 29109.2238671\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2739, C = 100\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 46626.217287\n",
      "\n",
      "-------------------------------\n",
      "LR: f1_score = 0.2755, C = 1000\n",
      "-------------------------------\n",
      "Label = 1, sum of squared weights = 151950.673399\n",
      "\n",
      "('Logistic Regression: optimal C =', 1000)\n",
      "\n",
      "('max accuracy =', 74.233490566037744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dschan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: Passing additional arguments to the metric function as **kwargs is deprecated and will no longer be supported in 0.18. Use metric_params instead.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Notes\n",
    "# Classifier precision--when a positive value is predicted, proportion of time the prediction is correct--equals (TP) / (TP + FP)\n",
    "# Classifier recall--when the actual value is positive, the proportion of time the prediction is correct--equals (TP) / (TP + FN)\n",
    "\n",
    "def explore_models():\n",
    "\n",
    "    # create empty vector\n",
    "    accuracies = []\n",
    "\n",
    "    # Source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "    # where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "    # The relative contribution of precision and recall to the F1 score are equal. \n",
    "    # The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "    train_vectors = vectorizer.fit_transform(train_text)\n",
    "    print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "    \n",
    "    dev_vectors = vectorizer.transform(dev_text)\n",
    "    print (\"dev_vectors.shape:\", dev_vectors.shape)\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # K Nearest Neighbors\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------------\")\n",
    "    print (\"K Nearest Neighbors (K-NN)\")\n",
    "    print (\"------------------------------\")\n",
    "    \n",
    "    # Euclidean distance, when you go to 10 to 20+ dimensions, too many examples can be close to each other\n",
    "    # With K-NN on text, Cosine or Manhattan distance might be better. Cosine distance measures the angle between examples,\n",
    "    # more robust for high-dimensional problems. \n",
    "    # Dot product measures length of vectors AND angle between these vectors. \n",
    "    # With Cosine distance, you can get a value 0 to 1.\n",
    "    \n",
    "    # create two vectors\n",
    "    # ks refers to a vector of k nearest neighbor values\n",
    "    \n",
    "    ks = [1, 5, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 150, 300]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, distance='cosine', algorithm='brute')\n",
    "        knn.fit(train_vectors, train_labels)\n",
    "        pred_1 = knn.predict(dev_vectors)\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "        # f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]¶\n",
    "            # y_true = Ground truth (correct) target values \n",
    "            # y_pred = Estimated targets as returned by a classifier.\n",
    "            # average = required for multiclass/multilabel targets.\n",
    "                # 'weighted': Calculate metrics for each label, and find their average, weighted by \n",
    "                # the number of true instances for each label. This alters ‘macro’ to account for label imbalance; \n",
    "                # it can result in an F-score that is not between precision and recall.\n",
    "            \n",
    "        print (\"K-NN: f1_score = %s, k = %s\" %(round(metrics.f1_score(dev_labels, pred_1, average='binary'),4), k))\n",
    "\n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_1))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_knn = dict(zip(ks, f1_scores))\n",
    "    #print (results_knn)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"K-NN: optimal k =\", max(results_knn.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "\n",
    "    \n",
    "    #------------------------\n",
    "    # Bernoulli Naive Bayes\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"-----------------------------\")\n",
    "    print (\"Bernoulli Naive Bayes (BNB)\")\n",
    "    print (\"-----------------------------\")\n",
    "    \n",
    "    # create two vectors\n",
    "    \n",
    "    alphas = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.094, 0.095, 0.096, 0.1, 0.105, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0, 10.0]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for a in alphas:\n",
    "        bnb = BernoulliNB(alpha=a,binarize=0.5)\n",
    "        bnb.fit(train_vectors, train_labels)\n",
    "        pred_2 = bnb.predict(dev_vectors)\n",
    "        print (\"BNB: f1_score = %s, alpha = %s\" % (round(metrics.f1_score(dev_labels, pred_2, average='binary'), 4), a))\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_2))\n",
    "        \n",
    "    print\n",
    "    \n",
    "    # map two vectors into a dictionary\n",
    "    results_bnb = dict(zip(alphas, f1_scores))\n",
    "    #print (results_mnb)\n",
    "    \n",
    "    # print (the key wit)h the max fl_score\n",
    "    print (\"Bernoulli Naive Bayes: optimal alpha =\", max(results_bnb.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    \n",
    "    #------------------------\n",
    "    # Logistic Regression\n",
    "    #------------------------\n",
    "    \n",
    "    print (\"------------------------\")\n",
    "    print (\"Logistic Regression (LR)\")\n",
    "    print (\"------------------------\")\n",
    "    print\n",
    "    \n",
    "    # create two vectors\n",
    "    # cs refers to the vector of C (inverse of regularization strength) values\n",
    "    \n",
    "    cs = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, \\\n",
    "          10, 12, 20, 30, 40, 50, 100, 1000]\n",
    "    f1_scores = []\n",
    "    \n",
    "    for c in cs:\n",
    "        \n",
    "        # logistic regression fits a line like linear regression, but instead of predicting any number, \n",
    "        # it predicts a number between 0 and 1 (sigmoid function).\n",
    "        \n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # C (inverse of regularization strength) controls how much the weights influence the loss, and\n",
    "        # penalizes the sum of squared weights if very different weights exist between different tokens.\n",
    "  \n",
    "        # use l2 regularization, per instructions\n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "        pred_3 = lr.predict(dev_vectors)\n",
    "        \n",
    "        print (\"-------------------------------\")\n",
    "        print (\"LR: f1_score = %s, C = %s\" % (round(metrics.f1_score(dev_labels, pred_3, average='binary'),4), c))\n",
    "        print (\"-------------------------------\")\n",
    "        \n",
    "        # append f1_scores to vector\n",
    "        f1_scores.append(metrics.f1_score(dev_labels, pred_3, average='binary'))\n",
    "        \n",
    "        accuracies.append((lr.score(dev_vectors, dev_labels))*100) \n",
    "\n",
    "        #print (lr.coef_.shape)\n",
    "        \n",
    "        # first define function that squares a given value, for later use in the 'for loop' below\n",
    "        fun_sq_wts = lambda x: x**2\n",
    "        \n",
    "        # use map function, likely faster (because written in C) than list comprehension.\n",
    "        # map function itself applies a function, specifically the first argument on the second argument.\n",
    "        # from coef_, take raw weights (coefficient of the features in the decision function), \n",
    "        # and sum the squares of these weights.\n",
    "\n",
    "        # note: averege=weight vs. average=default should be about same score if similar number of examples across classes\n",
    "        sq_wts = map(fun_sq_wts, lr.coef_[0])\n",
    "        sum_sq_wts =  sum(sq_wts)\n",
    "        print (\"Label = 1, sum of squared weights = %s\" % (sum_sq_wts))\n",
    "\n",
    "        print\n",
    "        \n",
    "    # map two vectors into a dictionary\n",
    "    results_lr = dict(zip(cs, f1_scores))\n",
    "    #print (results_lr)\n",
    "    \n",
    "    # print (the key with the max fl_score\n",
    "    print (\"Logistic Regression: optimal C =\", max(results_lr.items(), key=operator.itemgetter(1))[0])\n",
    "    print\n",
    "    print (\"max accuracy =\", max(accuracies))\n",
    "        \n",
    "explore_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on initial results above, proceed with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "unigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 12313))\n",
      "\n",
      "lr.coef_:\n",
      "[[-1.48092905 -0.39311615 -0.12825177 ...,  0.44453487  1.63180069\n",
      "  -0.87946286]]\n",
      "top 20:\n",
      "[3697, 4147, 6786, 8221, 1039, 6858, 3236, 1286, 6712, 10159, 1523, 10688, 4918, 10689, 9907, 7050, 3901, 2573, 2155, 9349]\n",
      "       Feature      word\n",
      "0         edit  4.824324\n",
      "1       father  4.627494\n",
      "2         mean  4.376547\n",
      "3      pockets  4.057164\n",
      "4          ass  3.995387\n",
      "5    mentioned  3.977975\n",
      "6          die  3.930683\n",
      "7        basic  3.899384\n",
      "8      married  3.850098\n",
      "9     southern -3.846223\n",
      "10       bloke  3.807984\n",
      "11    surprise  3.805289\n",
      "12   graveyard  3.749854\n",
      "13   surprised  3.697465\n",
      "14     sitting -3.697393\n",
      "15       mommy  3.658543\n",
      "16  especially  3.627059\n",
      "17  constantly  3.593246\n",
      "18      cheesy  3.548595\n",
      "19     running  3.539010\n",
      "\n",
      "----------\n",
      "bigram\n",
      "----------\n",
      "\n",
      "('train_vectors.shape:', (3975, 90693))\n",
      "\n",
      "lr.coef_:\n",
      "[[-0.31775435  0.42001296 -0.29407174 ..., -0.26401881 -0.22234693\n",
      "   0.57766744]]\n",
      "top 20:\n",
      "[23002, 38997, 37806, 32065, 28212, 73029, 33880, 44381, 54471, 23001, 81228, 41801, 77046, 2781, 47597, 9433, 84630, 16480, 57349, 59245]\n",
      "                Feature      word\n",
      "0           edit thanks  3.882043\n",
      "1             imgur com  3.879634\n",
      "2            http imgur  3.547851\n",
      "3             got pizza  3.038731\n",
      "4         forward money  2.984111\n",
      "5        sounds amazing  2.960338\n",
      "6        happy birthday  2.947813\n",
      "7   letsfytinglove best  2.947813\n",
      "8        north carolina  2.936326\n",
      "9            edit thank  2.927036\n",
      "10      tonight greatly  2.813269\n",
      "11           just spent  2.809594\n",
      "12         surprise son  2.768559\n",
      "13         afford ramen  2.674511\n",
      "14             love pie  2.633101\n",
      "15         broke payday  2.627687\n",
      "16               ve got  2.625031\n",
      "17        craving pizza -2.599092\n",
      "18          pay forward  2.518712\n",
      "19       pizza actually  2.512937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import *\n",
    "\n",
    "# Feature Selection Notes:\n",
    "'''\n",
    "These objects take as input a scoring function that returns univariate p-values:\n",
    "-For regression: f_regression\n",
    "-For classification: chi2 or f_classif\n",
    "\n",
    "Feature selection with sparse data:\n",
    "-If you use sparse data (i.e. data represented as sparse matrices), \n",
    "only chi2 will deal with the data without making it dense.\n",
    "-Warning: Beware not to use a regression scoring function with a classification problem, \n",
    "you will get useless results.\n",
    "\n",
    "With SVMs and logistic-regression, the parameter C controls the sparsity: \n",
    "the smaller C the less features selected. \n",
    "'''\n",
    "def top20(type):\n",
    "\n",
    "    if type == \"unigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words. \n",
    "        # only applies if default analyzer='word'.\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"unigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "        \n",
    "    elif type == \"bigram\":\n",
    "        \n",
    "        # use stop_words='english' to remove less meaningful words from the resulting tokens. \n",
    "        # only applies if default analyzer='word'.\n",
    "        # set bigrams to be 2 words only\n",
    "        vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        #vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(2, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train_text)\n",
    "        print\n",
    "        print (\"----------\")\n",
    "        print (\"bigram\")\n",
    "        print (\"----------\")\n",
    "        print\n",
    "        print (\"train_vectors.shape:\", train_vectors.shape)\n",
    "        print\n",
    "      \n",
    "    # use C=12\n",
    "    for c in [12]:\n",
    "        \n",
    "        # in the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the default ‘multi_class’ option is set to ‘ovr’ \n",
    "        lr = LogisticRegression(penalty='l2',C=c)\n",
    "        #print (lr)\n",
    "        \n",
    "        # fit the model and generate coef_\n",
    "        lr.fit(train_vectors, train_labels)\n",
    "         \n",
    "        # interested in magnitude of the weights (coefficients), so take absolute value.\n",
    "        # sort absolute values in descending order.\n",
    "        # important to know if negative or positive weight, so still output the positive/negative sign.\n",
    "        # after fitting logistic regression for class vs. all other classes, negative weight of a token \n",
    "        # indicates a class other than class of interest.\n",
    "        # (visual example of negative and positive on a sigmoid function helps) \n",
    "        \n",
    "        print (\"lr.coef_:\")\n",
    "        print (lr.coef_)\n",
    "\n",
    "        # for each label, store the column indices of the top 5 weights \n",
    "        top20 = sorted(range(len(lr.coef_[0])), key=lambda i: abs(lr.coef_[0][i]), reverse=True)[:20]\n",
    "       \n",
    "        col_1 = []\n",
    "        \n",
    "        # for each label, access and store weights via column indices\n",
    "        for index in (top20):\n",
    "\n",
    "            col_1.append(lr.coef_[0][index])\n",
    "           \n",
    "        print (\"top 20:\" )\n",
    "        print (top20)\n",
    "        \n",
    "        # store feature names, after converting to an array\n",
    "        feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "       \n",
    "        # create a Pandas dataframe with 20 rows and 4 columns, plus descriptive headers\n",
    "        df = pd.DataFrame({'Feature': feature_names[top20], 'word': col_1})\n",
    "        print (df    )\n",
    "\n",
    "top20(\"unigram\")\n",
    "top20(\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis\n",
    "This section is the numerical part of the model: utilize all numeric information from the dataset to \n",
    "predict the labels.  This model will be combined the text based model to improve predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.tseries.holiday.USFederalHolidayCalendar object at 0x114d47a90>\n",
      "1341601084\n",
      "2012-07-06 11:58:04\n",
      "<type 'str'>\n",
      "2012-07-06 06:58:04-05:00\n"
     ]
    }
   ],
   "source": [
    "print USFederalHolidayCalendar()\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc'][0]\n",
    "print temp\n",
    "temp2 = datetime.datetime.fromtimestamp(temp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print temp2\n",
    "print type(temp2)\n",
    "\n",
    "# utc = datetime.utcnow()\n",
    "utc = datetime.datetime.strptime(temp2, '%Y-%m-%d %H:%M:%S')\n",
    "utc = utc.replace(tzinfo=from_zone)\n",
    "central = utc.astimezone(to_zone)\n",
    "print central\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-07-06 06:58:04-05:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a float is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-9f3ab1de4077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtemp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#t = temp3['State'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2012-07-06 06:58:04'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: a float is required"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "temp =  train_data['unix_timestamp_of_request_utc']\n",
    "temp =  temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "utc =   temp.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "utc =   utc.apply(lambda x: x.replace(tzinfo=from_zone))\n",
    "#central =  utc.apply(lambda x: x.astimezone(to_zone))\n",
    "\n",
    "temp = [] \n",
    "for i in utc:\n",
    "    temp.append(i.astimezone(to_zone))\n",
    "\n",
    "temp3 = pd.DataFrame(temp, columns=[\"State\"])\n",
    "print temp3.iloc[0,0]\n",
    "#t = temp3['State'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m'))\n",
    "print datetime.datetime.fromtimestamp('2012-07-06 06:58:04').strftime('%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-f3ffd4f78bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocal_tz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mutc_to_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1341601084\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-f3ffd4f78bb1>\u001b[0m in \u001b[0;36mutc_to_local\u001b[0;34m(utc_dt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mutc_to_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutc_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlocal_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutc_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpytz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastimezone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_tz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocal_tz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "\n",
    "local_tz = pytz.timezone('Europe/Moscow') # use your local timezone name here\n",
    "\n",
    "\n",
    "def utc_to_local(utc_dt):\n",
    "    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "    return local_tz.normalize(local_dt) #\n",
    "\n",
    "print utc_to_local(1341601084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Accuracy = 0.7441\n",
      "[[  1.02766350e-02   3.22809077e-03  -7.86677855e-03  -7.03764601e-01\n",
      "   -7.03764601e-01   2.67358626e-05   7.02878469e-04]]\n",
      "Logistic regression on numeric data: F-Score = 0.0606: \n",
      "Accuracy = 0.7441\n",
      "accuracy_score: 0.608696\n",
      "recall_score: 0.031891\n",
      "roc_auc_score: 0.512365378029\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115a8ab10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil import tz\n",
    "np.random.seed(0)\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/Chicago')\n",
    "\n",
    "#Extra numerical data from text and text titles \n",
    "\n",
    "#Created new df for training data(numeric only)\n",
    "train_data_numeric = np.zeros((len(train_data), 7))\n",
    "dev_data_numeric =  np.zeros((len(dev_data), 7))\n",
    "\n",
    "temp = train_data['unix_timestamp_of_request_utc']\n",
    "temp = temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "train_data_numeric[:,0]= train_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "train_data_numeric[:,1]= train_data['request_text'].apply(lambda x: len(x.split(' '))) \n",
    "train_data_numeric[:,2]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m')) # Month in integer\n",
    "train_data_numeric[:,3]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%d')) > 15  # Second Half\n",
    "train_data_numeric[:,4]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%H')) > 12\n",
    "train_data_numeric[:,5] = train_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "train_data_numeric[:,6] = train_data['requester_account_age_in_days_at_retrieval']\n",
    "\n",
    "#post_was_edited\n",
    "#Created new df for dev data(numeric only)\n",
    "\n",
    "temp = dev_data['unix_timestamp_of_request']\n",
    "\n",
    "dev_data_numeric[:,0]= dev_data['request_title'].apply(lambda x: len(x.split(' '))) \n",
    "dev_data_numeric[:,1]= dev_data['request_text'].apply(lambda x: len(x.split(' ')))\n",
    "dev_data_numeric[:,2]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%m'))\n",
    "dev_data_numeric[:,3]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%d')) > 15\n",
    "dev_data_numeric[:,4]= temp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%H')) > 12 \n",
    "dev_data_numeric[:,5] = dev_data['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "dev_data_numeric[:,6] = dev_data['requester_account_age_in_days_at_retrieval']\n",
    "\n",
    "\n",
    "logit = LogisticRegression(C=1, penalty = 'l2')\n",
    "result = logit.fit(train_data_numeric, train_labels)\n",
    "\n",
    "preds = logit.predict(dev_data_numeric)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit.predict_proba(dev_data_numeric)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print \"Accuracy = %0.4f\" % (accuracy)\n",
    "print result.coef_\n",
    "\n",
    "model_output(pred_probas, F_Score, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "    #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "    #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "    #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "    #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "    #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "    #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "    #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "    s = BeautifulSoup(s).get_text() # Newly addition\n",
    "\n",
    "    return s\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=L1)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=L2 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "    \n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "    \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "giver_username_if_known\n",
      "in_test_set\n",
      "number_of_downvotes_of_request_at_retrieval\n",
      "number_of_upvotes_of_request_at_retrieval\n",
      "post_was_edited\n",
      "request_id\n",
      "request_number_of_comments_at_retrieval\n",
      "request_text\n",
      "request_text_edit_aware\n",
      "request_title\n",
      "requester_account_age_in_days_at_request\n",
      "requester_account_age_in_days_at_retrieval\n",
      "requester_days_since_first_post_on_raop_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval\n",
      "requester_number_of_comments_at_request\n",
      "requester_number_of_comments_at_retrieval\n",
      "requester_number_of_comments_in_raop_at_request\n",
      "requester_number_of_comments_in_raop_at_retrieval\n",
      "requester_number_of_posts_at_request\n",
      "requester_number_of_posts_at_retrieval\n",
      "requester_number_of_posts_on_raop_at_request\n",
      "requester_number_of_posts_on_raop_at_retrieval\n",
      "requester_number_of_subreddits_at_request\n",
      "requester_subreddits_at_request\n",
      "requester_upvotes_minus_downvotes_at_request\n",
      "requester_upvotes_minus_downvotes_at_retrieval\n",
      "requester_upvotes_plus_downvotes_at_request\n",
      "requester_upvotes_plus_downvotes_at_retrieval\n",
      "requester_user_flair\n",
      "requester_username\n",
      "unix_timestamp_of_request\n",
      "unix_timestamp_of_request_utc\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print train_data.loc[0,'requester_upvotes_minus_downvotes_at_retrieval']\n",
    "c = 0\n",
    "for i in list(train_data):\n",
    "    c = c +1\n",
    "    print i\n",
    "print len(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1)Baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301158\n",
      "precision_score: 0.346154\n",
      "recall_score: 0.266515\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary'))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2)Add preprocessing and L1 feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===>>Add Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=580 ,l2=47 , f1 score is 0.336075\n"
     ]
    }
   ],
   "source": [
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor = first_preprocessor,analyzer='word',stop_words='english', tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,600,10):\n",
    "    for j in range(1,50,2):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "print('When l1=%i ,l2=%i , f1 score is %f' %(l1,l2,max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Roc curve:\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def model_output(pred_probas, F_Score, preds):\n",
    "\n",
    "    print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "    accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "    print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "    print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "    print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "    print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)\n",
    "\n",
    "#Create ROC Curve\n",
    "\n",
    "    fpr,tpr,_ = roc_curve(dev_labels, pred_probas[:,1])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Best Model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      "Logistic regression on numeric data: F-Score = 0.3252: \n",
      "Accuracy = 0.6745\n",
      "accuracy_score: 0.350923\n",
      "recall_score: 0.302961\n",
      "roc_auc_score: 0.553628609174\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEgpiQvIhLElORFRIKYkryISBD7/1MJzHbs54WuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137fe210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best non-ensemble method so far\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = model_LG.predict_proba(dev_matrix_process[:,index])\n",
    "\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "Logistic regression on numeric data: F-Score = 0.3285: \n",
      "Accuracy = 0.6698\n",
      "accuracy_score: 0.346835\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.55341114814\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c31890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Basic Ensemble Method:\n",
    "np.random.seed(0)\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=510)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=11)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "\n",
    "#######Ensemble Method Starts###################################\n",
    "logit = LogisticRegression()\n",
    "logit_en = LogisticRegression()\n",
    "logit.fit(train_data_numeric[:,2:5], train_labels)\n",
    "\n",
    "#Create a new set of step 2 logistic regression based on step 1 probability outputs\n",
    "#X1: predicted prob. of positive labels with text-based model\n",
    "#X2: predicted prob. of positive labels with text-based model\n",
    "En_X1_train = logit.predict_proba(train_data_numeric[:,2:5])[:,1]\n",
    "En_X2_train = model_LG.predict_proba(word_matrix_process[:,index])[:,1]\n",
    "En_X1_dev = logit.predict_proba(dev_data_numeric[:,2:5])[:,1]\n",
    "En_X2_dev = model_LG.predict_proba(dev_matrix_process[:,index])[:,1]\n",
    "\n",
    "t_size = len(En_X1_train)\n",
    "d_size = len(En_X1_dev)\n",
    "En_train = np.zeros((t_size, 2))\n",
    "En_dev = np.zeros((d_size, 2))\n",
    "\n",
    "En_train[:,0] = np.round(En_X1_train, 2)\n",
    "En_train[:,1] = np.round(En_X2_train, 2)\n",
    "En_dev[:,0] =   np.round(En_X1_dev, 2)\n",
    "En_dev[:,1] =   np.round(En_X2_dev, 2)\n",
    "\n",
    "logit_en.fit(En_train, train_labels)\n",
    "\n",
    "preds = logit_en.predict(En_dev)\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "pred_probas = logit_en.predict_proba(En_dev)\n",
    "\n",
    "print preds.sum()\n",
    "\n",
    "#Call to display outputs\n",
    "model_output(pred_probas, F_Score, preds) #Please input pred_probas, F_Score, and prediction(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3)NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on numeric data: F-Score = 0.3224: \n",
      "Accuracy = 0.6604\n",
      "accuracy_score: 0.333333\n",
      "recall_score: 0.312073\n",
      "roc_auc_score: 0.547046788554\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = first_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=580)#C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "#Update the new index\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=47)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "preds = model_LG.predict(dev_matrix_process[:,index])\n",
    "F_Score = metrics.f1_score(dev_labels, preds, average='binary')\n",
    "\n",
    "\n",
    "print \"Logistic regression on numeric data: F-Score = %0.4f: \"%(F_Score)  #Sum up Squared Weights\n",
    "accuracy = np.where(preds==dev_labels, 1, 0).sum() / float(len(dev_labels))\n",
    "print (\"Accuracy = %0.4f\" % (accuracy))\n",
    "print 'accuracy_score: %f' %metrics.precision_score(dev_labels, preds)\n",
    "print 'recall_score: %f' %metrics.recall_score(dev_labels, preds)\n",
    "print \"roc_auc_score:\", metrics.roc_auc_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4)PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of features based on the L1 regularization, instead of inputting all words into PCA to improve the speed\n",
    "vectorizer_process = CountVectorizer(preprocessor = empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560) #C from the above test\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n",
    "plt.plot(pca_mod.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.316069\n",
      "recall_score: 0.271071\n",
      "precision_score: 0.378981\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=19 )#C from the above test\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary'))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print('precision_score: %f' %metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
