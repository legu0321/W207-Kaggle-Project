{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Title and Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.chdir('C:\\\\Users\\\\lgu\\\\Downloads\\\\pizza_request_dataset.tar\\\\pizza_request_dataset\\\\pizza_request_dataset' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giver_username_if_known' 'in_test_set'\n",
      " 'number_of_downvotes_of_request_at_retrieval'\n",
      " 'number_of_upvotes_of_request_at_retrieval' 'post_was_edited' 'request_id'\n",
      " 'request_number_of_comments_at_retrieval' 'request_text'\n",
      " 'request_text_edit_aware' 'request_title'\n",
      " 'requester_account_age_in_days_at_request'\n",
      " 'requester_account_age_in_days_at_retrieval'\n",
      " 'requester_days_since_first_post_on_raop_at_request'\n",
      " 'requester_days_since_first_post_on_raop_at_retrieval'\n",
      " 'requester_number_of_comments_at_request'\n",
      " 'requester_number_of_comments_at_retrieval'\n",
      " 'requester_number_of_comments_in_raop_at_request'\n",
      " 'requester_number_of_comments_in_raop_at_retrieval'\n",
      " 'requester_number_of_posts_at_request'\n",
      " 'requester_number_of_posts_at_retrieval'\n",
      " 'requester_number_of_posts_on_raop_at_request'\n",
      " 'requester_number_of_posts_on_raop_at_retrieval'\n",
      " 'requester_number_of_subreddits_at_request' 'requester_received_pizza'\n",
      " 'requester_subreddits_at_request'\n",
      " 'requester_upvotes_minus_downvotes_at_request'\n",
      " 'requester_upvotes_minus_downvotes_at_retrieval'\n",
      " 'requester_upvotes_plus_downvotes_at_request'\n",
      " 'requester_upvotes_plus_downvotes_at_retrieval' 'requester_user_flair'\n",
      " 'requester_username' 'unix_timestamp_of_request'\n",
      " 'unix_timestamp_of_request_utc']\n",
      "(5671, 33)\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas, json\n",
    "from sklearn.feature_extraction.text import *\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "json_data = json.load(open('pizza_request_dataset.json'))\n",
    "df=pandas.io.json.json_normalize(json_data)\n",
    "print(df.columns.values)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (3969,)\n",
      "test shape: (3969, 32)\n",
      "test label shape: (1702,)\n",
      "test shape: (1702, 32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train, dev = train_test_split(df, test_size = 0.3)\n",
    "\n",
    "train_labels=train[\"requester_received_pizza\"]\n",
    "train_data = train\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "dev_labels=dev[\"requester_received_pizza\"]\n",
    "dev_data = dev\n",
    "del dev_data[\"requester_received_pizza\"]\n",
    "\n",
    "print ('training label shape:', train_labels.shape)\n",
    "print ('test shape:', train_data.shape)\n",
    "print ('test label shape:', dev_labels.shape)\n",
    "print ('test shape:', dev_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text=train_data['request_text'].as_matrix()\n",
    "train_title=train_data['request_title'].as_matrix()\n",
    "train_all = train_text+train_title\n",
    "\n",
    "dev_text=dev_data['request_text'].as_matrix()\n",
    "dev_title=dev_data['request_title'].as_matrix()\n",
    "dev_all = dev_text+dev_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###baseline: no reprocess, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319123020706\n",
      "0.334183673469\n",
      "0.305361305361\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "\n",
    "f1_score=metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary')\n",
    "print(f1_score)\n",
    "print(metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print(metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Add preprocess and L1 feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure none of the doc is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([], shape=(1, 0), dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "# total_sum=csr_matrix.sum(word_matrix_process,axis=1)\n",
    "# total_sum.shape\n",
    "np.where(total_sum==0 )[0]\n",
    "# total_sum[1:5]\n",
    "# np.zeros(total_sum[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "#     #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "#     #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "#     #remove word longer than 20 digit\n",
    "#     s=re.sub(\"[0-9a-z]{20,}\",\"\",s)\n",
    "#     #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "#     s=re.sub(\"ly\\Z\",\" \",s)\n",
    "#     #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "#     #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "#     #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "#     #remove stem end with 'ful'\n",
    "#     s=re.sub(\"ful\\s\",\" \",s)\n",
    "#     s=re.sub(\"ful\\Z\",\" \",s)\n",
    "#     #remove stem end with 'able'\n",
    "#     s=re.sub(\"able\\s\",\" \",s)\n",
    "#     s=re.sub(\"able\\Z\",\" \",s)\n",
    "#     #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "#     s=re.sub(\"ing\\Z\",\" \",s)\n",
    "#     #remove stop words\n",
    "#     s=re.sub(\" about \",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "\n",
    "    return s\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "# train_data_array=train_data['request_text'].as_matrix()\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "def tune_para(L1,L2):\n",
    "    model_LG_L1 = LogisticRegression(penalty ='l1',C=570)\n",
    "    model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "    index=[]\n",
    "    for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "        if z!=0:\n",
    "            index.append(i)\n",
    "#     print(word_matrix_process.shape)\n",
    "#     print(len(index))\n",
    "    # print('When C= %f, the number of learned weights that are not equal to zero is %d' %(c,len(index)))\n",
    "    #train the logistic regression with the features after pruning\n",
    "\n",
    "    model_LG_L2 = LogisticRegression(penalty ='l2',C=12 )\n",
    "    model_LG_L2.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "    # accuracy.append(metrics.accuracy_score(test_labels,model_LG_p6_l2_retrain.predict(vectorizer_p6.transform(test_data)[:,index])))\n",
    "\n",
    "    f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index]),average='binary')\n",
    "#     print(f1_score)\n",
    "#     print(metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process[:,index])))\n",
    "# print(metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop in parameters and get the best parameters for C of L1 and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When l1=500 ,l2=1 , taccuracy is 0.343529\n",
      "When l1=500 ,l2=2 , taccuracy is 0.341578\n",
      "When l1=500 ,l2=3 , taccuracy is 0.340828\n",
      "When l1=500 ,l2=4 , taccuracy is 0.332547\n",
      "When l1=500 ,l2=5 , taccuracy is 0.337264\n",
      "When l1=500 ,l2=6 , taccuracy is 0.341578\n",
      "When l1=500 ,l2=7 , taccuracy is 0.342169\n",
      "When l1=500 ,l2=8 , taccuracy is 0.341232\n",
      "When l1=500 ,l2=9 , taccuracy is 0.334123\n",
      "When l1=500 ,l2=10 , taccuracy is 0.333728\n",
      "When l1=500 ,l2=11 , taccuracy is 0.338863\n",
      "When l1=500 ,l2=12 , taccuracy is 0.343934\n",
      "When l1=500 ,l2=13 , taccuracy is 0.331375\n",
      "When l1=500 ,l2=14 , taccuracy is 0.334906\n",
      "When l1=500 ,l2=15 , taccuracy is 0.336867\n",
      "When l1=500 ,l2=16 , taccuracy is 0.336516\n",
      "When l1=500 ,l2=17 , taccuracy is 0.344009\n",
      "When l1=500 ,l2=18 , taccuracy is 0.332147\n",
      "When l1=500 ,l2=19 , taccuracy is 0.333333\n",
      "When l1=500 ,l2=20 , taccuracy is 0.343195\n",
      "When l1=500 ,l2=21 , taccuracy is 0.352526\n",
      "When l1=500 ,l2=22 , taccuracy is 0.334112\n",
      "When l1=500 ,l2=23 , taccuracy is 0.336114\n",
      "When l1=500 ,l2=24 , taccuracy is 0.351001\n",
      "When l1=500 ,l2=25 , taccuracy is 0.334123\n",
      "When l1=500 ,l2=26 , taccuracy is 0.339265\n",
      "When l1=500 ,l2=27 , taccuracy is 0.349765\n",
      "When l1=500 ,l2=28 , taccuracy is 0.337662\n",
      "When l1=500 ,l2=29 , taccuracy is 0.342723\n",
      "When l1=500 ,l2=30 , taccuracy is 0.338902\n",
      "When l1=500 ,l2=31 , taccuracy is 0.341981\n",
      "When l1=500 ,l2=32 , taccuracy is 0.341289\n",
      "When l1=500 ,l2=33 , taccuracy is 0.334520\n",
      "When l1=500 ,l2=34 , taccuracy is 0.338863\n",
      "When l1=500 ,l2=35 , taccuracy is 0.338863\n",
      "When l1=500 ,l2=36 , taccuracy is 0.336842\n",
      "When l1=500 ,l2=37 , taccuracy is 0.335301\n",
      "When l1=500 ,l2=38 , taccuracy is 0.330189\n",
      "When l1=500 ,l2=39 , taccuracy is 0.340882\n",
      "When l1=500 ,l2=40 , taccuracy is 0.338061\n",
      "When l1=500 ,l2=41 , taccuracy is 0.338498\n",
      "When l1=500 ,l2=42 , taccuracy is 0.331347\n",
      "When l1=500 ,l2=43 , taccuracy is 0.332934\n",
      "When l1=500 ,l2=44 , taccuracy is 0.337662\n",
      "When l1=500 ,l2=45 , taccuracy is 0.332147\n",
      "When l1=500 ,l2=46 , taccuracy is 0.350000\n",
      "When l1=500 ,l2=47 , taccuracy is 0.343195\n",
      "When l1=500 ,l2=48 , taccuracy is 0.342385\n",
      "When l1=500 ,l2=49 , taccuracy is 0.341981\n",
      "When l1=510 ,l2=1 , taccuracy is 0.337662\n",
      "When l1=510 ,l2=2 , taccuracy is 0.331361\n",
      "When l1=510 ,l2=3 , taccuracy is 0.338425\n",
      "When l1=510 ,l2=4 , taccuracy is 0.332937\n",
      "When l1=510 ,l2=5 , taccuracy is 0.340024\n",
      "When l1=510 ,l2=6 , taccuracy is 0.339265\n",
      "When l1=510 ,l2=7 , taccuracy is 0.337693\n",
      "When l1=510 ,l2=8 , taccuracy is 0.339667\n",
      "When l1=510 ,l2=9 , taccuracy is 0.334112\n",
      "When l1=510 ,l2=10 , taccuracy is 0.335697\n",
      "When l1=510 ,l2=11 , taccuracy is 0.339265\n",
      "When l1=510 ,l2=12 , taccuracy is 0.337292\n",
      "When l1=510 ,l2=13 , taccuracy is 0.340476\n",
      "When l1=510 ,l2=14 , taccuracy is 0.340828\n",
      "When l1=510 ,l2=15 , taccuracy is 0.351415\n",
      "When l1=510 ,l2=16 , taccuracy is 0.333333\n",
      "When l1=510 ,l2=17 , taccuracy is 0.340828\n",
      "When l1=510 ,l2=18 , taccuracy is 0.336075\n",
      "When l1=510 ,l2=19 , taccuracy is 0.331361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-4fba10074285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtune_para\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-4cbe51a63e73>\u001b[0m in \u001b[0;36mtune_para\u001b[1;34m(L1, L2)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtune_para\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel_LG_L1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m570\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel_LG_L1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_matrix_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgu\\AppData\\Local\\Continuum\\Anaconda\\envs\\mpl33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m                 )\n\u001b[0;32m   1038\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgu\\AppData\\Local\\Continuum\\Anaconda\\envs\\mpl33\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         epsilon)\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "max=0\n",
    "l1=0\n",
    "l2=0\n",
    "for i in range(500,700,10):\n",
    "    for j in range(1,50,1):\n",
    "        acc=tune_para(i,j)\n",
    "        if acc>max:\n",
    "            max=acc\n",
    "            l1=i\n",
    "            l2=j\n",
    "        print('When l1=%i ,l2=%i , taccuracy is %f' %(i,j,acc))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When l1=550 ,l2=45 , accuracy is 0.355556 empty_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2621\n"
     ]
    }
   ],
   "source": [
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=550)\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "print(len(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first 1000 element, 0.961224 of the total variance in the training data is explained \n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "i=1000\n",
    "pca_mod = PCA(n_components = i)\n",
    "word_matrix_process_pca=pca_mod.fit_transform(word_matrix_process.toarray()[:,index])\n",
    "print('For the first %i element, %f of the total variance in the training data is explained ' %(i,sum(pca_mod.explained_variance_ratio_)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.330322580645\n",
      "0.369942196532\n",
      "0.298368298368\n",
      "0.695064629847\n"
     ]
    }
   ],
   "source": [
    "dev_matrix_process_pca=pca_mod.transform(dev_matrix_process.toarray()[:,index])\n",
    "# dev_matrix_process_pca.shape\n",
    "\n",
    "model_LG_L2 = LogisticRegression(penalty ='l2',C=45 )\n",
    "model_LG_L2.fit(word_matrix_process_pca, train_labels)\n",
    "f1_score=metrics.f1_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca),average='binary')\n",
    "print(f1_score)\n",
    "print(metrics.precision_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print(metrics.recall_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))\n",
    "print(metrics.accuracy_score(dev_labels,model_LG_L2.predict(dev_matrix_process_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Narrative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_featureCount(feature_list,matrix,vectorizer):\n",
    "    a=csr_matrix((matrix.shape[0],1), dtype=np.int8)\n",
    "    for word in feature_list:\n",
    "        print(word)\n",
    "        if vectorizer.vocabulary_.get(word)!=None:\n",
    "            b=matrix[:,vectorizer_process.vocabulary_.get(word)]\n",
    "            a= hstack([a,b],format= 'csr')\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4285714286\n",
      "(3969, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  2.94117647])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "desire = ['party','birthday','boyfriend','girlfriend','date','drinks','drunk','wasted','invite','invited','celebrate',\n",
    "          'celebrating','game','games','movie','beer','crave','craving']\n",
    "\n",
    "narrative_count_abs=get_featureCount(desire,word_matrix_process,vectorizer_process) \n",
    "narrative_count_adj=csr_matrix.sum(narrative_count_abs,axis=1)/(csr_matrix.sum(word_matrix_process,axis=1) + 0.0)*100\n",
    "print(np.max(narrative_count_adj))\n",
    "print(narrative_count_adj.shape)\n",
    "# print(type(narrative_count_adj.toarray()))\n",
    "np.percentile( narrative_count_adj[:,0] , np.arange(0, 100, 10))\n",
    "# a=csr_matrix((2,1), dtype=np.int8)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3969, 1)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "(1, 734)\n"
     ]
    }
   ],
   "source": [
    "print(narrative_count_adj.shape)\n",
    "print(narrative_count_abs.toarray()[0])\n",
    "train_all[0]\n",
    "# np.median(narrative_count_adj)\n",
    "a=narrative_count_adj \n",
    "print(  np.nonzero(a)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    investment\n",
      "0            1\n",
      "1            7\n",
      "2            8\n",
      "3            2\n",
      "4            3\n",
      "5            4\n",
      "6            5\n",
      "7            6\n",
      "8            6\n",
      "9            6\n",
      "10           7\n",
      "    investment  decile\n",
      "0            1       0\n",
      "1            7       2\n",
      "2            8       2\n",
      "3            2       0\n",
      "4            3       0\n",
      "5            4       0\n",
      "6            5       1\n",
      "7            6       1\n",
      "8            6       1\n",
      "9            6       1\n",
      "10           7       2\n"
     ]
    }
   ],
   "source": [
    "df = pandas.DataFrame([1,7,8,2,3,4,5,6,6,6,7], columns=['investment'])\n",
    "print(df)\n",
    "df['decile'] = pandas.qcut(df['investment'], 3, labels=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'matrix' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-566c0499e0cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnarrative_count_adj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnarrative_count_adj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# list_count=pandas.DataFrame(narrative_count_adj[np.nonzero(narrative_count_adj)],columns=['desire'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# # # print(desire)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# np.nonzero(list_count['desire'])[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'matrix' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "narrative_count_adj[np.nonzero(narrative_count_adj)].as_matrix()\n",
    "\n",
    "# list_count=pandas.DataFrame(narrative_count_adj[np.nonzero(narrative_count_adj)],columns=['desire'])\n",
    "# # # print(desire)\n",
    "# np.nonzero(list_count['desire'])[0]\n",
    "# list_count[:np.nonzero(list_count['desire'])[0]]\n",
    "# list_count['decile'] = pandas.qcut(list_count[np.nonzero(list_count['desire'])], 10, labels=False)\n",
    "# print(sum(list_count['decile']==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.332564\n",
      "accuracy_score: 0.329519\n",
      "recall_score: 0.335664\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer_process = CountVectorizer(analyzer='word',stop_words='english',preprocessor = empty_preprocessor,tokenizer=tokenize)\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_all)\n",
    "dev_matrix_process = vectorizer_process.transform(dev_all)\n",
    "\n",
    "model_LG_L1 = LogisticRegression(penalty ='l1',C=560)\n",
    "model_LG_L1.fit(word_matrix_process, train_labels)\n",
    "\n",
    "index=[]\n",
    "for i,z in enumerate(np.sum(np.abs(model_LG_L1.coef_),axis=0) ):\n",
    "    if z!=0:\n",
    "        index.append(i)\n",
    "            \n",
    "model_LG = LogisticRegression(penalty ='l2',C=19)#C from the above test\n",
    "model_LG.fit(word_matrix_process[:,index], train_labels)\n",
    "\n",
    "print('f1_score: %f' %metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process[:,index]),average='binary'))\n",
    "print('accuracy_score: %f' %metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process[:,index])))\n",
    "print('recall_score: %f' %metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process[:,index])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
