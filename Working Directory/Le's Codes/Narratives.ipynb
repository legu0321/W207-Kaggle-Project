{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Narrative\n",
    "\n",
    "I tried to add up the count for each narrative sets and remove the single word feature from the word matrix based on Count Vectorizer.\n",
    "\n",
    "\"Narrative To measure the usage of all five narratives in requests we use word count features that measure how often\n",
    "a given requests mentions words from the previously defined narrative lexicons. We normalize these features by the total\n",
    "number of words in the request to remove length effects. Here, we use median-thresholded binary variables for easier\n",
    "interpretation but use decile-coded variants in the following prediction task.\"\n",
    "--Thesis Page 5\n",
    "\n",
    "However, the accuracy is not improved...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import pandas, json\n",
    "from sklearn.feature_extraction.text import *\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "json_data = json.load(open('pizza_request_dataset.json'))\n",
    "df=pandas.io.json.json_normalize(json_data)\n",
    "print(df.columns.values)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train, dev = train_test_split(df, test_size = 0.3)\n",
    "\n",
    "train_labels=train[\"requester_received_pizza\"]\n",
    "train_data = train\n",
    "del train_data[\"requester_received_pizza\"]\n",
    "\n",
    "dev_labels=dev[\"requester_received_pizza\"]\n",
    "dev_data = dev\n",
    "del dev_data[\"requester_received_pizza\"]\n",
    "\n",
    "print ('training label shape:', train_labels.shape)\n",
    "print ('test shape:', train_data.shape)\n",
    "print ('test label shape:', dev_labels.shape)\n",
    "print ('test shape:', dev_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build count Vectorizer and test the accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_preprocessor(s):\n",
    "    #convert to lowercase\n",
    "    s=s.lower()\n",
    "    s=re.sub(\"[,.!?:;/~*]\",\" \",s)\n",
    "#     #remove duplicated 0s and 1s\n",
    "    s=re.sub(\"[0-9]*\",\"\",s)\n",
    "#     #remove number longer than 5 digit\n",
    "    s=re.sub(\"[0-9]{5,}\",\"\",s)\n",
    "#     #remove word longer than 20 digit\n",
    "#     s=re.sub(\"[0-9a-z]{20,}\",\"\",s)\n",
    "#     #remove stem end with 'ly'\n",
    "    s=re.sub(\"ly\\s\",\" \",s)\n",
    "#     s=re.sub(\"ly\\Z\",\" \",s)\n",
    "#     #remove plural form\n",
    "    s=re.sub(\"s\\s\",\" \",s)\n",
    "    s=re.sub(\"s\\Z\",\" \",s)\n",
    "#     #remove _ as the end of word\n",
    "    s=re.sub(\"[_]+\",\" \",s)\n",
    "#     #remove _ as start of the word\n",
    "    s=re.sub(\"\\s[_]+\",\" \",s)\n",
    "#     #remove stem end with 'ful'\n",
    "#     s=re.sub(\"ful\\s\",\" \",s)\n",
    "#     s=re.sub(\"ful\\Z\",\" \",s)\n",
    "#     #remove stem end with 'able'\n",
    "#     s=re.sub(\"able\\s\",\" \",s)\n",
    "#     s=re.sub(\"able\\Z\",\" \",s)\n",
    "#     #remove stem end with 'ness'\n",
    "    s=re.sub(\"ness\\s\",\" \",s)\n",
    "    s=re.sub(\"ing\\s\",\" \",s)\n",
    "#     s=re.sub(\"ing\\Z\",\" \",s)\n",
    "#     #remove stop words\n",
    "#     s=re.sub(\" about \",\" \",s)\n",
    "    #remove words that are too short\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\s\",\" \",s)\n",
    "    s=re.sub(\"\\s[0-9a-z]{1,2}\\Z\",\" \",s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "vectorizer_process = CountVectorizer(preprocessor =empty_preprocessor,analyzer='word',stop_words='english' )\n",
    "word_matrix_process= vectorizer_process.fit_transform(train_data['request_text'])\n",
    "dev_matrix_process = vectorizer_process.transform(dev_data['request_text'])\n",
    "\n",
    "\n",
    "model_LG = LogisticRegression(penalty ='l2',C=12)\n",
    "model_LG.fit(word_matrix_process, train_labels)\n",
    "\n",
    "\n",
    "f1_score=metrics.f1_score(dev_labels,model_LG.predict(dev_matrix_process),average='binary')\n",
    "print(f1_score)\n",
    "print(metrics.precision_score(dev_labels,model_LG.predict(dev_matrix_process)))\n",
    "print(metrics.recall_score(dev_labels,model_LG.predict(dev_matrix_process)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "def combine_features(feature_list,matrix,vectorizer):\n",
    "    a=csr_matrix((matrix.shape[0],1), dtype=np.int8)\n",
    "    #create count matric for each word in the list\n",
    "    for word in feature_list:\n",
    "        if vectorizer.vocabulary_.get(word)!=None:\n",
    "            b=matrix[:,vectorizer_process.vocabulary_.get(word)]\n",
    "            a= hstack([a,b],format= 'csr')\n",
    "            \n",
    "    #get the total count \n",
    "    a_sum= csr_matrix.sum(a,axis=1)\n",
    "    #convert to the ndarray and remove the each single word feature\n",
    "    del_ls=[]\n",
    "    for word in feature_list:\n",
    "        if vectorizer.vocabulary_.get(word)!=None:\n",
    "            del_ls.append(vectorizer_process.vocabulary_.get(word))\n",
    "    print(del_ls)\n",
    "    \n",
    "    matrix=matrix.toarray()\n",
    "    matrix=np.delete(matrix,del_ls,axis=1)\n",
    "    \n",
    "    return (hstack([sparse.csr_matrix(matrix),a_sum],format= 'csr'))\n",
    "\n",
    "\n",
    "desire = ['party','birthday','boyfriend','girlfriend','date','drinks','drunk','wasted','invite','invited','celebrate',\n",
    "          'celebrating','game','games','movie','beer','crave','craving']\n",
    "family = ['husband','wife','family','parent','parents','mother','father','mom','mum','son','dad','daughter']\n",
    "job =    ['job','unemployment','employment','hire','hired','fired','interview','work','paycheck']\n",
    "money=   ['money','bill','bills','rent','bank','account','paycheck','due','broke','bills','deposit','cash','dollar','dollars',\n",
    "          'bucks','paid','payed','buy','check','spent','financial',\n",
    "          'poor','loan','credit','budget','day','now','time','week','until','last','month','tonight','today','next','night',\n",
    "          'when','tomorrow','first','after','while','before','long','hour','Friday','ago','still','due','past','soon','current',\n",
    "          'years','never','till','yesterday','morning','evening']\n",
    "\n",
    "for i in [desire,family,job,money]:\n",
    "    word_matrix_process=combine_features(i,word_matrix_process,vectorizer_process)\n",
    "\n",
    "print(word_matrix_process.shape)\n",
    "\n",
    "for i in [desire,family,job,money]:\n",
    "    dev_matrix_process=combine_features(i,dev_matrix_process,vectorizer_process)\n",
    "\n",
    "print(dev_matrix_process.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
